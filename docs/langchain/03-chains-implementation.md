# LangChain Chains ç»„ä»¶æ·±åº¦å®ç°åˆ†æ

## ç›®å½•

1. [Chainsçš„æ ¸å¿ƒæ¦‚å¿µ](#chainsçš„æ ¸å¿ƒæ¦‚å¿µ)
2. [Runnableæ¥å£ä½“ç³»](#runnableæ¥å£ä½“ç³»)
3. [LCELè¯­æ³•æœºåˆ¶](#lcelè¯­æ³•æœºåˆ¶)
4. [æ ¸å¿ƒå®ç°ç»„ä»¶](#æ ¸å¿ƒå®ç°ç»„ä»¶)
5. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
6. [å®é™…åº”ç”¨æ¨¡å¼](#å®é™…åº”ç”¨æ¨¡å¼)
7. [æœ€ä½³å®è·µæŒ‡å—](#æœ€ä½³å®è·µæŒ‡å—)

---

## Chainsçš„æ ¸å¿ƒæ¦‚å¿µ

### è®¾è®¡å“²å­¦çš„è½¬å˜

LangChainçš„Chainsç»„ä»¶ç»å†äº†ä»ä¼ ç»Ÿé¢å‘å¯¹è±¡è®¾è®¡åˆ°ç°ä»£å‡½æ•°å¼ç»„åˆçš„é‡å¤§è½¬å˜ï¼Œè¿™ä¸€æ¼”è¿›ä½“ç°äº†è½¯ä»¶æ¶æ„è®¾è®¡çš„æœ€ä½³å®è·µã€‚

```python
# ä¼ ç»ŸChainè®¾è®¡çš„é—®é¢˜
class TraditionalChain(Chain):
    """ä¼ ç»ŸåŸºäºç»§æ‰¿çš„Chainè®¾è®¡"""
    
    def __init__(self, llm, prompt, memory=None, callbacks=None):
        self.llm = llm
        self.prompt = prompt
        self.memory = memory
        self.callbacks = callbacks
    
    def _call(self, inputs: dict) -> dict:
        # å¤æ‚çš„æ‰‹åŠ¨ç¼–æ’é€»è¾‘
        if self.memory:
            memory_vars = self.memory.load_memory_variables(inputs)
            inputs.update(memory_vars)
        
        prompt_text = self.prompt.format(**inputs)
        response = self.llm(prompt_text)
        
        if self.memory:
            self.memory.save_context(inputs, {"output": response})
        
        return {"output": response}

# ç°ä»£LCELè®¾è®¡çš„ä¼˜é›…
modern_chain = (
    RunnablePassthrough.assign(
        memory_context=lambda x: memory.load_memory_variables(x)
    )
    | prompt
    | llm
    | RunnableLambda(lambda x: memory.save_context(x) or x)
)
```

### Chainçš„æœ¬è´¨ï¼šæ•°æ®æµå˜æ¢

åœ¨LCELçš„è®¾è®¡ä¸­ï¼ŒChainæœ¬è´¨ä¸Šæ˜¯ä¸€ç³»åˆ—æ•°æ®å˜æ¢å‡½æ•°çš„ç»„åˆï¼š

```python
# Chain = f(g(h(input))) çš„å‡½æ•°ç»„åˆ
# input â†’ transform1 â†’ transform2 â†’ transform3 â†’ output

from typing import TypeVar, Callable

Input = TypeVar('Input')
Intermediate = TypeVar('Intermediate') 
Output = TypeVar('Output')

# æ•°å­¦ä¸Šçš„å‡½æ•°ç»„åˆ
def compose(f: Callable[[Intermediate], Output], 
           g: Callable[[Input], Intermediate]) -> Callable[[Input], Output]:
    return lambda x: f(g(x))

# LangChainçš„å®ç°
chain = component_a | component_b | component_c
# ç­‰ä»·äº compose(component_c, compose(component_b, component_a))
```

---

## Runnableæ¥å£ä½“ç³»

### æ ¸å¿ƒæŠ½è±¡è®¾è®¡

```python
# langchain_core/runnables/base.py æ ¸å¿ƒå®ç°åˆ†æ
from abc import ABC, abstractmethod
from typing import Generic, TypeVar, Optional, Union, Iterator, AsyncIterator

Input = TypeVar('Input')
Output = TypeVar('Output')

class Runnable(Generic[Input, Output], ABC):
    """
    LangChainç»Ÿä¸€æ‰§è¡Œæ¥å£çš„æ ¸å¿ƒæŠ½è±¡
    
    è®¾è®¡ç›®æ ‡ï¼š
    1. ç»Ÿä¸€æ‰€æœ‰ç»„ä»¶çš„è°ƒç”¨æ¥å£
    2. æ”¯æŒå¤šç§æ‰§è¡Œæ¨¡å¼ï¼ˆåŒæ­¥/å¼‚æ­¥/æ‰¹å¤„ç†/æµå¼ï¼‰
    3. é€šè¿‡æ³›å‹ç¡®ä¿ç±»å‹å®‰å…¨
    4. æ”¯æŒå£°æ˜å¼ç»„åˆ
    """
    
    # === æ ¸å¿ƒæŠ½è±¡æ–¹æ³• ===
    @abstractmethod
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """åŒæ­¥è°ƒç”¨æ¥å£ - æ‰€æœ‰Runnableå¿…é¡»å®ç°"""
        
    # === é»˜è®¤å¼‚æ­¥å®ç° ===
    async def ainvoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """å¼‚æ­¥è°ƒç”¨çš„é»˜è®¤å®ç°"""
        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥æ–¹æ³•
        import asyncio
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.invoke, input, config)
    
    # === æ‰¹å¤„ç†é»˜è®¤å®ç° ===
    def batch(self, inputs: list[Input], 
              config: Optional[Union[RunnableConfig, list[RunnableConfig]]] = None,
              **kwargs) -> list[Output]:
        """æ‰¹å¤„ç†çš„é»˜è®¤å®ç°"""
        if config is None:
            configs = [None] * len(inputs)
        elif isinstance(config, dict):
            configs = [config] * len(inputs)
        else:
            configs = config
            
        # å¹¶å‘æ‰§è¡Œï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(self, '_batch') and callable(self._batch):
            return self._batch(inputs, configs, **kwargs)
        
        # é»˜è®¤é¡ºåºæ‰§è¡Œ
        return [self.invoke(inp, cfg) for inp, cfg in zip(inputs, configs)]
    
    # === æµå¼å¤„ç†é»˜è®¤å®ç° ===
    def stream(self, input: Input, config: Optional[RunnableConfig] = None) -> Iterator[Output]:
        """æµå¼å¤„ç†çš„é»˜è®¤å®ç°"""
        # é»˜è®¤æƒ…å†µä¸‹ï¼Œæµå¼å¤„ç†å°±æ˜¯å•æ¬¡è°ƒç”¨
        yield self.invoke(input, config)
    
    # === ç®¡é“æ“ä½œç¬¦é‡è½½ ===
    def __or__(self, other: Union["Runnable[Output, Other]", 
                                "Callable[[Output], Other]",
                                "Callable[[Iterator[Output]], Iterator[Other]]",
                                dict]) -> "Runnable[Input, Other]":
        """ç®¡é“æ“ä½œç¬¦ |ï¼Œå®ç°é“¾å¼ç»„åˆ"""
        return RunnableSequence(self, coerce_to_runnable(other))
    
    def __ror__(self, other: Union["Runnable[Other, Input]",
                                  "Callable[[Other], Input]",
                                  dict]) -> "Runnable[Other, Output]":
        """åå‘ç®¡é“æ“ä½œç¬¦"""
        return RunnableSequence(coerce_to_runnable(other), self)
```

### ç±»å‹æ¨æ–­æœºåˆ¶

```python
class RunnableSerializable(Runnable[Input, Output]):
    """å¯åºåˆ—åŒ–çš„RunnableåŸºç±»"""
    
    @property
    def InputType(self) -> type[Input]:
        """é€šè¿‡åå°„è·å–è¾“å…¥ç±»å‹"""
        # å¤æ‚çš„ç±»å‹æ¨æ–­é€»è¾‘
        for base in self.__class__.mro():
            if hasattr(base, "__orig_bases__"):
                for orig_base in base.__orig_bases__:
                    if hasattr(orig_base, "__origin__") and orig_base.__origin__ is Runnable:
                        args = orig_base.__args__
                        if len(args) >= 1:
                            return args[0]
        return Any
    
    @property
    def OutputType(self) -> type[Output]:
        """é€šè¿‡åå°„è·å–è¾“å‡ºç±»å‹"""
        # ç±»ä¼¼çš„ç±»å‹æ¨æ–­é€»è¾‘
        for base in self.__class__.mro():
            if hasattr(base, "__orig_bases__"):
                for orig_base in base.__orig_bases__:
                    if hasattr(orig_base, "__origin__") and orig_base.__origin__ is Runnable:
                        args = orig_base.__args__
                        if len(args) >= 2:
                            return args[1]
        return Any
```

### é…ç½®ç³»ç»Ÿè®¾è®¡

```python
from typing import Dict, Any, List, Optional
from pydantic import BaseModel

class RunnableConfig(BaseModel):
    """Runnableçš„è¿è¡Œæ—¶é…ç½®"""
    
    # è¿½è¸ªå’Œç›‘æ§
    tags: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None
    callbacks: Optional[List[BaseCallbackHandler]] = None
    
    # æ‰§è¡Œæ§åˆ¶
    max_concurrency: Optional[int] = None
    recursion_limit: Optional[int] = None
    
    # å¯é…ç½®å‚æ•°
    configurable: Optional[Dict[str, Any]] = None
    
    class Config:
        arbitrary_types_allowed = True

# é…ç½®çš„åˆå¹¶å’Œç»§æ‰¿
def merge_configs(base_config: Optional[RunnableConfig], 
                 override_config: Optional[RunnableConfig]) -> Optional[RunnableConfig]:
    """æ™ºèƒ½åˆå¹¶é…ç½®"""
    if not base_config:
        return override_config
    if not override_config:
        return base_config
    
    # æ·±åº¦åˆå¹¶é…ç½®é¡¹
    merged = base_config.copy(deep=True)
    
    # æ ‡ç­¾åˆå¹¶
    if override_config.tags:
        merged.tags = (merged.tags or []) + override_config.tags
    
    # å…ƒæ•°æ®åˆå¹¶
    if override_config.metadata:
        merged.metadata = {**(merged.metadata or {}), **override_config.metadata}
    
    # å…¶ä»–å­—æ®µè¦†ç›–
    for field, value in override_config.dict(exclude_unset=True).items():
        if field not in ['tags', 'metadata'] and value is not None:
            setattr(merged, field, value)
    
    return merged
```

---

## LCELè¯­æ³•æœºåˆ¶

### ç®¡é“æ“ä½œç¬¦çš„é­”æ³•

LCELçš„æ ¸å¿ƒé­…åŠ›åœ¨äºç®¡é“æ“ä½œç¬¦ï¼ˆ`|`ï¼‰çš„å·§å¦™é‡è½½ï¼Œå®ç°äº†è‡ªç„¶çš„æ•°æ®æµè¡¨è¾¾ï¼š

```python
# æ“ä½œç¬¦é‡è½½çš„å®ç°ç»†èŠ‚
class Runnable(Generic[Input, Output]):
    
    def __or__(self, other):
        """ç®¡é“æ“ä½œç¬¦çš„æ ¸å¿ƒå®ç°"""
        # 1. ç±»å‹å¼ºåˆ¶è½¬æ¢
        other_runnable = self._coerce_to_runnable(other)
        
        # 2. åˆ›å»ºåºåˆ—ç»„åˆ
        return RunnableSequence(first=self, last=other_runnable)
    
    def _coerce_to_runnable(self, other) -> "Runnable":
        """å°†å„ç§è¾“å…¥è½¬æ¢ä¸ºRunnable"""
        
        # å­—å…¸ -> RunnableParallel
        if isinstance(other, dict):
            return RunnableParallel(other)
        
        # å‡½æ•° -> RunnableLambda
        if callable(other):
            return RunnableLambda(other)
        
        # å·²ç»æ˜¯Runnable
        if isinstance(other, Runnable):
            return other
        
        # å…¶ä»–æƒ…å†µ -> RunnablePassthrough
        return RunnablePassthrough.assign(**{str(other): other})

# å¤æ‚ç»„åˆçš„å®é™…å®ç°
chain = (
    # å­—å…¸è‡ªåŠ¨è½¬æ¢ä¸ºRunnableParallel
    {"context": retriever, "question": RunnablePassthrough()}
    # å‡½æ•°è‡ªåŠ¨è½¬æ¢ä¸ºRunnableLambda  
    | lambda x: format_context(x)
    # å·²æœ‰çš„Runnableç›´æ¥ä½¿ç”¨
    | prompt_template
    | llm
    | output_parser
)
```

### RunnableSequenceï¼šé¡ºåºç»„åˆçš„æ ¸å¿ƒ

```python
class RunnableSequence(RunnableSerializable[Input, Output]):
    """é¡ºåºæ‰§è¡Œçš„Runnableç»„åˆ"""
    
    def __init__(self, *steps: Runnable):
        # æ‰å¹³åŒ–å¤„ç†ï¼šé¿å…åµŒå¥—çš„RunnableSequence
        self.steps: List[Runnable] = []
        for step in steps:
            if isinstance(step, RunnableSequence):
                self.steps.extend(step.steps)
            else:
                self.steps.append(step)
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """é¡ºåºæ‰§è¡Œæ‰€æœ‰æ­¥éª¤"""
        # é…ç½®åˆ†å‘åˆ°æ¯ä¸ªæ­¥éª¤
        step_configs = self._get_step_configs(config)
        
        # é“¾å¼æ‰§è¡Œ
        result = input
        for i, step in enumerate(self.steps):
            result = step.invoke(result, step_configs[i])
        
        return result
    
    async def ainvoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """å¼‚æ­¥é¡ºåºæ‰§è¡Œ"""
        step_configs = self._get_step_configs(config)
        
        result = input
        for i, step in enumerate(self.steps):
            result = await step.ainvoke(result, step_configs[i])
        
        return result
    
    def batch(self, inputs: List[Input], 
              config: Optional[RunnableConfig] = None) -> List[Output]:
        """æ‰¹å¤„ç†ä¼˜åŒ–"""
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ­¥éª¤éƒ½æ”¯æŒæ‰¹å¤„ç†
        if all(hasattr(step, 'batch') for step in self.steps):
            # æ‰¹å¤„ç†ç®¡é“
            results = inputs
            for step in self.steps:
                results = step.batch(results, config)
            return results
        
        # å›é€€åˆ°å•ç‹¬å¤„ç†
        return [self.invoke(inp, config) for inp in inputs]
    
    def stream(self, input: Input, config: Optional[RunnableConfig] = None) -> Iterator[Output]:
        """æµå¼å¤„ç†"""
        # æ£€æŸ¥æœ€åä¸€æ­¥æ˜¯å¦æ”¯æŒæµå¼å¤„ç†
        if hasattr(self.steps[-1], 'stream'):
            # æ‰§è¡Œå‰é¢çš„æ­¥éª¤
            intermediate_result = input
            for step in self.steps[:-1]:
                intermediate_result = step.invoke(intermediate_result, config)
            
            # æµå¼æ‰§è¡Œæœ€åä¸€æ­¥
            yield from self.steps[-1].stream(intermediate_result, config)
        else:
            # å›é€€åˆ°å•æ¬¡è°ƒç”¨
            yield self.invoke(input, config)
```

### RunnableParallelï¼šå¹¶è¡Œç»„åˆçš„å®ç°

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio

class RunnableParallel(RunnableSerializable[Input, Dict[str, Any]]):
    """å¹¶è¡Œæ‰§è¡Œçš„Runnableç»„åˆ"""
    
    def __init__(self, steps: Union[Dict[str, Runnable], List[Runnable]]):
        if isinstance(steps, dict):
            self.steps = steps
        else:
            # åˆ—è¡¨è½¬æ¢ä¸ºç´¢å¼•å­—å…¸
            self.steps = {str(i): step for i, step in enumerate(steps)}
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ­¥éª¤"""
        # ç¡®å®šå¹¶å‘æ•°
        max_concurrency = (config and config.max_concurrency) or len(self.steps)
        
        with ThreadPoolExecutor(max_workers=max_concurrency) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_key = {
                executor.submit(step.invoke, input, config): key
                for key, step in self.steps.items()
            }
            
            # æ”¶é›†ç»“æœ
            results = {}
            for future in as_completed(future_to_key):
                key = future_to_key[future]
                try:
                    results[key] = future.result()
                except Exception as exc:
                    # é”™è¯¯å¤„ç†
                    results[key] = self._handle_error(key, exc, input, config)
            
            return results
    
    async def ainvoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
        """å¼‚æ­¥å¹¶è¡Œæ‰§è¡Œ"""
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = {
            key: asyncio.create_task(step.ainvoke(input, config))
            for key, step in self.steps.items()
        }
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = {}
        for key, task in tasks.items():
            try:
                results[key] = await task
            except Exception as exc:
                results[key] = self._handle_error(key, exc, input, config)
        
        return results
    
    def batch(self, inputs: List[Input], config: Optional[RunnableConfig] = None) -> List[Dict[str, Any]]:
        """æ‰¹å¤„ç†ä¼˜åŒ–"""
        # å¹¶è¡Œæ‰§è¡Œæ¯ä¸ªæ­¥éª¤çš„æ‰¹å¤„ç†
        step_results = {}
        with ThreadPoolExecutor() as executor:
            future_to_key = {
                executor.submit(step.batch, inputs, config): key
                for key, step in self.steps.items()
            }
            
            for future in as_completed(future_to_key):
                key = future_to_key[future]
                step_results[key] = future.result()
        
        # é‡æ–°ç»„ç»‡ç»“æœ
        return [
            {key: step_results[key][i] for key in self.steps.keys()}
            for i in range(len(inputs))
        ]
```

---

## æ ¸å¿ƒå®ç°ç»„ä»¶

### RunnableLambdaï¼šå‡½æ•°é€‚é…å™¨

```python
class RunnableLambda(RunnableSerializable[Input, Output]):
    """å°†Pythonå‡½æ•°è½¬æ¢ä¸ºRunnable"""
    
    def __init__(self, func: Callable[[Input], Output], 
                 afunc: Optional[Callable[[Input], Awaitable[Output]]] = None):
        self.func = func
        self.afunc = afunc
        
        # å‡½æ•°ç­¾ååˆ†æ
        self.signature = inspect.signature(func)
        self.takes_config = 'config' in self.signature.parameters
        self.takes_run_manager = any(
            param.annotation and 'CallbackManagerForChainRun' in str(param.annotation)
            for param in self.signature.parameters.values()
        )
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """è°ƒç”¨åŒ…è£…çš„å‡½æ•°"""
        kwargs = {}
        
        # åŠ¨æ€å‚æ•°æ³¨å…¥
        if self.takes_config:
            kwargs['config'] = config
        
        if self.takes_run_manager and config and config.callbacks:
            from langchain_core.callbacks import CallbackManagerForChainRun
            kwargs['run_manager'] = CallbackManagerForChainRun.on_chain_start(
                {"name": self.__class__.__name__}, 
                input,
                callbacks=config.callbacks
            )
        
        try:
            result = self.func(input, **kwargs)
            return result
        except Exception as e:
            # é”™è¯¯å¤„ç†å’Œè¿½è¸ª
            if 'run_manager' in kwargs:
                kwargs['run_manager'].on_chain_error(e)
            raise
    
    async def ainvoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """å¼‚æ­¥è°ƒç”¨"""
        if self.afunc:
            # ä½¿ç”¨æä¾›çš„å¼‚æ­¥å‡½æ•°
            return await self.afunc(input)
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥å‡½æ•°
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.invoke, input, config)

# ä½¿ç”¨ç¤ºä¾‹
def custom_transform(data: dict) -> dict:
    """è‡ªå®šä¹‰æ•°æ®è½¬æ¢å‡½æ•°"""
    return {
        "processed": True,
        "original_keys": list(data.keys()),
        "transformed_data": {k: v.upper() if isinstance(v, str) else v 
                           for k, v in data.items()}
    }

# è‡ªåŠ¨è½¬æ¢ä¸ºRunnable
transform_runnable = RunnableLambda(custom_transform)

# åœ¨é“¾ä¸­ä½¿ç”¨
chain = input_validator | transform_runnable | output_formatter
```

### RunnablePassthroughï¼šæ•°æ®ä¼ é€’å’Œä¿®æ”¹

```python
class RunnablePassthrough(RunnableSerializable[Input, Input]):
    """æ•°æ®ä¼ é€’ç»„ä»¶ï¼Œæ”¯æŒæ•°æ®ä¿®æ”¹å’Œåˆ†å‘"""
    
    def __init__(self, func: Optional[Callable[[Input], Input]] = None):
        self.func = func
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Input:
        """ä¼ é€’è¾“å…¥ï¼Œå¯é€‰åœ°è¿›è¡Œè½¬æ¢"""
        if self.func:
            return self.func(input)
        return input
    
    @classmethod
    def assign(cls, **kwargs: Union[Runnable, Callable]) -> "RunnableAssign":
        """åˆ›å»ºæ•°æ®åˆ†é…å™¨"""
        return RunnableAssign(RunnableParallel(kwargs))

class RunnableAssign(RunnableSerializable[Dict, Dict]):
    """å°†æ–°å­—æ®µåˆ†é…ç»™è¾“å…¥å­—å…¸"""
    
    def __init__(self, mapper: RunnableParallel):
        self.mapper = mapper
    
    def invoke(self, input: Dict, config: Optional[RunnableConfig] = None) -> Dict:
        """æ‰§è¡Œå­—æ®µåˆ†é…"""
        # å¹¶è¡Œæ‰§è¡Œæ˜ å°„
        mapped = self.mapper.invoke(input, config)
        
        # åˆå¹¶åˆ°åŸå§‹è¾“å…¥
        if isinstance(input, dict):
            return {**input, **mapped}
        else:
            # éå­—å…¸è¾“å…¥ï¼Œè¿”å›æ˜ å°„ç»“æœ
            return mapped

# å®é™…ä½¿ç”¨
chain = (
    RunnablePassthrough.assign(
        # å¼‚æ­¥è·å–ä¸Šä¸‹æ–‡
        context=lambda x: retriever.invoke(x["question"]),
        # è·å–ç”¨æˆ·å†å²
        history=lambda x: get_chat_history(x["session_id"]),
        # å¤„ç†æ—¶é—´æˆ³
        timestamp=lambda x: datetime.now().isoformat()
    )
    | prompt_template
    | llm
    | output_parser
)
```

### RunnableBranchï¼šæ¡ä»¶è·¯ç”±

```python
from typing import List, Tuple, Union

class RunnableBranch(RunnableSerializable[Input, Output]):
    """æ¡ä»¶åˆ†æ”¯è·¯ç”±å™¨"""
    
    def __init__(self, *branches: Union[
        Tuple[Callable[[Input], bool], Runnable[Input, Output]],
        Runnable[Input, Output]  # é»˜è®¤åˆ†æ”¯
    ]):
        self.branches: List[Tuple[Callable[[Input], bool], Runnable[Input, Output]]] = []
        self.default: Optional[Runnable[Input, Output]] = None
        
        for branch in branches:
            if isinstance(branch, tuple) and len(branch) == 2:
                condition, runnable = branch
                self.branches.append((condition, runnable))
            else:
                # æœ€åä¸€ä¸ªå‚æ•°ä½œä¸ºé»˜è®¤åˆ†æ”¯
                self.default = branch
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """æ ¹æ®æ¡ä»¶é€‰æ‹©åˆ†æ”¯æ‰§è¡Œ"""
        # æ£€æŸ¥æ‰€æœ‰æ¡ä»¶
        for condition, runnable in self.branches:
            try:
                if condition(input):
                    return runnable.invoke(input, config)
            except Exception as e:
                # æ¡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œè®°å½•ä½†ç»§ç»­
                if config and config.callbacks:
                    for callback in config.callbacks:
                        if hasattr(callback, 'on_condition_error'):
                            callback.on_condition_error(e, condition, input)
        
        # æ‰§è¡Œé»˜è®¤åˆ†æ”¯
        if self.default:
            return self.default.invoke(input, config)
        
        # æ²¡æœ‰åŒ¹é…çš„åˆ†æ”¯
        raise ValueError(f"No branch matched input: {input}")

# ä½¿ç”¨ç¤ºä¾‹
intelligent_router = RunnableBranch(
    # ä»£ç ç”Ÿæˆåˆ†æ”¯
    (lambda x: "code" in x["input"].lower() or "function" in x["input"].lower(),
     code_generation_chain),
    
    # æ•°å­¦è®¡ç®—åˆ†æ”¯
    (lambda x: any(op in x["input"] for op in ["+", "-", "*", "/", "calculate"]),
     math_solving_chain),
    
    # æœç´¢æŸ¥è¯¢åˆ†æ”¯
    (lambda x: "search" in x["input"].lower() or "find" in x["input"].lower(),
     search_chain),
    
    # é»˜è®¤å¯¹è¯åˆ†æ”¯
    general_conversation_chain
)
```

---

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### æ‰¹å¤„ç†ä¼˜åŒ–

```python
class BatchOptimizedRunnable(RunnableSerializable[Input, Output]):
    """æ‰¹å¤„ç†ä¼˜åŒ–çš„Runnableå®ç°"""
    
    def __init__(self, base_runnable: Runnable[Input, Output], 
                 batch_size: int = 10,
                 enable_dynamic_batching: bool = True):
        self.base_runnable = base_runnable
        self.batch_size = batch_size
        self.enable_dynamic_batching = enable_dynamic_batching
        
        # åŠ¨æ€æ‰¹å¤„ç†çŠ¶æ€
        self._pending_inputs: List[Input] = []
        self._pending_futures: List[concurrent.futures.Future] = []
        self._batch_timer: Optional[threading.Timer] = None
        self._lock = threading.Lock()
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """æ™ºèƒ½æ‰¹å¤„ç†è°ƒç”¨"""
        if not self.enable_dynamic_batching:
            return self.base_runnable.invoke(input, config)
        
        # åŠ¨æ€æ‰¹å¤„ç†é€»è¾‘
        with self._lock:
            future = concurrent.futures.Future()
            self._pending_inputs.append(input)
            self._pending_futures.append(future)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ‰¹å¤„ç†æ¡ä»¶
            if len(self._pending_inputs) >= self.batch_size:
                self._execute_batch()
            else:
                # è®¾ç½®å®šæ—¶å™¨ç¡®ä¿åŠæ—¶å¤„ç†
                if self._batch_timer is None:
                    self._batch_timer = threading.Timer(0.1, self._execute_batch)
                    self._batch_timer.start()
        
        return future.result()  # é˜»å¡ç­‰å¾…ç»“æœ
    
    def _execute_batch(self):
        """æ‰§è¡Œæ‰¹å¤„ç†"""
        with self._lock:
            if not self._pending_inputs:
                return
            
            inputs = self._pending_inputs[:]
            futures = self._pending_futures[:]
            
            # æ¸…ç©ºå¾…å¤„ç†åˆ—è¡¨
            self._pending_inputs.clear()
            self._pending_futures.clear()
            
            # å–æ¶ˆå®šæ—¶å™¨
            if self._batch_timer:
                self._batch_timer.cancel()
                self._batch_timer = None
        
        try:
            # æ‰§è¡Œæ‰¹å¤„ç†
            results = self.base_runnable.batch(inputs)
            
            # åˆ†å‘ç»“æœ
            for future, result in zip(futures, results):
                future.set_result(result)
                
        except Exception as e:
            # é”™è¯¯å¤„ç†
            for future in futures:
                future.set_exception(e)
```

### ç¼“å­˜æœºåˆ¶

```python
from functools import lru_cache
import hashlib
import pickle
from typing import Hashable

class CachedRunnable(RunnableSerializable[Input, Output]):
    """å¸¦ç¼“å­˜çš„RunnableåŒ…è£…å™¨"""
    
    def __init__(self, runnable: Runnable[Input, Output],
                 cache_size: int = 128,
                 ttl: Optional[float] = None,
                 key_func: Optional[Callable[[Input], str]] = None):
        self.runnable = runnable
        self.cache_size = cache_size
        self.ttl = ttl
        self.key_func = key_func or self._default_key_func
        
        # åˆå§‹åŒ–ç¼“å­˜
        self._cache: Dict[str, Tuple[Output, float]] = {}
        self._access_order: List[str] = []
        self._lock = threading.RLock()
    
    def _default_key_func(self, input: Input) -> str:
        """é»˜è®¤çš„é”®ç”Ÿæˆå‡½æ•°"""
        try:
            # å°è¯•ä½¿ç”¨å“ˆå¸Œ
            if isinstance(input, Hashable):
                return str(hash(input))
            
            # ä½¿ç”¨åºåˆ—åŒ–
            serialized = pickle.dumps(input, protocol=pickle.HIGHEST_PROTOCOL)
            return hashlib.md5(serialized).hexdigest()
            
        except Exception:
            # å›é€€åˆ°å­—ç¬¦ä¸²è¡¨ç¤º
            return str(input)
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        """å¸¦ç¼“å­˜çš„è°ƒç”¨"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.key_func(input)
        current_time = time.time()
        
        with self._lock:
            # æ£€æŸ¥ç¼“å­˜
            if cache_key in self._cache:
                cached_result, timestamp = self._cache[cache_key]
                
                # æ£€æŸ¥TTL
                if self.ttl is None or (current_time - timestamp) < self.ttl:
                    # æ›´æ–°è®¿é—®é¡ºåº
                    if cache_key in self._access_order:
                        self._access_order.remove(cache_key)
                    self._access_order.append(cache_key)
                    
                    return cached_result
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜
                    del self._cache[cache_key]
                    if cache_key in self._access_order:
                        self._access_order.remove(cache_key)
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®é™…è°ƒç”¨
        result = self.runnable.invoke(input, config)
        
        # æ›´æ–°ç¼“å­˜
        with self._lock:
            # LRUæ·˜æ±°
            if len(self._cache) >= self.cache_size:
                # æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
                oldest_key = self._access_order.pop(0)
                del self._cache[oldest_key]
            
            # æ·»åŠ æ–°ç»“æœ
            self._cache[cache_key] = (result, current_time)
            self._access_order.append(cache_key)
        
        return result
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            self._access_order.clear()
```

### æµå¼å¤„ç†ä¼˜åŒ–

```python
class StreamOptimizedRunnable(RunnableSerializable[Input, Output]):
    """æµå¼å¤„ç†ä¼˜åŒ–çš„Runnable"""
    
    def __init__(self, runnable: Runnable[Input, Output],
                 buffer_size: int = 1024,
                 flush_interval: float = 0.1):
        self.runnable = runnable
        self.buffer_size = buffer_size
        self.flush_interval = flush_interval
    
    def stream(self, input: Input, config: Optional[RunnableConfig] = None) -> Iterator[Output]:
        """ä¼˜åŒ–çš„æµå¼å¤„ç†"""
        
        # æ£€æŸ¥åº•å±‚Runnableæ˜¯å¦æ”¯æŒåŸç”Ÿæµå¼å¤„ç†
        if hasattr(self.runnable, 'stream') and callable(self.runnable.stream):
            # ä½¿ç”¨åº•å±‚çš„æµå¼å¤„ç†ï¼Œä½†æ·»åŠ ç¼“å†²
            buffer = []
            last_flush = time.time()
            
            for chunk in self.runnable.stream(input, config):
                buffer.append(chunk)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ç¼“å†²åŒº
                if (len(buffer) >= self.buffer_size or 
                    time.time() - last_flush >= self.flush_interval):
                    
                    # æ‰¹é‡è¾“å‡º
                    for buffered_chunk in buffer:
                        yield buffered_chunk
                    
                    buffer.clear()
                    last_flush = time.time()
            
            # è¾“å‡ºå‰©ä½™çš„ç¼“å†²åŒºå†…å®¹
            for buffered_chunk in buffer:
                yield buffered_chunk
        
        else:
            # æ¨¡æ‹Ÿæµå¼å¤„ç†
            result = self.runnable.invoke(input, config)
            
            # å¦‚æœç»“æœæ˜¯å­—ç¬¦ä¸²ï¼ŒæŒ‰å­—ç¬¦æµå¼è¾“å‡º
            if isinstance(result, str):
                for char in result:
                    yield char
                    time.sleep(0.01)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
            else:
                yield result

    async def astream(self, input: Input, config: Optional[RunnableConfig] = None) -> AsyncIterator[Output]:
        """å¼‚æ­¥æµå¼å¤„ç†"""
        if hasattr(self.runnable, 'astream') and callable(self.runnable.astream):
            async for chunk in self.runnable.astream(input, config):
                yield chunk
        else:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæµå¼å¤„ç†
            loop = asyncio.get_event_loop()
            
            # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œå¼‚æ­¥é€šä¿¡
            queue = asyncio.Queue()
            
            def stream_worker():
                try:
                    for chunk in self.stream(input, config):
                        asyncio.run_coroutine_threadsafe(
                            queue.put(chunk), loop
                        ).result()
                    asyncio.run_coroutine_threadsafe(
                        queue.put(None), loop  # ç»“æŸæ ‡è®°
                    ).result()
                except Exception as e:
                    asyncio.run_coroutine_threadsafe(
                        queue.put(e), loop
                    ).result()
            
            # åœ¨çº¿ç¨‹æ± ä¸­å¯åŠ¨worker
            executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            executor.submit(stream_worker)
            
            # å¼‚æ­¥è¯»å–ç»“æœ
            while True:
                chunk = await queue.get()
                if chunk is None:  # ç»“æŸæ ‡è®°
                    break
                if isinstance(chunk, Exception):
                    raise chunk
                yield chunk
```

---

## å®é™…åº”ç”¨æ¨¡å¼

### å¤æ‚RAGåº”ç”¨

```python
# ä¼ä¸šçº§RAGåº”ç”¨çš„å®Œæ•´å®ç°
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def build_advanced_rag_chain():
    """æ„å»ºé«˜çº§RAGé“¾"""
    
    # 1. å¤šæºæ£€ç´¢å™¨
    vector_retriever = Chroma(...).as_retriever()
    keyword_retriever = BM25Retriever(...)
    
    # 2. æ··åˆæ£€ç´¢
    hybrid_retriever = RunnableParallel({
        "vector_docs": vector_retriever,
        "keyword_docs": keyword_retriever
    }) | RunnableLambda(merge_and_rerank_docs)
    
    # 3. ä¸Šä¸‹æ–‡å¢å¼º
    context_enhancer = RunnableParallel({
        "relevant_docs": hybrid_retriever,
        "chat_history": lambda x: get_chat_history(x.get("session_id", "")),
        "user_profile": lambda x: get_user_profile(x.get("user_id", "")),
        "query_expansion": query_expansion_chain
    })
    
    # 4. æç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ç›¸å…³æ–‡æ¡£: {relevant_docs}
èŠå¤©å†å²: {chat_history}  
ç”¨æˆ·æ¡£æ¡ˆ: {user_profile}
æŸ¥è¯¢æ‰©å±•: {query_expansion}

è¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"""),
        ("human", "{question}")
    ])
    
    # 5. LLMé…ç½®
    llm = ChatOpenAI(
        model="gpt-4",
        temperature=0.1,
        streaming=True
    )
    
    # 6. è¾“å‡ºå¤„ç†
    output_processor = RunnableParallel({
        "answer": StrOutputParser(),
        "sources": lambda x: extract_sources(x),
        "confidence": lambda x: calculate_confidence(x),
        "follow_up": lambda x: generate_follow_up_questions(x)
    })
    
    # 7. ç»„è£…å®Œæ•´é“¾
    rag_chain = (
        RunnablePassthrough.assign(**context_enhancer)
        | prompt
        | llm
        | output_processor
    )
    
    return rag_chain

# ä½¿ç”¨
rag_chain = build_advanced_rag_chain()
result = rag_chain.invoke({
    "question": "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ",
    "session_id": "user_123_session_456",
    "user_id": "user_123"
})
```

### å¤šæ¨¡æ€å¤„ç†é“¾

```python
from langchain_core.runnables import RunnableParallel
import base64
from PIL import Image
import io

class MultimodalChain:
    """å¤šæ¨¡æ€å¤„ç†é“¾"""
    
    def __init__(self):
        self.image_analyzer = self._build_image_analyzer()
        self.text_processor = self._build_text_processor()
        self.fusion_layer = self._build_fusion_layer()
    
    def _build_image_analyzer(self):
        """æ„å»ºå›¾åƒåˆ†æé“¾"""
        return RunnableParallel({
            "image_description": image_to_text_model,
            "objects_detected": object_detection_model,
            "scene_analysis": scene_classification_model,
            "text_in_image": ocr_model
        })
    
    def _build_text_processor(self):
        """æ„å»ºæ–‡æœ¬å¤„ç†é“¾"""
        return RunnableParallel({
            "intent_analysis": intent_classifier,
            "entity_extraction": ner_model,
            "sentiment": sentiment_analyzer,
            "topic_classification": topic_classifier
        })
    
    def _build_fusion_layer(self):
        """æ„å»ºå¤šæ¨¡æ€èåˆå±‚"""
        
        fusion_prompt = ChatPromptTemplate.from_messages([
            ("system", """åŸºäºä»¥ä¸‹å¤šæ¨¡æ€åˆ†æç»“æœï¼Œæä¾›ç»¼åˆæ€§å›ç­”ï¼š

å›¾åƒåˆ†æç»“æœ:
- æè¿°: {image_description}
- æ£€æµ‹åˆ°çš„å¯¹è±¡: {objects_detected}
- åœºæ™¯åˆ†æ: {scene_analysis}
- å›¾ä¸­æ–‡å­—: {text_in_image}

æ–‡æœ¬åˆ†æç»“æœ:
- ç”¨æˆ·æ„å›¾: {intent_analysis}
- å®ä½“æå–: {entity_extraction}
- æƒ…æ„Ÿåˆ†æ: {sentiment}
- ä¸»é¢˜åˆ†ç±»: {topic_classification}

è¯·ç»“åˆå›¾åƒå’Œæ–‡æœ¬ä¿¡æ¯ï¼Œæä¾›å‡†ç¡®ã€å…¨é¢çš„å›ç­”ã€‚"""),
            ("human", "{user_query}")
        ])
        
        return (
            fusion_prompt
            | ChatOpenAI(model="gpt-4-vision-preview")
            | StrOutputParser()
        )
    
    def build_chain(self):
        """æ„å»ºå®Œæ•´çš„å¤šæ¨¡æ€å¤„ç†é“¾"""
        
        def process_multimodal_input(input_data):
            """å¤„ç†å¤šæ¨¡æ€è¾“å…¥"""
            # åˆ†ç¦»å›¾åƒå’Œæ–‡æœ¬
            image_data = input_data.get("image")
            text_query = input_data.get("text", "")
            
            # å¹¶è¡Œå¤„ç†
            image_results = self.image_analyzer.invoke(image_data) if image_data else {}
            text_results = self.text_processor.invoke(text_query) if text_query else {}
            
            # åˆå¹¶ç»“æœ
            return {
                **image_results,
                **text_results,
                "user_query": text_query
            }
        
        return (
            RunnableLambda(process_multimodal_input)
            | self.fusion_layer
        )

# ä½¿ç”¨ç¤ºä¾‹
multimodal_chain = MultimodalChain().build_chain()

# å¤„ç†åŒ…å«å›¾åƒå’Œæ–‡æœ¬çš„è¾“å…¥
result = multimodal_chain.invoke({
    "image": base64_encoded_image,
    "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿèƒ½è¯¦ç»†æè¿°ä¸€ä¸‹å—ï¼Ÿ"
})
```

---

## æœ€ä½³å®è·µæŒ‡å—

### 1. æ€§èƒ½ä¼˜åŒ–å»ºè®®

```python
# æ€§èƒ½ä¼˜åŒ–çš„æœ€ä½³å®è·µ

# âœ… æ­£ç¡®ï¼šä½¿ç”¨æ‰¹å¤„ç†
def efficient_processing(documents: List[str]):
    # æ‰¹é‡åµŒå…¥
    embeddings = embedding_model.embed_documents(documents)
    
    # æ‰¹é‡LLMè°ƒç”¨
    summaries = summarization_chain.batch([{"text": doc} for doc in documents])
    
    return list(zip(documents, embeddings, summaries))

# âŒ é”™è¯¯ï¼šé€ä¸ªå¤„ç†
def inefficient_processing(documents: List[str]):
    results = []
    for doc in documents:
        embedding = embedding_model.embed_query(doc)  # å•ä¸ªè°ƒç”¨
        summary = summarization_chain.invoke({"text": doc})  # å•ä¸ªè°ƒç”¨
        results.append((doc, embedding, summary))
    return results

# âœ… æ­£ç¡®ï¼šä½¿ç”¨ç¼“å­˜
cached_chain = RunnableLambda(expensive_operation).with_config(
    tags=["cached"]
) | CachedRunnable(llm, cache_size=1000, ttl=3600)

# âœ… æ­£ç¡®ï¼šæµå¼å¤„ç†é•¿å†…å®¹
def stream_long_content(content: str):
    """æµå¼å¤„ç†é•¿å†…å®¹"""
    streaming_chain = (
        chunk_splitter
        | RunnableParallel({
            "chunk": RunnablePassthrough(),
            "context": retriever
        })
        | prompt_template
        | ChatOpenAI(streaming=True)
        | StrOutputParser()
    )
    
    for chunk in streaming_chain.stream({"content": content}):
        yield chunk
```

### 2. é”™è¯¯å¤„ç†æ¨¡å¼

```python
from langchain_core.runnables import RunnableWithFallbacks
import logging

# é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥
def build_robust_chain():
    """æ„å»ºå…·æœ‰å®¹é”™èƒ½åŠ›çš„é“¾"""
    
    # ä¸»è¦å¤„ç†é“¾
    primary_chain = (
        input_validator
        | expensive_but_accurate_model
        | output_formatter
    )
    
    # å¤‡ç”¨å¤„ç†é“¾
    fallback_chain = (
        simple_input_processor
        | cheaper_model
        | basic_formatter
    )
    
    # æœ€ç»ˆå¤‡ç”¨
    emergency_fallback = RunnableLambda(
        lambda x: {"result": "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"}
    )
    
    # ç»„åˆä¸ºå®¹é”™é“¾
    robust_chain = RunnableWithFallbacks(
        primary_chain,
        fallbacks=[fallback_chain, emergency_fallback]
    )
    
    return robust_chain

# è‡ªå®šä¹‰é”™è¯¯å¤„ç†
class ErrorHandlingRunnable(RunnableSerializable[Input, Output]):
    """å¸¦æœ‰è¯¦ç»†é”™è¯¯å¤„ç†çš„Runnable"""
    
    def __init__(self, base_runnable: Runnable[Input, Output]):
        self.base_runnable = base_runnable
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def invoke(self, input: Input, config: Optional[RunnableConfig] = None) -> Output:
        try:
            return self.base_runnable.invoke(input, config)
        
        except ValidationError as e:
            self.logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {e}")
            raise ValueError(f"è¾“å…¥æ•°æ®æ ¼å¼é”™è¯¯: {str(e)}")
        
        except RateLimitError as e:
            self.logger.warning(f"é‡åˆ°é€Ÿç‡é™åˆ¶: {e}")
            time.sleep(e.retry_after or 60)
            return self.base_runnable.invoke(input, config)
        
        except Exception as e:
            self.logger.error(f"æœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            # è¿”å›å®‰å…¨çš„é»˜è®¤å€¼æˆ–é‡æ–°æŠ›å‡º
            if hasattr(e, 'recoverable') and e.recoverable:
                return self._get_safe_default(input)
            raise
    
    def _get_safe_default(self, input: Input) -> Output:
        """è¿”å›å®‰å…¨çš„é»˜è®¤å€¼"""
        return {"error": "å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥å¹¶é‡è¯•"}
```

### 3. ç›‘æ§å’Œè°ƒè¯•

```python
from langchain_core.callbacks import StdOutCallbackHandler
from langchain_core.tracers import ConsoleCallbackHandler

def setup_monitoring_chain():
    """è®¾ç½®ç›‘æ§å’Œè°ƒè¯•"""
    
    # è‡ªå®šä¹‰å›è°ƒå¤„ç†å™¨
    class ProductionCallbackHandler(BaseCallbackHandler):
        def __init__(self, metrics_client):
            self.metrics = metrics_client
        
        def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):
            self.metrics.increment('chain.start')
            self.metrics.histogram('chain.input_length', len(str(inputs)))
        
        def on_chain_end(self, outputs: Dict[str, Any], **kwargs):
            self.metrics.increment('chain.success')
            self.metrics.histogram('chain.output_length', len(str(outputs)))
        
        def on_chain_error(self, error: Exception, **kwargs):
            self.metrics.increment('chain.error')
            self.metrics.increment(f'chain.error.{error.__class__.__name__}')
    
    # é…ç½®ç›‘æ§
    monitored_chain = base_chain.with_config(
        callbacks=[
            ProductionCallbackHandler(metrics_client),
            StdOutCallbackHandler(),  # å¼€å‘ç¯å¢ƒ
        ],
        tags=["production", "monitored"],
        metadata={
            "version": "1.0",
            "environment": "prod"
        }
    )
    
    return monitored_chain

# è°ƒè¯•æ¨¡å¼
def debug_chain_execution(chain: Runnable, input_data: Any):
    """è°ƒè¯•é“¾çš„æ‰§è¡Œè¿‡ç¨‹"""
    
    debug_config = RunnableConfig(
        callbacks=[ConsoleCallbackHandler()],
        tags=["debug"],
        metadata={"debug_mode": True}
    )
    
    print(f"ğŸ” å¼€å§‹è°ƒè¯•é“¾æ‰§è¡Œ")
    print(f"ğŸ“¥ è¾“å…¥æ•°æ®: {input_data}")
    
    try:
        result = chain.invoke(input_data, debug_config)
        print(f"âœ… æ‰§è¡ŒæˆåŠŸ")
        print(f"ğŸ“¤ è¾“å‡ºç»“æœ: {result}")
        return result
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        print(f"ğŸ” é”™è¯¯ç±»å‹: {type(e).__name__}")
        raise
```

### 4. æµ‹è¯•ç­–ç•¥

```python
import pytest
from unittest.mock import Mock, patch

class TestChainImplementation:
    """Chainå®ç°çš„æµ‹è¯•ç­–ç•¥"""
    
    @pytest.fixture
    def mock_llm(self):
        """æ¨¡æ‹ŸLLM"""
        llm = Mock()
        llm.invoke.return_value = "æµ‹è¯•è¾“å‡º"
        llm.batch.return_value = ["è¾“å‡º1", "è¾“å‡º2", "è¾“å‡º3"]
        return llm
    
    @pytest.fixture
    def test_chain(self, mock_llm):
        """æµ‹è¯•ç”¨çš„é“¾"""
        return prompt_template | mock_llm | output_parser
    
    def test_basic_invoke(self, test_chain):
        """æµ‹è¯•åŸºæœ¬è°ƒç”¨"""
        result = test_chain.invoke({"input": "æµ‹è¯•è¾“å…¥"})
        assert result is not None
        assert isinstance(result, str)
    
    def test_batch_processing(self, test_chain):
        """æµ‹è¯•æ‰¹å¤„ç†"""
        inputs = [{"input": f"è¾“å…¥{i}"} for i in range(3)]
        results = test_chain.batch(inputs)
        
        assert len(results) == 3
        assert all(isinstance(r, str) for r in results)
    
    def test_streaming(self, test_chain):
        """æµ‹è¯•æµå¼å¤„ç†"""
        results = list(test_chain.stream({"input": "æµ‹è¯•è¾“å…¥"}))
        assert len(results) > 0
    
    @patch('your_module.external_api_call')
    def test_external_integration(self, mock_api, test_chain):
        """æµ‹è¯•å¤–éƒ¨APIé›†æˆ"""
        mock_api.return_value = {"status": "success"}
        
        result = test_chain.invoke({"input": "APIæµ‹è¯•"})
        mock_api.assert_called_once()
        assert result is not None
    
    def test_error_handling(self, test_chain):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        # æµ‹è¯•æ— æ•ˆè¾“å…¥
        with pytest.raises(ValidationError):
            test_chain.invoke(None)
        
        # æµ‹è¯•ç½‘ç»œé”™è¯¯
        with patch('your_module.llm') as mock_llm:
            mock_llm.invoke.side_effect = ConnectionError("ç½‘ç»œé”™è¯¯")
            
            with pytest.raises(ConnectionError):
                test_chain.invoke({"input": "æµ‹è¯•"})

# æ€§èƒ½æµ‹è¯•
import time
import statistics

def benchmark_chain_performance(chain: Runnable, test_inputs: List[Any], iterations: int = 100):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    print(f"ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•: {iterations} æ¬¡è¿­ä»£")
    
    # å•ä¸ªè°ƒç”¨æ€§èƒ½
    single_times = []
    for _ in range(iterations):
        start_time = time.time()
        chain.invoke(test_inputs[0])
        single_times.append(time.time() - start_time)
    
    # æ‰¹å¤„ç†æ€§èƒ½
    batch_start = time.time()
    chain.batch(test_inputs)
    batch_time = time.time() - batch_start
    
    # ç»“æœåˆ†æ
    print(f"ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"   å•æ¬¡è°ƒç”¨å¹³å‡æ—¶é—´: {statistics.mean(single_times):.3f}s")
    print(f"   å•æ¬¡è°ƒç”¨ä¸­ä½æ•°: {statistics.median(single_times):.3f}s")
    print(f"   æ‰¹å¤„ç†æ€»æ—¶é—´: {batch_time:.3f}s")
    print(f"   æ‰¹å¤„ç†å¹³å‡æ¯ä¸ª: {batch_time / len(test_inputs):.3f}s")
    print(f"   æ‰¹å¤„ç†æ•ˆç‡æå‡: {(statistics.mean(single_times) * len(test_inputs) / batch_time):.2f}x")
```

---

## æ€»ç»“

LangChainçš„Chainsç»„ä»¶é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„Runnableæ¥å£ä½“ç³»å’ŒLCELè¯­æ³•ï¼Œå®ç°äº†ä»ä¼ ç»Ÿé¢å‘å¯¹è±¡è®¾è®¡åˆ°ç°ä»£å‡½æ•°å¼ç»„åˆçš„é‡å¤§è·ƒå‡ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åŒ…æ‹¬ï¼š

1. **ç»Ÿä¸€æŠ½è±¡**ï¼šRunnableæ¥å£ä¸ºæ‰€æœ‰ç»„ä»¶æä¾›ä¸€è‡´çš„è°ƒç”¨æ–¹å¼
2. **ç»„åˆçµæ´»æ€§**ï¼šLCELè¯­æ³•è®©å¤æ‚å·¥ä½œæµçš„æ„å»ºå˜å¾—ç›´è§‚å’Œç±»å‹å®‰å…¨  
3. **æ€§èƒ½ä¼˜åŒ–**ï¼šå†…ç½®æ‰¹å¤„ç†ã€ç¼“å­˜ã€æµå¼å¤„ç†ç­‰ä¼˜åŒ–æœºåˆ¶
4. **ä¼ä¸šçº§ç‰¹æ€§**ï¼šå®Œå–„çš„ç›‘æ§ã€é”™è¯¯å¤„ç†å’Œå¯æ‰©å±•æ€§æ”¯æŒ

é€šè¿‡æ·±å…¥ç†è§£è¿™äº›å®ç°ç»†èŠ‚å’Œæœ€ä½³å®è·µï¼Œå¼€å‘è€…èƒ½å¤Ÿæ„å»ºå‡ºé«˜æ€§èƒ½ã€å¯ç»´æŠ¤çš„LLMåº”ç”¨ï¼Œå……åˆ†å‘æŒ¥LangChainæ¡†æ¶çš„å¼ºå¤§èƒ½åŠ›ã€‚