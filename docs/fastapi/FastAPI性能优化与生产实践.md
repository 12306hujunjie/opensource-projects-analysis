# FastAPIæ€§èƒ½ä¼˜åŒ–ä¸ç”Ÿäº§å®è·µ

> **æŠ€æœ¯èšç„¦**: é«˜æ€§èƒ½+ç”Ÿäº§å°±ç»ª | **æ ¸å¿ƒåˆ›æ–°**: å¼‚æ­¥ä¼˜å…ˆæ¶æ„ | **å®è·µç‰¹è‰²**: ä¼ä¸šçº§è¿ç»´ç»éªŒ

---

## ğŸŒŸ æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯å®šä½ä¸ç”Ÿäº§ä»·å€¼

### è§£å†³çš„æ ¸å¿ƒç”Ÿäº§é—®é¢˜
FastAPIæ€§èƒ½ä¼˜åŒ–è§£å†³çš„æ ¹æœ¬æŒ‘æˆ˜ï¼š**å¦‚ä½•è®©Python Webåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¾¾åˆ°ä¼ä¸šçº§æ€§èƒ½æ ‡å‡†ï¼ŒåŒæ—¶ä¿æŒå¼€å‘æ•ˆç‡å’Œä»£ç è´¨é‡ï¼Ÿ**

- **æ€§èƒ½ç“¶é¢ˆé—®é¢˜**ï¼šä¼ ç»ŸPython Webæ¡†æ¶åœ¨é«˜å¹¶å‘ä¸‹æ€§èƒ½ä¸è¶³
- **ç”Ÿäº§ç¨³å®šæ€§æŒ‘æˆ˜**ï¼šå¼€å‘ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒçš„æ€§èƒ½å·®å¼‚å·¨å¤§
- **æ‰©å®¹å›°éš¾**ï¼šæ°´å¹³æ‰©å®¹å’Œå‚ç›´æ‰©å®¹ç­–ç•¥ä¸æ˜ç¡®
- **ç›‘æ§ç›²åŒº**ï¼šç¼ºä¹æœ‰æ•ˆçš„æ€§èƒ½ç›‘æ§å’Œé—®é¢˜è¯Šæ–­æœºåˆ¶

### æŠ€æœ¯åˆ›æ–°ä¸ç”Ÿäº§ä»·å€¼
- **å¼‚æ­¥ä¼˜å…ˆæ¶æ„**ï¼šåŸç”ŸASGIæ”¯æŒï¼ŒçœŸæ­£çš„é«˜å¹¶å‘å¤„ç†èƒ½åŠ›
- **é›¶å¼€é”€ä¼˜åŒ–**ï¼šç¼–è¯‘æ—¶ä¼˜åŒ–ï¼Œè¿è¡Œæ—¶é›¶é¢å¤–å¼€é”€çš„ç‰¹æ€§
- **ç”Ÿäº§å°±ç»ªè®¾è®¡**ï¼šå†…ç½®å¥åº·æ£€æŸ¥ã€ä¼˜é›…å…³é—­ã€é”™è¯¯å¤„ç†æœºåˆ¶
- **ä¼ä¸šçº§ç›‘æ§**ï¼šå®Œæ•´çš„å¯è§‚æµ‹æ€§å’Œæ€§èƒ½ç›‘æ§æ”¯æŒ

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ¶æ„å…¨æ™¯å›¾

### æ•´ä½“æ€§èƒ½æ¶æ„å±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FastAPI æ€§èƒ½ä¼˜åŒ–æ¶æ„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   ASGI      â”‚â”€â”€â”€â”€â–¶â”‚ Event Loop   â”‚â”€â”€â”€â”€â–¶â”‚ Concurrency â”‚ â”‚
â”‚  â”‚ Protocol    â”‚     â”‚ Optimization â”‚     â”‚  Control    â”‚ â”‚
â”‚  â”‚(åè®®ä¼˜åŒ–)   â”‚     â”‚ (äº‹ä»¶å¾ªç¯ä¼˜åŒ–) â”‚     â”‚ (å¹¶å‘æ§åˆ¶)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                    â”‚      â”‚
â”‚         â”‚                     â”‚                    â”‚      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Thread Pool â”‚     â”‚ Background   â”‚     â”‚ Memory      â”‚ â”‚
â”‚  â”‚Integration  â”‚â—„â”€â”€â”€â”€â”¤    Tasks     â”œâ”€â”€â”€â”€â–¶â”‚Management   â”‚ â”‚
â”‚  â”‚(çº¿ç¨‹æ± é›†æˆ) â”‚     â”‚  (åå°ä»»åŠ¡)   â”‚     â”‚ (å†…å­˜ç®¡ç†)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                     â”‚                    â”‚      â”‚
â”‚         â”‚                     â”‚                    â”‚      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Cache     â”‚     â”‚ Connection   â”‚     â”‚ Resource    â”‚ â”‚
â”‚  â”‚ Strategy    â”‚â—„â”€â”€â”€â”€â”¤    Pool      â”œâ”€â”€â”€â”€â–¶â”‚Optimization â”‚ â”‚
â”‚  â”‚ (ç¼“å­˜ç­–ç•¥)   â”‚     â”‚ (è¿æ¥æ± )      â”‚     â”‚ (èµ„æºä¼˜åŒ–)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                               â”‚                           â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚         â”‚            ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–å±‚                        â”‚â”‚
â”‚         â”‚                     â”‚                          â”‚â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚ Load        â”‚     â”‚ Monitoring   â”‚     â”‚ Deployment  â”‚ â”‚â”‚
â”‚  â”‚Balancing    â”‚     â”‚ & Alerting   â”‚     â”‚ Strategy    â”‚ â”‚â”‚
â”‚  â”‚(è´Ÿè½½å‡è¡¡)   â”‚     â”‚ (ç›‘æ§å‘Šè­¦)    â”‚     â”‚ (éƒ¨ç½²ç­–ç•¥)   â”‚ â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ€§èƒ½ç»„ä»¶åˆ†æ

#### **å¼‚æ­¥å¹¶å‘æ§åˆ¶** (`concurrency.py`)
```python
@asynccontextmanager
async def contextmanager_in_threadpool(cm: ContextManager[_T]) -> AsyncGenerator[_T, None]:
    """çº¿ç¨‹æ± ä¸­ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„å¼‚æ­¥é€‚é…å™¨"""
    
    # é˜²æ­¢æ­»é”çš„é™æµå™¨è®¾è®¡
    exit_limiter = CapacityLimiter(1)
    
    try:
        # å¼‚æ­¥æ‰§è¡Œ__enter__æ–¹æ³•
        yield await run_in_threadpool(cm.__enter__)
    except Exception as e:
        # å¼‚æ­¥æ‰§è¡Œ__exit__æ–¹æ³•ï¼Œå¤„ç†å¼‚å¸¸
        ok = bool(
            await anyio.to_thread.run_sync(
                cm.__exit__, type(e), e, e.__traceback__, 
                limiter=exit_limiter  # ä½¿ç”¨ç‹¬ç«‹é™æµå™¨é¿å…æ­»é”
            )
        )
        if not ok:
            raise e
    else:
        # æ­£å¸¸é€€å‡ºæ—¶çš„æ¸…ç†
        await anyio.to_thread.run_sync(
            cm.__exit__, None, None, None, 
            limiter=exit_limiter
        )
```

**è®¾è®¡äº®ç‚¹**ï¼š
- **æ­»é”é¢„é˜²**ï¼šä½¿ç”¨ç‹¬ç«‹çš„CapacityLimiteré¿å…å†…éƒ¨è¿æ¥æ± å†²çª
- **å¼‚å¸¸å®‰å…¨**ï¼šä¿è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„æ­£ç¡®æ‰§è¡Œé¡ºåº
- **æ€§èƒ½ä¼˜åŒ–**ï¼šé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼Œä¿æŒå¼‚æ­¥ç‰¹æ€§

#### **åå°ä»»åŠ¡ç³»ç»Ÿ** (`background.py`)
```python
class BackgroundTasks(StarletteBackgroundTasks):
    """å“åº”åæ‰§è¡Œçš„å¼‚æ­¥åå°ä»»åŠ¡ç³»ç»Ÿ"""
    
    def add_task(self, func: Callable[P, Any], *args: P.args, **kwargs: P.kwargs) -> None:
        """æ·»åŠ åå°ä»»åŠ¡ï¼Œæ”¯æŒåŒæ­¥å’Œå¼‚æ­¥å‡½æ•°"""
        return super().add_task(func, *args, **kwargs)

# ä½¿ç”¨ç¤ºä¾‹ï¼šæé«˜å“åº”æ—¶é—´
@app.post("/send-email")
async def send_email(
    email_data: EmailModel, 
    background_tasks: BackgroundTasks
):
    # ç«‹å³è¿”å›å“åº”ï¼Œæé«˜ç”¨æˆ·ä½“éªŒ
    background_tasks.add_task(send_email_async, email_data.to, email_data.content)
    background_tasks.add_task(log_email_sent, email_data.to)
    
    return {"message": "Email queued successfully"}

# åå°ä»»åŠ¡åœ¨å“åº”å‘é€åå¼‚æ­¥æ‰§è¡Œï¼š
# 1. send_email_async(to, content)  
# 2. log_email_sent(to)
```

**æ€§èƒ½ä¼˜åŠ¿**ï¼š
- **å“åº”æ—¶é—´ä¼˜åŒ–**ï¼šè€—æ—¶ä»»åŠ¡å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›å“åº”
- **èµ„æºåˆ©ç”¨ç‡**ï¼šåå°ä»»åŠ¡ä¸è¯·æ±‚å¤„ç†å¹¶è¡Œæ‰§è¡Œ
- **ç”¨æˆ·ä½“éªŒ**ï¼šé¿å…ç”¨æˆ·ç­‰å¾…è€—æ—¶æ“ä½œå®Œæˆ

---

## âš¡ æ ¸å¿ƒæ€§èƒ½ä¼˜åŒ–æŠ€æœ¯æ·±åº¦è§£æ

### ASGIåè®®æ€§èƒ½ä¼˜åŒ–

#### äº‹ä»¶å¾ªç¯ä¼˜åŒ–ç­–ç•¥
```python
# FastAPI + Uvicorn é«˜æ€§èƒ½é…ç½®
import uvicorn
from fastapi import FastAPI

app = FastAPI()

if __name__ == "__main__":
    # ç”Ÿäº§çº§æ€§èƒ½é…ç½®
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        # äº‹ä»¶å¾ªç¯ä¼˜åŒ–
        loop="uvloop",           # ä½¿ç”¨uvloopæ›¿ä»£é»˜è®¤äº‹ä»¶å¾ªç¯ (2-4xæ€§èƒ½æå‡)
        http="httptools",        # ä½¿ç”¨httptoolsè§£æå™¨ (æ›´å¿«çš„HTTPè§£æ)
        
        # å¹¶å‘ä¼˜åŒ–  
        workers=4,               # å¤šè¿›ç¨‹worker (CPUæ ¸å¿ƒæ•°)
        worker_class="uvicorn.workers.UvicornWorker",
        
        # è¿æ¥ä¼˜åŒ–
        limit_concurrency=1000,  # å¹¶å‘è¿æ¥é™åˆ¶
        limit_max_requests=10000, # å•workeræœ€å¤§è¯·æ±‚æ•°
        timeout_keep_alive=30,   # Keep-Aliveè¶…æ—¶
        
        # èµ„æºä¼˜åŒ–
        backlog=2048,           # TCP backlogé˜Ÿåˆ—
        access_log=False,       # ç”Ÿäº§ç¯å¢ƒå…³é—­è®¿é—®æ—¥å¿—
    )
```

#### ASGIä¸­é—´ä»¶æ€§èƒ½æ ˆ
```python
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.middleware.gzip import GZipMiddleware
from starlette.middleware.cors import CORSMiddleware
import time

class PerformanceMiddleware(BaseHTTPMiddleware):
    """é«˜æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶"""
    
    async def dispatch(self, request, call_next):
        start_time = time.perf_counter()
        
        # è¯·æ±‚å¤„ç†
        response = await call_next(request)
        
        # æ€§èƒ½ç›‘æ§
        process_time = time.perf_counter() - start_time
        response.headers["X-Process-Time"] = str(process_time)
        
        # æ€§èƒ½å‘Šè­¦
        if process_time > 1.0:  # è¶…è¿‡1ç§’å‘Šè­¦
            logger.warning(f"Slow request: {request.url.path} took {process_time:.2f}s")
            
        return response

# ä¸­é—´ä»¶æ ˆä¼˜åŒ–é¡ºåº
app.add_middleware(PerformanceMiddleware)           # æ€§èƒ½ç›‘æ§
app.add_middleware(GZipMiddleware, minimum_size=1000) # å“åº”å‹ç¼©
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],      # é™åˆ¶CORSåŸŸå
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)
```

### å¼‚æ­¥æ•°æ®åº“ä¼˜åŒ–

#### æ•°æ®åº“è¿æ¥æ± é…ç½®
```python
from databases import Database
from sqlalchemy import create_engine, MetaData
import asyncio

# é«˜æ€§èƒ½æ•°æ®åº“é…ç½®
DATABASE_URL = "postgresql://user:pass@localhost/db"

# å¼‚æ­¥æ•°æ®åº“è¿æ¥
database = Database(
    DATABASE_URL,
    # è¿æ¥æ± ä¼˜åŒ–
    min_size=10,              # æœ€å°è¿æ¥æ•°
    max_size=20,              # æœ€å¤§è¿æ¥æ•°  
    max_queries=50000,        # å•è¿æ¥æœ€å¤§æŸ¥è¯¢æ•°
    max_inactive_connection_lifetime=300,  # éæ´»è·ƒè¿æ¥è¶…æ—¶(5åˆ†é’Ÿ)
    
    # æ€§èƒ½ä¼˜åŒ–
    command_timeout=60,       # å‘½ä»¤è¶…æ—¶
    server_settings={
        "jit": "off",                    # å…³é—­JITç¼–è¯‘(å‡å°‘CPU)
        "application_name": "fastapi_app",
    }
)

# ä¾èµ–æ³¨å…¥ä¼˜åŒ–
from functools import lru_cache

@lru_cache()
def get_database() -> Database:
    """æ•°æ®åº“è¿æ¥å•ä¾‹æ¨¡å¼"""
    return database

async def get_db_connection():
    """æ•°æ®åº“è¿æ¥ä¾èµ–"""
    async with database.transaction():
        yield database

# é«˜æ€§èƒ½æŸ¥è¯¢ç¤ºä¾‹
class UserRepository:
    def __init__(self, db: Database):
        self.db = db
        
    async def get_users_batch(self, user_ids: List[int]) -> List[User]:
        """æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–"""
        query = """
        SELECT id, name, email, created_at 
        FROM users 
        WHERE id = ANY($1)
        """
        rows = await self.db.fetch_all(query, user_ids)
        return [User(**row) for row in rows]
        
    async def get_user_with_posts(self, user_id: int) -> UserWithPosts:
        """JOINæŸ¥è¯¢ä¼˜åŒ–ï¼Œé¿å…N+1é—®é¢˜"""
        query = """
        SELECT 
            u.id, u.name, u.email,
            array_agg(
                json_build_object(
                    'id', p.id,
                    'title', p.title,
                    'created_at', p.created_at
                )
            ) as posts
        FROM users u
        LEFT JOIN posts p ON u.id = p.user_id  
        WHERE u.id = $1
        GROUP BY u.id, u.name, u.email
        """
        row = await self.db.fetch_one(query, user_id)
        return UserWithPosts(**row)
```

### ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

#### Redisç¼“å­˜é›†æˆ
```python
import redis.asyncio as redis
from typing import Optional
import json
import pickle
from functools import wraps

# Redisè¿æ¥æ± 
redis_pool = redis.ConnectionPool.from_url(
    "redis://localhost:6379",
    max_connections=20,
    retry_on_timeout=True,
    health_check_interval=30,
)
redis_client = redis.Redis(connection_pool=redis_pool)

class CacheManager:
    """é«˜æ€§èƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        
    async def get(self, key: str, model_class=None) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        try:
            data = await self.redis.get(key)
            if data is None:
                return None
                
            # æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©ååºåˆ—åŒ–æ–¹å¼
            if model_class and issubclass(model_class, BaseModel):
                return model_class.parse_raw(data)
            else:
                return pickle.loads(data)
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None
            
    async def set(self, key: str, value: Any, ttl: int = 3600) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        try:
            # æ ¹æ®ç±»å‹é€‰æ‹©åºåˆ—åŒ–æ–¹å¼
            if isinstance(value, BaseModel):
                data = value.json()
            else:
                data = pickle.dumps(value)
                
            await self.redis.setex(key, ttl, data)
            return True
        except Exception as e:
            logger.error(f"Cache set error: {e}")
            return False
            
    async def invalidate_pattern(self, pattern: str) -> int:
        """æ‰¹é‡åˆ é™¤åŒ¹é…çš„ç¼“å­˜é”®"""
        keys = await self.redis.keys(pattern)
        if keys:
            return await self.redis.delete(*keys)
        return 0

# ç¼“å­˜è£…é¥°å™¨
def cached(ttl: int = 3600, key_prefix: str = ""):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
            
            # å°è¯•è·å–ç¼“å­˜
            cached_result = await cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
                
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = await func(*args, **kwargs)
            await cache_manager.set(cache_key, result, ttl)
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cached(ttl=1800, key_prefix="user")
async def get_user_profile(user_id: int) -> UserProfile:
    """å¸¦ç¼“å­˜çš„ç”¨æˆ·é…ç½®è·å–"""
    user = await database.fetch_one(
        "SELECT * FROM users WHERE id = $1", user_id
    )
    return UserProfile(**user)
```

---

## ğŸ—ƒï¸ å†…å­˜ä¼˜åŒ–ä¸èµ„æºç®¡ç†

### å­—æ®µå…‹éš†ä¼˜åŒ–æœºåˆ¶

#### create_cloned_fieldæ€§èƒ½ä¼˜åŒ–
```python
# FastAPIå†…éƒ¨ä¼˜åŒ–ï¼šå­—æ®µå…‹éš†ç¼“å­˜æœºåˆ¶
_CLONED_TYPES_CACHE: MutableMapping[Type[BaseModel], Type[BaseModel]] = {}

def create_cloned_field(
    field: ModelField,
    cloned_types: Optional[MutableMapping[Type[BaseModel], Type[BaseModel]]] = None,
) -> ModelField:
    """ä¼˜åŒ–çš„å­—æ®µå…‹éš†ï¼Œé¿å…é‡å¤è®¡ç®—"""
    
    if PYDANTIC_V2:
        return field  # Pydantic v2 æ— éœ€å…‹éš†
        
    # ä½¿ç”¨å…¨å±€ç¼“å­˜æé«˜æ€§èƒ½
    if cloned_types is None:
        cloned_types = _CLONED_TYPES_CACHE
        
    original_type = field.type_
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºBaseModelå­ç±»
    if lenient_issubclass(original_type, BaseModel):
        # ç¼“å­˜æ£€æŸ¥ï¼Œé¿å…é‡å¤å…‹éš†
        use_type = cloned_types.get(original_type)
        if use_type is None:
            # åˆ›å»ºæ–°çš„æ¨¡å‹ç±»å‹
            use_type = create_model(original_type.__name__, __base__=original_type)
            cloned_types[original_type] = use_type
            
            # é€’å½’å¤„ç†å­—æ®µï¼Œå¤ç”¨ç¼“å­˜
            for f in original_type.__fields__.values():
                use_type.__fields__[f.name] = create_cloned_field(
                    f, cloned_types=cloned_types  # ä¼ é€’ç¼“å­˜
                )
                
    # åˆ›å»ºæ–°å­—æ®µï¼Œå¤åˆ¶æ‰€æœ‰å±æ€§
    new_field = create_model_field(name=field.name, type_=use_type)
    # ... å¤åˆ¶å…¶ä»–å­—æ®µå±æ€§
    
    return new_field
```

**ç¼“å­˜ä¼˜åŒ–æ”¶ç›Š**ï¼š
- **å†…å­˜æ•ˆç‡**ï¼šé¿å…é‡å¤åˆ›å»ºç›¸åŒçš„å…‹éš†ç±»å‹
- **CPUä¼˜åŒ–**ï¼šå‡å°‘é‡å¤çš„ç±»å‹åˆ†æå’Œå­—æ®µå¤„ç†
- **é€’å½’ä¼˜åŒ–**ï¼šæ”¯æŒå¤æ‚åµŒå¥—æ¨¡å‹çš„é«˜æ•ˆå¤„ç†

### å†…å­˜æ³„æ¼é¢„é˜²

#### èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†
```python
from contextlib import asynccontextmanager
from typing import AsyncGenerator
import asyncio
import weakref

class ResourceManager:
    """èµ„æºç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    
    def __init__(self):
        self._resources: weakref.WeakSet = weakref.WeakSet()
        self._cleanup_tasks: List[asyncio.Task] = []
        
    async def startup(self):
        """åº”ç”¨å¯åŠ¨æ—¶çš„èµ„æºåˆå§‹åŒ–"""
        # æ•°æ®åº“è¿æ¥æ± 
        await database.connect()
        self._resources.add(database)
        
        # Redisè¿æ¥æ± 
        await redis_client.ping()
        self._resources.add(redis_client)
        
        # åå°æ¸…ç†ä»»åŠ¡
        cleanup_task = asyncio.create_task(self._periodic_cleanup())
        self._cleanup_tasks.append(cleanup_task)
        
    async def shutdown(self):
        """åº”ç”¨å…³é—­æ—¶çš„èµ„æºæ¸…ç†"""
        # å–æ¶ˆåå°ä»»åŠ¡
        for task in self._cleanup_tasks:
            task.cancel()
            
        # å…³é—­æ‰€æœ‰èµ„æº
        for resource in self._resources:
            if hasattr(resource, 'disconnect'):
                await resource.disconnect()
            elif hasattr(resource, 'close'):
                await resource.close()
                
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if self._cleanup_tasks:
            await asyncio.gather(*self._cleanup_tasks, return_exceptions=True)
            
    async def _periodic_cleanup(self):
        """å‘¨æœŸæ€§æ¸…ç†ä»»åŠ¡"""
        while True:
            try:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                await self._cleanup_expired_cache()
                
                # æ¸…ç†æ— ç”¨è¿æ¥
                await self._cleanup_idle_connections()
                
                # åƒåœ¾å›æ”¶
                import gc
                gc.collect()
                
                await asyncio.sleep(300)  # 5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Cleanup error: {e}")
                await asyncio.sleep(60)

# FastAPIåº”ç”¨ç”Ÿå‘½å‘¨æœŸ
@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    resource_manager = ResourceManager()
    
    # å¯åŠ¨
    await resource_manager.startup()
    logger.info("Application started")
    
    yield
    
    # å…³é—­
    await resource_manager.shutdown()  
    logger.info("Application shutdown")

app = FastAPI(lifespan=lifespan)
```

---

## ğŸš€ ç”Ÿäº§éƒ¨ç½²ç­–ç•¥ä¸é…ç½®ä¼˜åŒ–

### å®¹å™¨åŒ–éƒ¨ç½²æœ€ä½³å®è·µ

#### å¤šé˜¶æ®µDockeræ„å»º
```dockerfile
# Dockerfile - å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–
FROM python:3.11-slim AS builder

# æ„å»ºé˜¶æ®µä¼˜åŒ–
WORKDIR /app
COPY requirements.txt .

# å®‰è£…æ„å»ºä¾èµ–
RUN pip install --no-cache-dir --upgrade pip \
    && pip install --no-cache-dir -r requirements.txt

# ç”Ÿäº§é˜¶æ®µ
FROM python:3.11-slim AS production

# ç³»ç»Ÿä¼˜åŒ–
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        curl \
        nginx \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

WORKDIR /app

# å¤åˆ¶ä¾èµ–
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .
RUN chown -R appuser:appuser /app

USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨è„šæœ¬
CMD ["gunicorn", "main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", \
     "--bind", "0.0.0.0:8000", "--access-logfile", "-"]
```

#### Kuberneteséƒ¨ç½²é…ç½®
```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fastapi-app
  labels:
    app: fastapi-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: fastapi-app
  template:
    metadata:
      labels:
        app: fastapi-app
    spec:
      containers:
      - name: fastapi-app
        image: fastapi-app:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: app-secrets
              key: database-url
        - name: REDIS_URL
          value: "redis://redis-service:6379"
        
        # èµ„æºé™åˆ¶
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
            
        # å¥åº·æ£€æŸ¥
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
          
        # ä¼˜é›…å…³é—­
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
              
      terminationGracePeriodSeconds: 30
---
apiVersion: v1
kind: Service
metadata:
  name: fastapi-service
spec:
  selector:
    app: fastapi-app
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
  type: ClusterIP
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fastapi-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fastapi-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### è´Ÿè½½å‡è¡¡ä¸åå‘ä»£ç†

#### Nginxé«˜æ€§èƒ½é…ç½®
```nginx
# nginx.conf - ç”Ÿäº§çº§é…ç½®
worker_processes auto;
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    # åŸºç¡€ä¼˜åŒ–
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    keepalive_requests 1000;
    
    # Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml;
        
    # ä¸Šæ¸¸æœåŠ¡å™¨
    upstream fastapi_backend {
        least_conn;
        server fastapi-app-1:8000 max_fails=3 fail_timeout=30s;
        server fastapi-app-2:8000 max_fails=3 fail_timeout=30s;
        server fastapi-app-3:8000 max_fails=3 fail_timeout=30s;
        
        keepalive 32;
    }
    
    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_conn_zone $binary_remote_addr zone=conn:10m;
    
    server {
        listen 80;
        server_name api.yourdomain.com;
        
        # å®‰å…¨å¤´
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        
        # APIä»£ç†
        location /api/ {
            # é™æµåº”ç”¨
            limit_req zone=api burst=20 nodelay;
            limit_conn conn 10;
            
            proxy_pass http://fastapi_backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection 'upgrade';
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 5s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # ç¼“å­˜é™æ€å“åº”
            location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
                expires 1y;
                add_header Cache-Control "public, immutable";
            }
        }
        
        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            proxy_pass http://fastapi_backend/health;
        }
        
        # ç›‘æ§æŒ‡æ ‡
        location /metrics {
            allow 10.0.0.0/8;
            deny all;
            proxy_pass http://fastapi_backend/metrics;
        }
    }
}
```

---

## ğŸ“Š ç›‘æ§ã€è°ƒè¯•ä¸æ€§èƒ½åˆ†æ

### å®Œæ•´ç›‘æ§æ ˆé›†æˆ

#### Prometheus + Grafanaç›‘æ§
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

# ç›‘æ§æŒ‡æ ‡å®šä¹‰
REQUEST_COUNT = Counter(
    'fastapi_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_DURATION = Histogram(
    'fastapi_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'fastapi_active_connections',
    'Active HTTP connections'
)

DATABASE_QUERIES = Counter(
    'fastapi_database_queries_total',
    'Total database queries',
    ['query_type']
)

class PrometheusMiddleware(BaseHTTPMiddleware):
    """Prometheusç›‘æ§ä¸­é—´ä»¶"""
    
    async def dispatch(self, request, call_next):
        start_time = time.time()
        method = request.method
        path = request.url.path
        
        # å¢åŠ æ´»è·ƒè¿æ¥æ•°
        ACTIVE_CONNECTIONS.inc()
        
        try:
            response = await call_next(request)
            status_code = response.status_code
        except Exception as e:
            status_code = 500
            raise
        finally:
            # è®°å½•æŒ‡æ ‡
            duration = time.time() - start_time
            REQUEST_COUNT.labels(
                method=method, 
                endpoint=path, 
                status_code=status_code
            ).inc()
            REQUEST_DURATION.labels(
                method=method, 
                endpoint=path
            ).observe(duration)
            ACTIVE_CONNECTIONS.dec()
            
        return response

# ç›‘æ§ç«¯ç‚¹
@app.get("/metrics")
async def get_metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    return Response(generate_latest(), media_type="text/plain")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        await database.fetch_one("SELECT 1")
        
        # æ£€æŸ¥Redisè¿æ¥
        await redis_client.ping()
        
        return {
            "status": "healthy",
            "timestamp": time.time(),
            "services": {
                "database": "up",
                "redis": "up"
            }
        }
    except Exception as e:
        raise HTTPException(
            status_code=503, 
            detail=f"Service unavailable: {str(e)}"
        )

@app.get("/ready")
async def readiness_check():
    """å°±ç»ªæ£€æŸ¥ç«¯ç‚¹"""
    # æ£€æŸ¥åº”ç”¨æ˜¯å¦å‡†å¤‡å¥½æ¥æ”¶è¯·æ±‚
    return {"status": "ready"}
```

#### ç»“æ„åŒ–æ—¥å¿—ä¸é“¾è·¯è¿½è¸ª
```python
import structlog
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# ç»“æ„åŒ–æ—¥å¿—é…ç½®
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
)

logger = structlog.get_logger()

# é“¾è·¯è¿½è¸ªé…ç½®
trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)

span_processor = BatchSpanProcessor(jaeger_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

class TracingMiddleware(BaseHTTPMiddleware):
    """åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªä¸­é—´ä»¶"""
    
    async def dispatch(self, request, call_next):
        with tracer.start_as_current_span("http_request") as span:
            # æ·»åŠ è¯·æ±‚ä¿¡æ¯åˆ°span
            span.set_attribute("http.method", request.method)
            span.set_attribute("http.url", str(request.url))
            span.set_attribute("http.user_agent", request.headers.get("user-agent", ""))
            
            # ç»“æ„åŒ–æ—¥å¿—
            logger.info(
                "Request started",
                method=request.method,
                path=request.url.path,
                user_agent=request.headers.get("user-agent"),
            )
            
            try:
                response = await call_next(request)
                span.set_attribute("http.status_code", response.status_code)
                
                logger.info(
                    "Request completed",
                    method=request.method,
                    path=request.url.path,
                    status_code=response.status_code,
                )
                
                return response
            except Exception as e:
                span.set_attribute("error", True)
                span.set_attribute("error.message", str(e))
                
                logger.error(
                    "Request failed",
                    method=request.method,
                    path=request.url.path,
                    error=str(e),
                    exc_info=True,
                )
                raise
```

### æ€§èƒ½åˆ†æä¸è°ƒä¼˜

#### æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
```python
import cProfile
import pstats
from functools import wraps
import asyncio

def profile_async(func):
    """å¼‚æ­¥å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        
        try:
            result = await func(*args, **kwargs)
            return result
        finally:
            pr.disable()
            
            # åˆ†æç»“æœ
            stats = pstats.Stats(pr)
            stats.sort_stats('cumulative')
            
            # è¾“å‡ºçƒ­ç‚¹å‡½æ•°
            print(f"\n=== Profile for {func.__name__} ===")
            stats.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªå‡½æ•°
            
    return wrapper

# æ€§èƒ½ç›‘æ§ç«¯ç‚¹
@app.get("/performance/analyze")
@profile_async
async def performance_analysis():
    """æ€§èƒ½åˆ†æç«¯ç‚¹"""
    # æ¨¡æ‹Ÿå¤æ‚æ“ä½œ
    users = await get_users_from_db(limit=1000)
    processed_users = [process_user(user) for user in users]
    return {"processed": len(processed_users)}

# å†…å­˜ä½¿ç”¨ç›‘æ§
import psutil
import gc

@app.get("/performance/memory")
async def memory_status():
    """å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    # åƒåœ¾å›æ”¶ç»Ÿè®¡
    gc_stats = gc.get_stats()
    
    return {
        "memory": {
            "rss": memory_info.rss,  # ç‰©ç†å†…å­˜
            "vms": memory_info.vms,  # è™šæ‹Ÿå†…å­˜
            "percent": process.memory_percent(),
        },
        "gc": {
            "collections": sum(stat['collections'] for stat in gc_stats),
            "collected": sum(stat['collected'] for stat in gc_stats),
            "uncollectable": sum(stat['uncollectable'] for stat in gc_stats),
        },
        "cache": {
            "cloned_types": len(_CLONED_TYPES_CACHE),
        }
    }
```

---

## âš–ï¸ ç”Ÿäº§ç¯å¢ƒæƒè¡¡ä¸æœ€ä½³å®è·µ

### æ€§èƒ½vså¯ç»´æŠ¤æ€§æƒè¡¡

#### æ€§èƒ½ä¼˜åŒ–å†³ç­–çŸ©é˜µ
```python
# å†³ç­–æ¡†æ¶ï¼šä½•æ—¶è¿›è¡Œæ€§èƒ½ä¼˜åŒ–
class PerformanceDecision:
    """æ€§èƒ½ä¼˜åŒ–å†³ç­–è¾…åŠ©"""
    
    @staticmethod
    def should_optimize(
        current_rps: int,
        target_rps: int,
        response_time_p99: float,
        error_rate: float,
        development_cost_hours: int
    ) -> dict:
        """æ€§èƒ½ä¼˜åŒ–å†³ç­–åˆ†æ"""
        
        # æ€§èƒ½å·®è·è¯„ä¼°
        performance_gap = (target_rps - current_rps) / target_rps
        
        # ä¼˜åŒ–ä¼˜å…ˆçº§è¯„åˆ†
        priority_score = 0
        recommendations = []
        
        # å“åº”æ—¶é—´æ£€æŸ¥
        if response_time_p99 > 1.0:
            priority_score += 3
            recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œä¼˜å…ˆçº§ï¼šé«˜")
        elif response_time_p99 > 0.5:
            priority_score += 2
            recommendations.append("å“åº”æ—¶é—´éœ€è¦æ”¹å–„ï¼Œä¼˜å…ˆçº§ï¼šä¸­")
            
        # é”™è¯¯ç‡æ£€æŸ¥
        if error_rate > 0.01:  # >1%
            priority_score += 4
            recommendations.append("é”™è¯¯ç‡è¿‡é«˜ï¼Œä¼˜å…ˆçº§ï¼šç´§æ€¥")
        elif error_rate > 0.001:  # >0.1%
            priority_score += 2
            recommendations.append("é”™è¯¯ç‡éœ€è¦å…³æ³¨ï¼Œä¼˜å…ˆçº§ï¼šé«˜")
            
        # æ€§èƒ½å·®è·æ£€æŸ¥
        if performance_gap > 0.5:  # æ€§èƒ½å·®è·>50%
            priority_score += 3
            recommendations.append("æ€§èƒ½å·®è·æ˜¾è‘—ï¼Œä¼˜å…ˆçº§ï¼šé«˜")
            
        # å¼€å‘æˆæœ¬è¯„ä¼°
        cost_effectiveness = performance_gap * 100 / development_cost_hours
        
        return {
            "should_optimize": priority_score >= 3,
            "priority_score": priority_score,
            "recommendations": recommendations,
            "cost_effectiveness": cost_effectiveness,
            "optimization_areas": get_optimization_areas(performance_gap, response_time_p99)
        }

def get_optimization_areas(performance_gap: float, response_time: float) -> List[str]:
    """è·å–ä¼˜åŒ–å»ºè®®é¢†åŸŸ"""
    areas = []
    
    if response_time > 1.0:
        areas.extend([
            "æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–",
            "ç¼“å­˜ç­–ç•¥å®æ–½",
            "å¼‚æ­¥å¤„ç†æ”¹è¿›"
        ])
    
    if performance_gap > 0.3:
        areas.extend([
            "è¿æ¥æ± é…ç½®è°ƒä¼˜",
            "ASGIæœåŠ¡å™¨ä¼˜åŒ–",
            "ä¸­é—´ä»¶æ€§èƒ½æ”¹è¿›"
        ])
        
    return areas
```

#### ç”Ÿäº§ç¯å¢ƒé…ç½®æ¨¡æ¿
```python
from pydantic import BaseSettings
from typing import Optional

class ProductionSettings(BaseSettings):
    """ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    
    # åº”ç”¨é…ç½®
    app_name: str = "FastAPI App"
    debug: bool = False
    log_level: str = "INFO"
    
    # æœåŠ¡å™¨é…ç½®
    host: str = "0.0.0.0"
    port: int = 8000
    workers: int = 4
    
    # æ•°æ®åº“é…ç½®
    database_url: str
    database_pool_size: int = 20
    database_max_overflow: int = 30
    database_pool_timeout: int = 30
    
    # Redisé…ç½®
    redis_url: str = "redis://localhost:6379"
    redis_max_connections: int = 20
    
    # ç¼“å­˜é…ç½®
    cache_ttl_default: int = 3600
    cache_ttl_user: int = 1800
    cache_ttl_static: int = 86400
    
    # æ€§èƒ½é…ç½®
    request_timeout: int = 30
    max_request_size: int = 16 * 1024 * 1024  # 16MB
    
    # ç›‘æ§é…ç½®
    enable_metrics: bool = True
    metrics_path: str = "/metrics"
    health_check_path: str = "/health"
    
    # å®‰å…¨é…ç½®
    cors_origins: List[str] = []
    rate_limit_requests: int = 100
    rate_limit_period: int = 60
    
    class Config:
        env_file = ".env"
        case_sensitive = False

# ç¯å¢ƒç‰¹å®šé…ç½®
class DevelopmentSettings(ProductionSettings):
    debug: bool = True
    log_level: str = "DEBUG"
    workers: int = 1

class StagingSettings(ProductionSettings):
    workers: int = 2
    database_pool_size: int = 10

class ProductionSettings(ProductionSettings):
    workers: int = 8
    database_pool_size: int = 50
    enable_metrics: bool = True

# é…ç½®å·¥å‚
def get_settings() -> ProductionSettings:
    env = os.getenv("ENVIRONMENT", "development")
    
    if env == "development":
        return DevelopmentSettings()
    elif env == "staging":
        return StagingSettings()
    else:
        return ProductionSettings()

settings = get_settings()
```

### æ‰©å®¹ç­–ç•¥ä¸å®¹é‡è§„åˆ’

#### è‡ªåŠ¨æ‰©å®¹é…ç½®
```yaml
# HPAé…ç½® - åŸºäºå¤šæŒ‡æ ‡æ‰©å®¹
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: fastapi-hpa-advanced
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: fastapi-app
  minReplicas: 3
  maxReplicas: 20
  
  # æ‰©å®¹è¡Œä¸ºé…ç½®
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
        
  metrics:
  # CPUä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
        
  # å†…å­˜ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
        
  # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šè¯·æ±‚QPS
  - type: Object
    object:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
      describedObject:
        apiVersion: v1
        kind: Service
        name: fastapi-service
        
  # è‡ªå®šä¹‰æŒ‡æ ‡ï¼šå“åº”æ—¶é—´
  - type: Object
    object:
      metric:
        name: response_time_p99
      target:
        type: Value
        value: "500m"  # 500ms
      describedObject:
        apiVersion: v1
        kind: Service
        name: fastapi-service
```

#### å®¹é‡è§„åˆ’è®¡ç®—å™¨
```python
class CapacityPlanner:
    """å®¹é‡è§„åˆ’è®¡ç®—å™¨"""
    
    def __init__(self):
        self.baseline_metrics = {
            "cpu_per_request": 0.01,      # CPUç§’/è¯·æ±‚
            "memory_per_request": 1024,    # å­—èŠ‚/è¯·æ±‚
            "db_connections_per_pod": 5,   # æ•°æ®åº“è¿æ¥/Pod
            "redis_connections_per_pod": 2, # Redisè¿æ¥/Pod
        }
        
    def calculate_capacity(
        self,
        target_rps: int,
        response_time_sla: float = 0.5,
        availability_sla: float = 0.999
    ) -> dict:
        """è®¡ç®—æ‰€éœ€å®¹é‡"""
        
        # åŸºç¡€å®¹é‡è®¡ç®—
        base_pods = max(3, target_rps * self.baseline_metrics["cpu_per_request"])
        
        # SLAå®¹ä½™é‡
        sla_buffer = 1.2 if availability_sla >= 0.999 else 1.1
        
        # å³°å€¼æµé‡å®¹ä½™é‡  
        peak_buffer = 2.0
        
        # æœ€ç»ˆPodæ•°é‡
        required_pods = int(base_pods * sla_buffer * peak_buffer)
        
        # èµ„æºéœ€æ±‚
        total_cpu = required_pods * 0.5  # 500m CPU/Pod
        total_memory = required_pods * 512  # 512MB/Pod
        
        # æ•°æ®åº“è¿æ¥éœ€æ±‚
        db_connections = required_pods * self.baseline_metrics["db_connections_per_pod"]
        redis_connections = required_pods * self.baseline_metrics["redis_connections_per_pod"]
        
        return {
            "pods": {
                "min": 3,
                "recommended": required_pods,
                "max": required_pods * 2,
            },
            "resources": {
                "cpu_cores": total_cpu,
                "memory_gb": total_memory / 1024,
            },
            "connections": {
                "database": db_connections,
                "redis": redis_connections,
            },
            "estimated_costs": {
                "compute": total_cpu * 0.048 * 24 * 30,  # AWSä¼°ç®—
                "database": db_connections * 10,  # RDSè¿æ¥æˆæœ¬
            }
        }
    
    def generate_report(self, target_rps: int) -> str:
        """ç”Ÿæˆå®¹é‡è§„åˆ’æŠ¥å‘Š"""
        capacity = self.calculate_capacity(target_rps)
        
        return f"""
å®¹é‡è§„åˆ’æŠ¥å‘Š
=============
ç›®æ ‡QPS: {target_rps:,}

Podé…ç½®:
  - æœ€å°å®ä¾‹: {capacity['pods']['min']}
  - æ¨èå®ä¾‹: {capacity['pods']['recommended']}
  - æœ€å¤§å®ä¾‹: {capacity['pods']['max']}

èµ„æºéœ€æ±‚:
  - CPUæ ¸å¿ƒ: {capacity['resources']['cpu_cores']:.1f}
  - å†…å­˜: {capacity['resources']['memory_gb']:.1f}GB

è¿æ¥æ± é…ç½®:
  - æ•°æ®åº“è¿æ¥: {capacity['connections']['database']}
  - Redisè¿æ¥: {capacity['connections']['redis']}

é¢„ä¼°æœˆæˆæœ¬:
  - è®¡ç®—èµ„æº: ${capacity['estimated_costs']['compute']:.2f}
  - æ•°æ®åº“: ${capacity['estimated_costs']['database']:.2f}
        """

# ä½¿ç”¨ç¤ºä¾‹
planner = CapacityPlanner()
print(planner.generate_report(10000))  # 1ä¸‡QPSçš„å®¹é‡è§„åˆ’
```

---

## ğŸ“ˆ ç”Ÿäº§ç›‘æ§ä¸æ•…éšœå¤„ç†

### å®Œæ•´å‘Šè­¦ç­–ç•¥

#### Prometheuså‘Šè­¦è§„åˆ™
```yaml
# prometheus-rules.yaml
groups:
- name: fastapi-alerts
  rules:
  
  # é«˜é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(fastapi_requests_total{status_code=~"5.."}[5m]) > 0.05
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
      
  # é«˜å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.99, rate(fastapi_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High response time detected"
      description: "99th percentile response time is {{ $value }}s"
      
  # æ•°æ®åº“è¿æ¥å‘Šè­¦
  - alert: DatabaseConnectionHigh
    expr: fastapi_active_connections > 80
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "High database connections"
      description: "Active database connections: {{ $value }}"
      
  # Podé‡å¯å‘Šè­¦
  - alert: PodRestartHigh
    expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 0
    for: 0m
    labels:
      severity: warning
    annotations:
      summary: "Pod restarting frequently"
      description: "Pod {{ $labels.pod }} restarted {{ $value }} times in the last 15 minutes"
      
  # å†…å­˜ä½¿ç”¨å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (container_memory_working_set_bytes / container_spec_memory_limit_bytes) > 0.9
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage"
      description: "Memory usage is {{ $value | humanizePercentage }}"
```

### æ•…éšœæ’æŸ¥æ‰‹å†Œ

#### å¸¸è§æ€§èƒ½é—®é¢˜è¯Šæ–­
```python
class PerformanceDiagnostics:
    """æ€§èƒ½é—®é¢˜è¯Šæ–­å·¥å…·"""
    
    @staticmethod
    async def diagnose_slow_requests():
        """è¯Šæ–­æ…¢è¯·æ±‚"""
        # æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½
        slow_queries = await get_slow_database_queries()
        
        # æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
        cache_stats = await get_cache_statistics()
        
        # æ£€æŸ¥å¼‚æ­¥ä»»åŠ¡ç§¯å‹
        task_backlog = await get_background_task_stats()
        
        report = {
            "database": {
                "slow_queries_count": len(slow_queries),
                "avg_query_time": sum(q.duration for q in slow_queries) / len(slow_queries) if slow_queries else 0,
                "recommendations": []
            },
            "cache": {
                "hit_rate": cache_stats.hit_rate,
                "recommendations": []
            },
            "tasks": {
                "backlog_size": task_backlog.queue_size,
                "recommendations": []
            }
        }
        
        # ç”Ÿæˆå»ºè®®
        if report["database"]["avg_query_time"] > 100:  # >100ms
            report["database"]["recommendations"].append("ä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•")
            
        if report["cache"]["hit_rate"] < 0.8:  # <80%
            report["cache"]["recommendations"].append("ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œå¢åŠ TTL")
            
        if report["tasks"]["backlog_size"] > 1000:
            report["tasks"]["recommendations"].append("å¢åŠ åå°ä»»åŠ¡å¤„ç†å™¨")
            
        return report
    
    @staticmethod
    async def diagnose_memory_leaks():
        """å†…å­˜æ³„æ¼è¯Šæ–­"""
        import gc
        import sys
        from collections import defaultdict
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        collected = gc.collect()
        
        # ç»Ÿè®¡å¯¹è±¡ç±»å‹
        obj_counts = defaultdict(int)
        for obj in gc.get_objects():
            obj_counts[type(obj).__name__] += 1
            
        # æ£€æŸ¥å¤§å¯¹è±¡
        large_objects = []
        for obj in gc.get_objects():
            if hasattr(obj, '__sizeof__'):
                size = sys.getsizeof(obj)
                if size > 1024 * 1024:  # >1MB
                    large_objects.append({
                        "type": type(obj).__name__,
                        "size_mb": size / (1024 * 1024),
                        "repr": repr(obj)[:100]
                    })
                    
        return {
            "gc_stats": {
                "collected": collected,
                "total_objects": len(gc.get_objects()),
                "top_types": dict(sorted(obj_counts.items(), key=lambda x: x[1], reverse=True)[:10])
            },
            "large_objects": large_objects[:5],  # å‰5ä¸ªæœ€å¤§å¯¹è±¡
            "recommendations": [
                "æ£€æŸ¥æ˜¯å¦æœ‰æœªå…³é—­çš„æ–‡ä»¶å¥æŸ„",
                "ç¡®è®¤æ•°æ®åº“è¿æ¥æ­£ç¡®å…³é—­",
                "æ£€æŸ¥ç¼“å­˜æ˜¯å¦æ­£ç¡®è¿‡æœŸ",
                "ä½¿ç”¨weakrefé¿å…å¾ªç¯å¼•ç”¨"
            ]
        }

# è¯Šæ–­ç«¯ç‚¹
@app.get("/diagnostics/performance")
async def performance_diagnostics():
    """æ€§èƒ½è¯Šæ–­ç«¯ç‚¹"""
    return await PerformanceDiagnostics.diagnose_slow_requests()

@app.get("/diagnostics/memory") 
async def memory_diagnostics():
    """å†…å­˜è¯Šæ–­ç«¯ç‚¹"""
    return await PerformanceDiagnostics.diagnose_memory_leaks()
```

### ç”Ÿäº§æ•…éšœå“åº”æµç¨‹

#### è‡ªåŠ¨æ•…éšœæ¢å¤æœºåˆ¶
```python
class FailureRecovery:
    """è‡ªåŠ¨æ•…éšœæ¢å¤ç³»ç»Ÿ"""
    
    def __init__(self):
        self.circuit_breakers = {}
        self.health_checks = {}
        
    async def database_circuit_breaker(self, operation: Callable):
        """æ•°æ®åº“ç†”æ–­å™¨"""
        breaker_key = "database"
        
        if breaker_key not in self.circuit_breakers:
            self.circuit_breakers[breaker_key] = {
                "failure_count": 0,
                "last_failure": None,
                "state": "CLOSED",  # CLOSED, OPEN, HALF_OPEN
                "failure_threshold": 5,
                "recovery_timeout": 60
            }
            
        breaker = self.circuit_breakers[breaker_key]
        
        # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
        if breaker["state"] == "OPEN":
            if time.time() - breaker["last_failure"] > breaker["recovery_timeout"]:
                breaker["state"] = "HALF_OPEN"
            else:
                raise Exception("Circuit breaker is OPEN")
                
        try:
            result = await operation()
            
            # æˆåŠŸæ—¶é‡ç½®è®¡æ•°å™¨
            if breaker["state"] == "HALF_OPEN":
                breaker["state"] = "CLOSED"
            breaker["failure_count"] = 0
            
            return result
            
        except Exception as e:
            breaker["failure_count"] += 1
            breaker["last_failure"] = time.time()
            
            if breaker["failure_count"] >= breaker["failure_threshold"]:
                breaker["state"] = "OPEN"
                logger.error(f"Circuit breaker OPEN for {breaker_key}")
                
            raise
            
    async def health_check_with_recovery(self):
        """å¸¦è‡ªåŠ¨æ¢å¤çš„å¥åº·æ£€æŸ¥"""
        health_status = {"status": "healthy", "services": {}}
        
        # æ•°æ®åº“å¥åº·æ£€æŸ¥
        try:
            await self.database_circuit_breaker(
                lambda: database.fetch_one("SELECT 1")
            )
            health_status["services"]["database"] = "healthy"
        except Exception as e:
            health_status["services"]["database"] = "unhealthy"
            health_status["status"] = "degraded"
            
            # å°è¯•è‡ªåŠ¨é‡è¿
            try:
                await database.connect()
                logger.info("Database reconnected successfully")
            except Exception as reconnect_error:
                logger.error(f"Database reconnection failed: {reconnect_error}")
                
        # Rediså¥åº·æ£€æŸ¥
        try:
            await redis_client.ping()
            health_status["services"]["redis"] = "healthy"
        except Exception as e:
            health_status["services"]["redis"] = "unhealthy"
            health_status["status"] = "degraded"
            
            # å°è¯•é‡å»ºRedisè¿æ¥
            try:
                await redis_client.connection_pool.disconnect()
                await redis_client.ping()
                logger.info("Redis reconnected successfully")
            except Exception as redis_error:
                logger.error(f"Redis reconnection failed: {redis_error}")
                
        return health_status

# è‡ªåŠ¨æ¢å¤ç³»ç»Ÿé›†æˆ
recovery_system = FailureRecovery()

@app.get("/health/advanced")
async def advanced_health_check():
    """é«˜çº§å¥åº·æ£€æŸ¥ï¼ŒåŒ…å«è‡ªåŠ¨æ¢å¤"""
    return await recovery_system.health_check_with_recovery()
```

---

*é€šè¿‡è¿™ä¸ªå…¨é¢çš„æ€§èƒ½ä¼˜åŒ–ä¸ç”Ÿäº§å®è·µåˆ†æï¼Œæˆ‘ä»¬æŒæ¡äº†FastAPIåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„å®Œæ•´è¿ç»´ä½“ç³»ã€‚ä»å¼‚æ­¥æ¶æ„ä¼˜åŒ–ã€èµ„æºç®¡ç†ã€ç›‘æ§å‘Šè­¦åˆ°æ•…éšœæ¢å¤ï¼Œå½¢æˆäº†ä¼ä¸šçº§FastAPIåº”ç”¨çš„å®Œæ•´ç”Ÿäº§è§£å†³æ–¹æ¡ˆã€‚*

**æ–‡æ¡£ç‰¹è‰²**ï¼šæ€§èƒ½ä¼˜åŒ– + ç”Ÿäº§éƒ¨ç½² + ç›‘æ§è¿ç»´ + æ•…éšœå¤„ç†  
**åˆ›å»ºæ—¶é—´**ï¼š2025å¹´1æœˆ  
**åˆ†ææ·±åº¦**ï¼šL2å±‚(æ¶æ„) + L3å±‚(å®ç°) + ç”Ÿäº§å®è·µ èåˆ