# L3: å¤šä»£ç†åä½œä¸å·¥å…·é›†æˆ

**å­¦ä¹ ç›®æ ‡**: æ„å»ºä¸“ä¸šåŒ–AIä»£ç†å›¢é˜Ÿï¼Œå®ç°å¤æ‚ä»»åŠ¡çš„åä½œå¤„ç†  
**é¢„è®¡ç”¨æ—¶**: 4-5å°æ—¶  
**æ ¸å¿ƒè½¬å˜**: ä»"å•å…µä½œæˆ˜"æ€ç»´ â†’ "å›¢é˜Ÿåä½œ"æ€ç»´

*ğŸ’¡ è¿™ä¸€ç« å°†å¸¦ä½ è¿›å…¥LangGraphçš„ç²¾é«“â€”â€”å¤šä»£ç†åä½œã€‚ä½ å°†å­¦ä¼šå¦‚ä½•è®©ä¸åŒçš„AIä»£ç†åƒä¸“ä¸šå›¢é˜Ÿä¸€æ ·åˆ†å·¥åˆä½œï¼Œæ¯ä¸ªä»£ç†éƒ½æœ‰è‡ªå·±çš„ä¸“é•¿ï¼Œå…±åŒè§£å†³å¤æ‚é—®é¢˜ã€‚*

---

## ğŸŒŸ å¼€ç¯‡ï¼šå›¢é˜Ÿçš„åŠ›é‡

### ä»¤äººç€è¿·çš„åä½œç°è±¡

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è¦æ±‚ä¸€ä¸ªAIç³»ç»Ÿ"åˆ†ææœ€è¿‘çš„ç§‘æŠ€è¶‹åŠ¿å¹¶å†™ä¸€ä»½æŠ•èµ„å»ºè®®æŠ¥å‘Š"ï¼Œç„¶åè¿™ä¸ªç³»ç»Ÿï¼š

```python
# ç”¨æˆ·è¯·æ±‚ï¼š"åˆ†ææœ€è¿‘çš„ç§‘æŠ€è¶‹åŠ¿å¹¶å†™ä¸€ä»½æŠ•èµ„å»ºè®®æŠ¥å‘Š"

# ğŸ” ç ”ç©¶å‘˜ä»£ç†å¯åŠ¨
"æ­£åœ¨æœç´¢æœ€æ–°çš„ç§‘æŠ€æ–°é—»ã€è®ºæ–‡å’Œå¸‚åœºæ•°æ®..."

# ğŸ“Š åˆ†æå¸ˆä»£ç†æ¥æ‰‹  
"æ­£åœ¨åˆ†ææŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œå¸‚åœºæ½œåŠ›..."

# âœï¸ æ’°å†™å‘˜ä»£ç†å¼€å§‹å·¥ä½œ
"æ­£åœ¨æ’°å†™ç»“æ„åŒ–çš„æŠ•èµ„å»ºè®®æŠ¥å‘Š..."

# ğŸ” è´¨æ£€å‘˜ä»£ç†éªŒè¯
"æ­£åœ¨éªŒè¯æ•°æ®å‡†ç¡®æ€§å’Œé€»è¾‘ä¸€è‡´æ€§..."

# ğŸ“ˆ æœ€ç»ˆè¾“å‡º
"å®Œæˆï¼è¿™æ˜¯ä¸€ä»½åŒ…å«æœ€æ–°æ•°æ®åˆ†æçš„ä¸“ä¸šæŠ•èµ„å»ºè®®æŠ¥å‘Šã€‚"
```

**è¿™ç§åä½œæ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ** ğŸ¤”

æ¯ä¸ªä»£ç†éƒ½æœ‰è‡ªå·±çš„ä¸“ä¸šé¢†åŸŸï¼š
- ğŸ” **ç ”ç©¶å‘˜**ï¼šæ“…é•¿ä¿¡æ¯æœç´¢å’Œæ•°æ®æ”¶é›†
- ğŸ“Š **åˆ†æå¸ˆ**ï¼šæ“…é•¿æ•°æ®åˆ†æå’Œè¶‹åŠ¿åˆ¤æ–­  
- âœï¸ **æ’°å†™å‘˜**ï¼šæ“…é•¿å†…å®¹åˆ›ä½œå’Œç»“æ„åŒ–è¡¨è¾¾
- ğŸ” **è´¨æ£€å‘˜**ï¼šæ“…é•¿éªŒè¯å’Œè´¨é‡æ§åˆ¶

å®ƒä»¬é€šè¿‡å…±äº«çš„å·¥ä½œçŠ¶æ€è¿›è¡Œåä½œï¼Œå°±åƒç°å®ä¸­çš„ä¸“ä¸šå›¢é˜Ÿä¸€æ ·ï¼

### ä¸ºä»€ä¹ˆéœ€è¦å¤šä»£ç†ï¼Ÿ

**å•ä¸€AIçš„å±€é™æ€§**ï¼š
```python
# å•ä¸€AIå¤„ç†å¤æ‚ä»»åŠ¡
def single_ai_solution(task):
    # éœ€è¦åŒæ—¶å…·å¤‡æ‰€æœ‰èƒ½åŠ›
    research_data = ai_research(task)      # æœç´¢èƒ½åŠ›
    analysis = ai_analyze(research_data)   # åˆ†æèƒ½åŠ›  
    report = ai_write(analysis)            # å†™ä½œèƒ½åŠ›
    quality = ai_check(report)             # è´¨æ£€èƒ½åŠ›
    return report if quality > 0.8 else "å¤±è´¥"
```

**é—®é¢˜æ˜¾è€Œæ˜“è§**ï¼š
- âŒ **èƒ½åŠ›ç¨€é‡Š**ï¼šä»€ä¹ˆéƒ½ä¼šï¼Œä½†ä»€ä¹ˆéƒ½ä¸ç²¾
- âŒ **ä¸Šä¸‹æ–‡ä¸¢å¤±**ï¼šé•¿ä»»åŠ¡ä¸­å®¹æ˜“é—å¿˜å‰é¢çš„ä¿¡æ¯
- âŒ **é”™è¯¯ä¼ æ’­**ï¼šä¸€ä¸ªç¯èŠ‚çš„é”™è¯¯å½±å“æ•´ä¸ªç»“æœ
- âŒ **éš¾ä»¥ä¼˜åŒ–**ï¼šæ— æ³•é’ˆå¯¹ç‰¹å®šç¯èŠ‚è¿›è¡Œä¸“é—¨ä¼˜åŒ–

**å¤šä»£ç†çš„ä¼˜åŠ¿**ï¼š
```python
# å¤šä»£ç†åä½œå¤„ç†å¤æ‚ä»»åŠ¡
class ResearchAgent:     # ä¸“ç²¾ä¿¡æ¯æœç´¢
class AnalysisAgent:     # ä¸“ç²¾æ•°æ®åˆ†æ
class WritingAgent:      # ä¸“ç²¾å†…å®¹åˆ›ä½œ
class QualityAgent:      # ä¸“ç²¾è´¨é‡æ§åˆ¶

# æ¯ä¸ªä»£ç†éƒ½æœ‰è‡ªå·±çš„ä¸“é•¿å’Œå·¥å…·
# é€šè¿‡çŠ¶æ€å…±äº«å®ç°æ— ç¼åä½œ
```

- âœ… **ä¸“ä¸šåŒ–åˆ†å·¥**ï¼šæ¯ä¸ªä»£ç†ä¸“æ³¨äºè‡ªå·±æœ€æ“…é•¿çš„é¢†åŸŸ
- âœ… **çŠ¶æ€å…±äº«**ï¼šä¿¡æ¯åœ¨ä»£ç†é—´é€æ˜ä¼ é€’
- âœ… **å¹¶è¡Œå¤„ç†**ï¼šå¤šä¸ªä»£ç†å¯ä»¥åŒæ—¶å·¥ä½œæé«˜æ•ˆç‡
- âœ… **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ˜“äºæµ‹è¯•ã€ç»´æŠ¤å’Œæ‰©å±•

## ğŸ¤ åŸºç¡€ï¼šç¬¬ä¸€ä¸ªåŒä»£ç†åä½œ

è®©æˆ‘ä»¬ä»æœ€ç®€å•çš„åŒä»£ç†åä½œå¼€å§‹ï¼Œå»ºç«‹å¯¹å¤šä»£ç†ç³»ç»Ÿçš„åŸºæœ¬ç†è§£ã€‚

### åœºæ™¯ï¼šæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹

**éœ€æ±‚**ï¼šç”¨æˆ·æå‡ºä¸€ä¸ªç ”ç©¶é—®é¢˜ï¼Œç³»ç»Ÿè‡ªåŠ¨æœç´¢ç›¸å…³ä¿¡æ¯å¹¶ç”Ÿæˆæ€»ç»“æŠ¥å‘Šã€‚

**ä»£ç†åˆ†å·¥**ï¼š
- ğŸ” **ç ”ç©¶å‘˜ä»£ç†**ï¼šè´Ÿè´£æœç´¢å’Œæ”¶é›†ç›¸å…³ä¿¡æ¯
- ğŸ“ **æ€»ç»“å‘˜ä»£ç†**ï¼šè´Ÿè´£åˆ†æä¿¡æ¯å¹¶ç”Ÿæˆç»“æ„åŒ–æ€»ç»“

### å®Œæ•´å®ç°

```python
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict, NotRequired
from typing import List, Dict, Any
import json

# 1. å¤šä»£ç†åä½œçŠ¶æ€è®¾è®¡
class ResearchState(TypedDict):
    # ç”¨æˆ·è¾“å…¥
    research_query: str
    
    # ç ”ç©¶å‘˜ä»£ç†çš„å·¥ä½œåŒº
    search_keywords: NotRequired[List[str]]
    raw_research_data: NotRequired[List[Dict[str, str]]]
    research_summary: NotRequired[str]
    
    # æ€»ç»“å‘˜ä»£ç†çš„å·¥ä½œåŒº
    structured_analysis: NotRequired[Dict[str, Any]]
    final_report: NotRequired[str]
    
    # åä½œä¿¡æ¯
    current_agent: NotRequired[str]
    completed_phases: NotRequired[List[str]]
    quality_score: NotRequired[float]

# 2. ç ”ç©¶å‘˜ä»£ç†å®ç°
def researcher_agent(state: ResearchState):
    """ğŸ” ç ”ç©¶å‘˜ä»£ç†ï¼šä¸“ç²¾ä¿¡æ¯æœç´¢å’Œæ•°æ®æ”¶é›†"""
    query = state["research_query"]
    
    print(f"ğŸ” ç ”ç©¶å‘˜ä»£ç†å¼€å§‹å·¥ä½œ...")
    print(f"   ç ”ç©¶ä¸»é¢˜: {query}")
    
    # ç”Ÿæˆæœç´¢å…³é”®è¯
    keywords = extract_keywords(query)
    print(f"   æå–å…³é”®è¯: {keywords}")
    
    # æ¨¡æ‹Ÿæœç´¢è¿‡ç¨‹ï¼ˆå®é™…é¡¹ç›®ä¸­ä¼šè°ƒç”¨çœŸå®æœç´¢APIï¼‰
    research_data = simulate_research(keywords)
    print(f"   æœç´¢åˆ° {len(research_data)} æ¡ç›¸å…³ä¿¡æ¯")
    
    # åˆæ­¥æ•´ç†æœç´¢ç»“æœ
    summary = create_research_summary(research_data)
    
    return {
        "search_keywords": keywords,
        "raw_research_data": research_data,
        "research_summary": summary,
        "current_agent": "researcher",
        "completed_phases": state.get("completed_phases", []) + ["research"]
    }

def extract_keywords(query: str) -> List[str]:
    """æå–æœç´¢å…³é”®è¯ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # å®é™…é¡¹ç›®ä¸­å¯èƒ½ä½¿ç”¨NLPæ¨¡å‹æˆ–AIæ¥æå–å…³é”®è¯
    common_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "å¦‚ä½•", "ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ"}
    words = [word for word in query.split() if word not in common_words]
    return words[:5]  # é™åˆ¶å…³é”®è¯æ•°é‡

def simulate_research(keywords: List[str]) -> List[Dict[str, str]]:
    """æ¨¡æ‹Ÿæœç´¢è¿‡ç¨‹"""
    # åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„æœç´¢API
    research_data = []
    for keyword in keywords:
        research_data.extend([
            {
                "title": f"å…³äº{keyword}çš„æœ€æ–°ç ”ç©¶è¿›å±•",
                "content": f"æ ¹æ®æœ€æ–°ç ”ç©¶ï¼Œ{keyword}åœ¨ç›¸å…³é¢†åŸŸè¡¨ç°å‡ºäº†æ˜¾è‘—çš„å‘å±•è¶‹åŠ¿...",
                "source": f"å­¦æœ¯æœŸåˆŠ_{keyword}",
                "relevance": 0.9
            },
            {
                "title": f"{keyword}çš„å®é™…åº”ç”¨æ¡ˆä¾‹",
                "content": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ{keyword}å·²ç»è¢«æˆåŠŸè¿ç”¨åˆ°å¤šä¸ªé¡¹ç›®ä¸­...",
                "source": f"è¡Œä¸šæŠ¥å‘Š_{keyword}",
                "relevance": 0.8
            }
        ])
    return research_data

def create_research_summary(data: List[Dict[str, str]]) -> str:
    """åˆ›å»ºç ”ç©¶æ‘˜è¦"""
    if not data:
        return "æœªæ‰¾åˆ°ç›¸å…³ç ”ç©¶èµ„æ–™"
    
    high_relevance = [item for item in data if item.get("relevance", 0) > 0.8]
    summary = f"å…±æœç´¢åˆ° {len(data)} æ¡ä¿¡æ¯ï¼Œå…¶ä¸­ {len(high_relevance)} æ¡é«˜ç›¸å…³åº¦èµ„æ–™ã€‚"
    summary += f"ä¸»è¦æ¥æºåŒ…æ‹¬ï¼š{', '.join(set(item['source'] for item in data[:3]))}"
    return summary

# 3. æ€»ç»“å‘˜ä»£ç†å®ç°
def summarizer_agent(state: ResearchState):
    """ğŸ“ æ€»ç»“å‘˜ä»£ç†ï¼šä¸“ç²¾ä¿¡æ¯åˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ"""
    research_data = state.get("raw_research_data", [])
    query = state["research_query"]
    
    print(f"ğŸ“ æ€»ç»“å‘˜ä»£ç†å¼€å§‹å·¥ä½œ...")
    print(f"   åˆ†æ {len(research_data)} æ¡ç ”ç©¶æ•°æ®")
    
    # ç»“æ„åŒ–åˆ†æ
    analysis = perform_structured_analysis(research_data, query)
    print(f"   å®Œæˆç»“æ„åŒ–åˆ†æ")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    report = generate_final_report(query, analysis, research_data)
    print(f"   ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ({len(report)} å­—ç¬¦)")
    
    # è´¨é‡è¯„ä¼°
    quality = assess_report_quality(report, research_data)
    print(f"   è´¨é‡è¯„åˆ†: {quality:.2f}")
    
    return {
        "structured_analysis": analysis,
        "final_report": report,
        "quality_score": quality,
        "current_agent": "summarizer",
        "completed_phases": state.get("completed_phases", []) + ["analysis", "reporting"]
    }

def perform_structured_analysis(data: List[Dict[str, str]], query: str) -> Dict[str, Any]:
    """æ‰§è¡Œç»“æ„åŒ–åˆ†æ"""
    analysis = {
        "total_sources": len(data),
        "high_relevance_count": len([item for item in data if item.get("relevance", 0) > 0.8]),
        "source_types": list(set(item.get("source", "").split("_")[0] for item in data)),
        "key_themes": extract_themes(data),
        "confidence_level": calculate_confidence(data)
    }
    return analysis

def extract_themes(data: List[Dict[str, str]]) -> List[str]:
    """æå–å…³é”®ä¸»é¢˜"""
    # ç®€åŒ–çš„ä¸»é¢˜æå–é€»è¾‘
    all_content = " ".join(item.get("content", "") for item in data)
    themes = ["æŠ€æœ¯å‘å±•", "å®é™…åº”ç”¨", "å‘å±•è¶‹åŠ¿", "æŒ‘æˆ˜ä¸æœºé‡"]
    return [theme for theme in themes if theme in all_content]

def calculate_confidence(data: List[Dict[str, str]]) -> float:
    """è®¡ç®—ç½®ä¿¡åº¦"""
    if not data:
        return 0.0
    avg_relevance = sum(item.get("relevance", 0) for item in data) / len(data)
    source_diversity = len(set(item.get("source", "") for item in data)) / len(data)
    return (avg_relevance + source_diversity) / 2

def generate_final_report(query: str, analysis: Dict[str, Any], data: List[Dict[str, str]]) -> str:
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    report = f"""# ç ”ç©¶æŠ¥å‘Šï¼š{query}

## æ‰§è¡Œæ‘˜è¦
æœ¬æ¬¡ç ”ç©¶å…±åˆ†æäº† {analysis['total_sources']} ä¸ªä¿¡æ¯æºï¼Œå…¶ä¸­ {analysis['high_relevance_count']} ä¸ªé«˜ç›¸å…³åº¦æ¥æºã€‚
ç ”ç©¶ç½®ä¿¡åº¦ä¸º {analysis['confidence_level']:.2f}ã€‚

## ä¸»è¦å‘ç°
"""
    
    # æ·»åŠ å…³é”®ä¸»é¢˜
    if analysis['key_themes']:
        report += "### å…³é”®ä¸»é¢˜\n"
        for theme in analysis['key_themes']:
            report += f"- {theme}\n"
        report += "\n"
    
    # æ·»åŠ è¯¦ç»†å†…å®¹
    report += "### è¯¦ç»†åˆ†æ\n"
    for i, item in enumerate(data[:3], 1):  # åªå±•ç¤ºå‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        report += f"{i}. **{item.get('title', 'æ— æ ‡é¢˜')}**\n"
        report += f"   æ¥æºï¼š{item.get('source', 'æœªçŸ¥')}\n"
        report += f"   å†…å®¹ï¼š{item.get('content', 'æ— å†…å®¹')[:100]}...\n\n"
    
    report += "## ç»“è®º\n"
    report += f"åŸºäºä»¥ä¸Šåˆ†æï¼Œé’ˆå¯¹"{query}"çš„ç ”ç©¶æ˜¾ç¤ºäº†ç›¸å…³é¢†åŸŸçš„é‡è¦è¿›å±•å’Œå‘å±•è¶‹åŠ¿ã€‚"
    
    return report

def assess_report_quality(report: str, data: List[Dict[str, str]]) -> float:
    """è¯„ä¼°æŠ¥å‘Šè´¨é‡"""
    # ç®€å•çš„è´¨é‡è¯„ä¼°é€»è¾‘
    length_score = min(1.0, len(report) / 1000)  # é•¿åº¦è¯„åˆ†
    structure_score = 1.0 if "## æ‰§è¡Œæ‘˜è¦" in report and "## ä¸»è¦å‘ç°" in report else 0.5
    data_usage_score = min(1.0, len(data) / 10)  # æ•°æ®ä½¿ç”¨è¯„åˆ†
    
    return (length_score + structure_score + data_usage_score) / 3

# 4. æ„å»ºåŒä»£ç†åä½œå›¾
def create_research_assistant():
    """åˆ›å»ºæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹"""
    workflow = StateGraph(ResearchState)
    
    # æ·»åŠ ä»£ç†èŠ‚ç‚¹
    workflow.add_node("researcher", researcher_agent)
    workflow.add_node("summarizer", summarizer_agent)
    
    # å®šä¹‰åä½œæµç¨‹ï¼šç ”ç©¶å‘˜ â†’ æ€»ç»“å‘˜
    workflow.set_entry_point("researcher")
    workflow.add_edge("researcher", "summarizer")
    workflow.add_edge("summarizer", END)
    
    return workflow.compile()

# 5. æµ‹è¯•åŒä»£ç†åä½œ
if __name__ == "__main__":
    app = create_research_assistant()
    
    # æµ‹è¯•ç ”ç©¶ä»»åŠ¡
    result = app.invoke({
        "research_query": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨å‰æ™¯"
    })
    
    print("\n" + "="*50)
    print("ğŸ¯ åŒä»£ç†åä½œç»“æœ")
    print("="*50)
    print(f"ç ”ç©¶ä¸»é¢˜: {result['research_query']}")
    print(f"å®Œæˆé˜¶æ®µ: {result['completed_phases']}")
    print(f"è´¨é‡è¯„åˆ†: {result['quality_score']:.2f}")
    print(f"\nğŸ“Š ç ”ç©¶æ•°æ®æ¦‚è§ˆ:")
    print(f"- æœç´¢å…³é”®è¯: {result['search_keywords']}")
    print(f"- æ•°æ®æ¥æºæ•°é‡: {len(result['raw_research_data'])}")
    print(f"- ç»“æ„åŒ–åˆ†æ: {result['structured_analysis']}")
    
    print(f"\nğŸ“„ æœ€ç»ˆæŠ¥å‘Š:")
    print(result['final_report'])
```

### ğŸ” æ·±å…¥ç†è§£ï¼šåä½œçš„æ ¸å¿ƒæœºåˆ¶

**1. çŠ¶æ€å…±äº«**ï¼š
```python
# ç ”ç©¶å‘˜ä»£ç†æ·»åŠ æ•°æ®åˆ°å…±äº«çŠ¶æ€
return {
    "raw_research_data": research_data,  # å…¶ä»–ä»£ç†å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ•°æ®
    "research_summary": summary
}

# æ€»ç»“å‘˜ä»£ç†è¯»å–ç ”ç©¶å‘˜çš„å·¥ä½œç»“æœ
research_data = state.get("raw_research_data", [])  # è·å–ç ”ç©¶å‘˜çš„æˆæœ
```

**2. å·¥ä½œåŒºéš”ç¦»**ï¼š
```python
class ResearchState(TypedDict):
    # ç ”ç©¶å‘˜çš„å·¥ä½œåŒº
    search_keywords: NotRequired[List[str]]
    raw_research_data: NotRequired[List[Dict[str, str]]]
    
    # æ€»ç»“å‘˜çš„å·¥ä½œåŒº  
    structured_analysis: NotRequired[Dict[str, Any]]
    final_report: NotRequired[str]
```

**3. åä½œè¿›åº¦è·Ÿè¸ª**ï¼š
```python
"completed_phases": state.get("completed_phases", []) + ["research"]
```

## ğŸ­ è¿›é˜¶ï¼šä¸“ä¸šåŒ–ä»£ç†å›¢é˜Ÿ

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªæ›´å¤æ‚çš„å¤šä»£ç†ç³»ç»Ÿï¼š**æ™ºèƒ½å†…å®¹åˆ›ä½œå·¥ä½œå®¤**

### å›¢é˜Ÿç»„æˆ

- ğŸ” **ç ”ç©¶å‘˜**ï¼šæ”¶é›†ä¸»é¢˜ç›¸å…³çš„èƒŒæ™¯ä¿¡æ¯å’Œæ•°æ®
- âœï¸ **æ’°å†™å‘˜**ï¼šæ ¹æ®ç ”ç©¶ç»“æœåˆ›ä½œåˆç¨¿å†…å®¹
- âœï¸ **ç¼–è¾‘**ï¼šæ”¹è¿›æ–‡ç« ç»“æ„ã€è¯­è¨€å’Œå¯è¯»æ€§
- ğŸ” **è´¨æ£€å‘˜**ï¼šéªŒè¯äº‹å®å‡†ç¡®æ€§å’Œæ•´ä½“è´¨é‡
- ğŸ¨ **è®¾è®¡å¸ˆ**ï¼šè´Ÿè´£é…å›¾å’Œæ ¼å¼ä¼˜åŒ–ï¼ˆå·¥å…·é›†æˆç¤ºä¾‹ï¼‰

### åä½œæµç¨‹è®¾è®¡

```
ç”¨æˆ·éœ€æ±‚ â†’ ç ”ç©¶å‘˜ â†’ æ’°å†™å‘˜ â†’ ç¼–è¾‘ â†’ è´¨æ£€å‘˜ â†’ è®¾è®¡å¸ˆ â†’ æœ€ç»ˆæˆå“
           â†“        â†“        â†“       â†“        â†“
        æ”¶é›†ä¿¡æ¯   åˆ›ä½œåˆç¨¿   æ”¹è¿›æ–‡æœ¬  è´¨é‡éªŒè¯   è§†è§‰ä¼˜åŒ–
```

### å®Œæ•´å®ç°

```python
from enum import Enum
from typing import Union

class ContentPhase(str, Enum):
    RESEARCH = "research"
    WRITING = "writing" 
    EDITING = "editing"
    QUALITY_CHECK = "quality_check"
    DESIGN = "design"
    COMPLETED = "completed"

class ContentQuality(str, Enum):
    POOR = "poor"
    FAIR = "fair"
    GOOD = "good"
    EXCELLENT = "excellent"

class ContentStudioState(TypedDict):
    # ç”¨æˆ·éœ€æ±‚
    content_topic: str
    content_type: NotRequired[str]  # "article", "blog", "report"
    target_audience: NotRequired[str]
    word_count_target: NotRequired[int]
    
    # ç ”ç©¶å‘˜å·¥ä½œåŒº
    research_keywords: NotRequired[List[str]]
    background_info: NotRequired[List[Dict[str, Any]]]
    research_confidence: NotRequired[float]
    
    # æ’°å†™å‘˜å·¥ä½œåŒº
    content_outline: NotRequired[List[str]]
    draft_content: NotRequired[str]
    writing_style: NotRequired[str]
    
    # ç¼–è¾‘å·¥ä½œåŒº
    editing_feedback: NotRequired[List[str]]
    revised_content: NotRequired[str]
    readability_score: NotRequired[float]
    
    # è´¨æ£€å‘˜å·¥ä½œåŒº
    fact_check_results: NotRequired[List[Dict[str, Any]]]
    quality_assessment: NotRequired[ContentQuality]
    improvement_suggestions: NotRequired[List[str]]
    
    # è®¾è®¡å¸ˆå·¥ä½œåŒº
    visual_elements: NotRequired[List[str]]
    formatted_content: NotRequired[str]
    design_assets: NotRequired[List[str]]
    
    # æµç¨‹æ§åˆ¶
    current_phase: NotRequired[ContentPhase]
    completed_agents: NotRequired[List[str]]
    overall_progress: NotRequired[float]
    final_output: NotRequired[str]

# ç ”ç©¶å‘˜ä»£ç†
def content_researcher(state: ContentStudioState):
    """ğŸ” å†…å®¹ç ”ç©¶å‘˜ï¼šæ”¶é›†ä¸»é¢˜ç›¸å…³ä¿¡æ¯"""
    topic = state["content_topic"]
    content_type = state.get("content_type", "article")
    
    print(f"ğŸ” ç ”ç©¶å‘˜å¼€å§‹å·¥ä½œ...")
    print(f"   ä¸»é¢˜: {topic}")
    print(f"   ç±»å‹: {content_type}")
    
    # ç”Ÿæˆç ”ç©¶å…³é”®è¯
    keywords = generate_research_keywords(topic, content_type)
    print(f"   å…³é”®è¯: {keywords}")
    
    # æ”¶é›†èƒŒæ™¯ä¿¡æ¯
    background_info = collect_background_info(keywords)
    print(f"   æ”¶é›†åˆ° {len(background_info)} æ¡èƒŒæ™¯ä¿¡æ¯")
    
    # è¯„ä¼°ç ”ç©¶ç½®ä¿¡åº¦
    confidence = assess_research_confidence(background_info)
    
    return {
        "research_keywords": keywords,
        "background_info": background_info,
        "research_confidence": confidence,
        "current_phase": ContentPhase.RESEARCH,
        "completed_agents": state.get("completed_agents", []) + ["researcher"],
        "overall_progress": 0.2
    }

def generate_research_keywords(topic: str, content_type: str) -> List[str]:
    """ç”Ÿæˆç ”ç©¶å…³é”®è¯"""
    base_keywords = topic.split()
    
    # æ ¹æ®å†…å®¹ç±»å‹æ·»åŠ ç‰¹å®šå…³é”®è¯
    type_keywords = {
        "article": ["åˆ†æ", "è¶‹åŠ¿", "å½±å“"],
        "blog": ["å®è·µ", "ç»éªŒ", "æŠ€å·§"],
        "report": ["æ•°æ®", "ç ”ç©¶", "æŠ¥å‘Š"]
    }
    
    return base_keywords + type_keywords.get(content_type, [])

def collect_background_info(keywords: List[str]) -> List[Dict[str, Any]]:
    """æ”¶é›†èƒŒæ™¯ä¿¡æ¯ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    info = []
    for keyword in keywords:
        info.extend([
            {
                "title": f"{keyword}çš„æœ€æ–°å‘å±•è¶‹åŠ¿",
                "content": f"å…³äº{keyword}çš„æœ€æ–°ç ”ç©¶è¡¨æ˜...",
                "source": "è¡Œä¸šæŠ¥å‘Š",
                "credibility": 0.9,
                "relevance": 0.8
            },
            {
                "title": f"{keyword}çš„å®é™…åº”ç”¨æ¡ˆä¾‹",
                "content": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ{keyword}å±•ç°å‡º...",
                "source": "æ¡ˆä¾‹ç ”ç©¶",
                "credibility": 0.8,
                "relevance": 0.9
            }
        ])
    return info

def assess_research_confidence(info: List[Dict[str, Any]]) -> float:
    """è¯„ä¼°ç ”ç©¶ç½®ä¿¡åº¦"""
    if not info:
        return 0.0
    
    avg_credibility = sum(item.get("credibility", 0) for item in info) / len(info)
    avg_relevance = sum(item.get("relevance", 0) for item in info) / len(info)
    
    return (avg_credibility + avg_relevance) / 2

# æ’°å†™å‘˜ä»£ç†
def content_writer(state: ContentStudioState):
    """âœï¸ å†…å®¹æ’°å†™å‘˜ï¼šåˆ›ä½œé«˜è´¨é‡åˆç¨¿"""
    topic = state["content_topic"]
    background_info = state.get("background_info", [])
    target_words = state.get("word_count_target", 1000)
    
    print(f"âœï¸ æ’°å†™å‘˜å¼€å§‹å·¥ä½œ...")
    print(f"   åŸºäº {len(background_info)} æ¡ç ”ç©¶ä¿¡æ¯")
    print(f"   ç›®æ ‡å­—æ•°: {target_words}")
    
    # åˆ›å»ºå†…å®¹å¤§çº²
    outline = create_content_outline(topic, background_info)
    print(f"   å¤§çº²ç« èŠ‚: {len(outline)}")
    
    # æ’°å†™åˆç¨¿
    draft = write_initial_draft(outline, background_info, target_words)
    print(f"   åˆç¨¿å®Œæˆ: {len(draft)} å­—ç¬¦")
    
    # ç¡®å®šå†™ä½œé£æ ¼
    style = determine_writing_style(topic, state.get("target_audience", "general"))
    
    return {
        "content_outline": outline,
        "draft_content": draft,
        "writing_style": style,
        "current_phase": ContentPhase.WRITING,
        "completed_agents": state.get("completed_agents", []) + ["writer"],
        "overall_progress": 0.4
    }

def create_content_outline(topic: str, info: List[Dict[str, Any]]) -> List[str]:
    """åˆ›å»ºå†…å®¹å¤§çº²"""
    # åŸºç¡€å¤§çº²ç»“æ„
    outline = [
        f"å¼•è¨€ï¼š{topic}çš„é‡è¦æ€§",
        f"{topic}çš„ç°çŠ¶åˆ†æ",
        f"{topic}çš„å‘å±•è¶‹åŠ¿",
        f"{topic}çš„å®é™…åº”ç”¨",
        f"ç»“è®ºä¸å±•æœ›"
    ]
    
    # æ ¹æ®ç ”ç©¶ä¿¡æ¯è°ƒæ•´å¤§çº²
    themes = set()
    for item in info:
        if "è¶‹åŠ¿" in item.get("title", ""):
            themes.add("å‘å±•è¶‹åŠ¿")
        if "æ¡ˆä¾‹" in item.get("title", ""):
            themes.add("å®é™…åº”ç”¨")
        if "æŒ‘æˆ˜" in item.get("title", ""):
            themes.add("æŒ‘æˆ˜åˆ†æ")
    
    return outline

def write_initial_draft(outline: List[str], info: List[Dict[str, Any]], target_words: int) -> str:
    """æ’°å†™åˆç¨¿"""
    words_per_section = target_words // len(outline)
    
    draft = ""
    for section in outline:
        draft += f"\n## {section}\n\n"
        
        # æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
        relevant_info = [item for item in info 
                        if any(keyword in item.get("title", "") for keyword in section.split())]
        
        if relevant_info:
            for item in relevant_info[:2]:  # æ¯ä¸ªç« èŠ‚æœ€å¤šç”¨2æ¡ä¿¡æ¯
                draft += f"{item.get('content', '')} "
        else:
            draft += f"å…³äº{section}çš„è¯¦ç»†åˆ†æå†…å®¹... "
        
        # è¡¥å……åˆ°ç›®æ ‡å­—æ•°
        current_length = len(draft.split())
        if current_length < words_per_section:
            draft += "è¿™é‡Œéœ€è¦æ›´å¤šçš„è¯¦ç»†è¯´æ˜å’Œåˆ†æå†…å®¹ã€‚" * ((words_per_section - current_length) // 8)
        
        draft += "\n\n"
    
    return draft

def determine_writing_style(topic: str, audience: str) -> str:
    """ç¡®å®šå†™ä½œé£æ ¼"""
    style_map = {
        "general": "é€šä¿—æ˜“æ‡‚",
        "professional": "ä¸“ä¸šä¸¥è°¨", 
        "academic": "å­¦æœ¯è§„èŒƒ",
        "casual": "è½»æ¾æ´»æ³¼"
    }
    return style_map.get(audience, "é€šä¿—æ˜“æ‡‚")

# ç¼–è¾‘ä»£ç†
def content_editor(state: ContentStudioState):
    """âœï¸ å†…å®¹ç¼–è¾‘ï¼šæ”¹è¿›æ–‡ç« è´¨é‡"""
    draft = state.get("draft_content", "")
    style = state.get("writing_style", "é€šä¿—æ˜“æ‡‚")
    
    print(f"âœï¸ ç¼–è¾‘å¼€å§‹å·¥ä½œ...")
    print(f"   åŸç¨¿é•¿åº¦: {len(draft)} å­—ç¬¦")
    print(f"   ç›®æ ‡é£æ ¼: {style}")
    
    # åˆ†æç°æœ‰å†…å®¹
    feedback = analyze_content_issues(draft)
    print(f"   å‘ç° {len(feedback)} ä¸ªæ”¹è¿›ç‚¹")
    
    # ä¿®è®¢å†…å®¹
    revised = revise_content(draft, feedback, style)
    print(f"   ä¿®è®¢å®Œæˆ: {len(revised)} å­—ç¬¦")
    
    # è¯„ä¼°å¯è¯»æ€§
    readability = assess_readability(revised)
    print(f"   å¯è¯»æ€§è¯„åˆ†: {readability:.2f}")
    
    return {
        "editing_feedback": feedback,
        "revised_content": revised,
        "readability_score": readability,
        "current_phase": ContentPhase.EDITING,
        "completed_agents": state.get("completed_agents", []) + ["editor"],
        "overall_progress": 0.6
    }

def analyze_content_issues(content: str) -> List[str]:
    """åˆ†æå†…å®¹é—®é¢˜"""
    issues = []
    
    # æ£€æŸ¥ç»“æ„é—®é¢˜
    if content.count("##") < 3:
        issues.append("ç« èŠ‚ç»“æ„ä¸å¤Ÿæ¸…æ™°ï¼Œå»ºè®®å¢åŠ å°æ ‡é¢˜")
    
    # æ£€æŸ¥é•¿åº¦é—®é¢˜
    paragraphs = content.split("\n\n")
    long_paragraphs = [p for p in paragraphs if len(p) > 500]
    if long_paragraphs:
        issues.append(f"æœ‰ {len(long_paragraphs)} ä¸ªæ®µè½è¿‡é•¿ï¼Œå»ºè®®åˆ†æ®µ")
    
    # æ£€æŸ¥è¯­è¨€é—®é¢˜
    if content.count("ã€‚") < content.count("\n") * 2:
        issues.append("å¥å­ç»“æ„å¯ä»¥æ›´åŠ ä¸°å¯Œå¤šæ ·")
    
    # æ£€æŸ¥è¿‡æ¸¡é—®é¢˜
    if "å¦å¤–" not in content and "æ­¤å¤–" not in content and "å› æ­¤" not in content:
        issues.append("æ®µè½é—´ç¼ºä¹é€»è¾‘è¿‡æ¸¡è¯")
    
    return issues

def revise_content(content: str, feedback: List[str], style: str) -> str:
    """ä¿®è®¢å†…å®¹"""
    revised = content
    
    # æ ¹æ®åé¦ˆè¿›è¡Œä¿®è®¢
    for issue in feedback:
        if "ç« èŠ‚ç»“æ„" in issue:
            # æ”¹è¿›ç« èŠ‚ç»“æ„ï¼ˆç®€åŒ–å®ç°ï¼‰
            revised = revised.replace("ã€‚", "ã€‚\n\n", 1)  # å¢åŠ æ®µè½åˆ†éš”
        
        if "æ®µè½è¿‡é•¿" in issue:
            # åˆ†å‰²é•¿æ®µè½
            paragraphs = revised.split("\n\n")
            new_paragraphs = []
            for p in paragraphs:
                if len(p) > 500:
                    # ç®€å•åˆ†å‰²
                    mid = len(p) // 2
                    new_paragraphs.extend([p[:mid], p[mid:]])
                else:
                    new_paragraphs.append(p)
            revised = "\n\n".join(new_paragraphs)
        
        if "è¿‡æ¸¡è¯" in issue:
            # æ·»åŠ è¿‡æ¸¡è¯
            revised = revised.replace("## ", "## æ­¤å¤–ï¼Œ", 1)
    
    # æ ¹æ®é£æ ¼è°ƒæ•´
    style_adjustments = {
        "ä¸“ä¸šä¸¥è°¨": lambda x: x.replace("å¾ˆ", "ç›¸å½“").replace("éå¸¸", "æå…¶"),
        "é€šä¿—æ˜“æ‡‚": lambda x: x.replace("å› æ­¤", "æ‰€ä»¥").replace("æ­¤å¤–", "å¦å¤–"),
        "å­¦æœ¯è§„èŒƒ": lambda x: x.replace("æˆ‘ä»¬", "æœ¬ç ”ç©¶").replace("è§‰å¾—", "è®¤ä¸º")
    }
    
    if style in style_adjustments:
        revised = style_adjustments[style](revised)
    
    return revised

def assess_readability(content: str) -> float:
    """è¯„ä¼°å¯è¯»æ€§"""
    # ç®€åŒ–çš„å¯è¯»æ€§è¯„ä¼°
    sentences = content.count("ã€‚")
    words = len(content.split())
    paragraphs = content.count("\n\n")
    
    if sentences == 0 or paragraphs == 0:
        return 0.0
    
    avg_sentence_length = words / sentences
    avg_paragraph_length = sentences / paragraphs
    
    # ç†æƒ³çš„å¥å­é•¿åº¦æ˜¯15-25å­—ï¼Œæ®µè½é•¿åº¦æ˜¯3-5å¥
    sentence_score = max(0, 1 - abs(avg_sentence_length - 20) / 20)
    paragraph_score = max(0, 1 - abs(avg_paragraph_length - 4) / 4)
    
    return (sentence_score + paragraph_score) / 2

# è´¨æ£€å‘˜ä»£ç†
def quality_checker(state: ContentStudioState):
    """ğŸ” è´¨é‡æ£€æŸ¥å‘˜ï¼šéªŒè¯å†…å®¹è´¨é‡"""
    content = state.get("revised_content", "")
    original_topic = state["content_topic"]
    background_info = state.get("background_info", [])
    
    print(f"ğŸ” è´¨æ£€å‘˜å¼€å§‹å·¥ä½œ...")
    print(f"   æ£€æŸ¥å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    
    # äº‹å®æ£€æŸ¥
    fact_results = perform_fact_check(content, background_info)
    print(f"   äº‹å®æ£€æŸ¥: {len(fact_results)} ä¸ªæ£€æŸ¥ç‚¹")
    
    # è´¨é‡è¯„ä¼°
    quality = assess_overall_quality(content, original_topic)
    print(f"   æ•´ä½“è´¨é‡: {quality.value}")
    
    # æ”¹è¿›å»ºè®®
    suggestions = generate_improvement_suggestions(content, fact_results, quality)
    print(f"   æ”¹è¿›å»ºè®®: {len(suggestions)} æ¡")
    
    return {
        "fact_check_results": fact_results,
        "quality_assessment": quality,
        "improvement_suggestions": suggestions,
        "current_phase": ContentPhase.QUALITY_CHECK,
        "completed_agents": state.get("completed_agents", []) + ["quality_checker"],
        "overall_progress": 0.8
    }

def perform_fact_check(content: str, background_info: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """æ‰§è¡Œäº‹å®æ£€æŸ¥"""
    results = []
    
    # æ£€æŸ¥å…³é”®å£°æ˜
    claims = extract_claims(content)
    
    for claim in claims:
        # åœ¨èƒŒæ™¯ä¿¡æ¯ä¸­éªŒè¯
        supporting_info = [info for info in background_info 
                          if any(word in info.get("content", "") for word in claim.split()[:3])]
        
        if supporting_info:
            confidence = sum(info.get("credibility", 0) for info in supporting_info) / len(supporting_info)
            results.append({
                "claim": claim,
                "verified": confidence > 0.7,
                "confidence": confidence,
                "sources": len(supporting_info)
            })
        else:
            results.append({
                "claim": claim,
                "verified": False,
                "confidence": 0.0,
                "sources": 0
            })
    
    return results

def extract_claims(content: str) -> List[str]:
    """æå–å…³é”®å£°æ˜"""
    # ç®€åŒ–çš„å£°æ˜æå–
    sentences = [s.strip() for s in content.split("ã€‚") if s.strip()]
    
    # è¿‡æ»¤å‡ºå¯èƒ½åŒ…å«äº‹å®å£°æ˜çš„å¥å­
    claim_indicators = ["ç ”ç©¶è¡¨æ˜", "æ•°æ®æ˜¾ç¤º", "æ ¹æ®", "è°ƒæŸ¥å‘ç°", "ä¸“å®¶è®¤ä¸º"]
    claims = []
    
    for sentence in sentences:
        if any(indicator in sentence for indicator in claim_indicators):
            claims.append(sentence)
    
    return claims[:5]  # æœ€å¤šæ£€æŸ¥5ä¸ªå…³é”®å£°æ˜

def assess_overall_quality(content: str, topic: str) -> ContentQuality:
    """è¯„ä¼°æ•´ä½“è´¨é‡"""
    score = 0
    
    # å†…å®¹é•¿åº¦è¯„åˆ†
    if len(content) > 1000:
        score += 25
    elif len(content) > 500:
        score += 15
    
    # ç»“æ„è¯„åˆ†
    if content.count("##") >= 3:
        score += 25
    
    # ä¸»é¢˜ç›¸å…³æ€§è¯„åˆ†
    topic_words = topic.split()
    topic_mentions = sum(content.lower().count(word.lower()) for word in topic_words)
    if topic_mentions >= 5:
        score += 25
    elif topic_mentions >= 3:
        score += 15
    
    # è¯­è¨€è´¨é‡è¯„åˆ†
    if len(content.split("ã€‚")) > 10:  # æœ‰è¶³å¤Ÿçš„å¥å­
        score += 25
    
    # è½¬æ¢ä¸ºè´¨é‡ç­‰çº§
    if score >= 80:
        return ContentQuality.EXCELLENT
    elif score >= 60:
        return ContentQuality.GOOD
    elif score >= 40:
        return ContentQuality.FAIR
    else:
        return ContentQuality.POOR

def generate_improvement_suggestions(content: str, fact_results: List[Dict[str, Any]], 
                                   quality: ContentQuality) -> List[str]:
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    # åŸºäºäº‹å®æ£€æŸ¥çš„å»ºè®®
    unverified_claims = [r for r in fact_results if not r["verified"]]
    if unverified_claims:
        suggestions.append(f"æœ‰ {len(unverified_claims)} ä¸ªå£°æ˜ç¼ºä¹å¯é æ¥æºæ”¯æŒï¼Œå»ºè®®è¡¥å……å¼•ç”¨")
    
    # åŸºäºè´¨é‡è¯„ä¼°çš„å»ºè®®
    if quality == ContentQuality.POOR:
        suggestions.extend([
            "å†…å®¹é•¿åº¦ä¸è¶³ï¼Œå»ºè®®æ‰©å……è¯¦ç»†ä¿¡æ¯",
            "ç»“æ„ä¸å¤Ÿæ¸…æ™°ï¼Œå»ºè®®å¢åŠ ç« èŠ‚æ ‡é¢˜",
            "ä¸»é¢˜ç›¸å…³æ€§è¾ƒä½ï¼Œå»ºè®®èšç„¦æ ¸å¿ƒè¯é¢˜"
        ])
    elif quality == ContentQuality.FAIR:
        suggestions.extend([
            "å¯ä»¥å¢åŠ æ›´å¤šå…·ä½“æ¡ˆä¾‹å’Œæ•°æ®æ”¯æŒ",
            "å»ºè®®æ”¹è¿›æ®µè½é—´çš„é€»è¾‘è¿æ¥"
        ])
    elif quality == ContentQuality.GOOD:
        suggestions.append("æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ å›¾è¡¨æˆ–è§†è§‰å…ƒç´ ")
    
    return suggestions

# è®¾è®¡å¸ˆä»£ç†ï¼ˆå·¥å…·é›†æˆç¤ºä¾‹ï¼‰
def content_designer(state: ContentStudioState):
    """ğŸ¨ å†…å®¹è®¾è®¡å¸ˆï¼šè§†è§‰ä¼˜åŒ–å’Œæ ¼å¼è®¾è®¡"""
    content = state.get("revised_content", "")
    topic = state["content_topic"]
    
    print(f"ğŸ¨ è®¾è®¡å¸ˆå¼€å§‹å·¥ä½œ...")
    print(f"   ä¼˜åŒ–å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    
    # è¯†åˆ«è§†è§‰éœ€æ±‚
    visual_needs = identify_visual_needs(content)
    print(f"   è¯†åˆ«è§†è§‰éœ€æ±‚: {visual_needs}")
    
    # ç”Ÿæˆè§†è§‰å…ƒç´ ï¼ˆæ¨¡æ‹Ÿå·¥å…·è°ƒç”¨ï¼‰
    visual_elements = generate_visual_elements(visual_needs, topic)
    print(f"   ç”Ÿæˆ {len(visual_elements)} ä¸ªè§†è§‰å…ƒç´ ")
    
    # æ ¼å¼åŒ–å†…å®¹
    formatted = format_content_with_visuals(content, visual_elements)
    print(f"   æ ¼å¼åŒ–å®Œæˆ")
    
    return {
        "visual_elements": visual_needs,
        "formatted_content": formatted,
        "design_assets": visual_elements,
        "current_phase": ContentPhase.DESIGN,
        "completed_agents": state.get("completed_agents", []) + ["designer"],
        "overall_progress": 1.0,
        "final_output": formatted
    }

def identify_visual_needs(content: str) -> List[str]:
    """è¯†åˆ«è§†è§‰éœ€æ±‚"""
    needs = []
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å›¾è¡¨
    if any(word in content for word in ["æ•°æ®", "ç»Ÿè®¡", "æ¯”ä¾‹", "å¢é•¿"]):
        needs.append("æ•°æ®å›¾è¡¨")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æµç¨‹å›¾
    if any(word in content for word in ["æ­¥éª¤", "æµç¨‹", "è¿‡ç¨‹", "é˜¶æ®µ"]):
        needs.append("æµç¨‹å›¾")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é…å›¾
    if any(word in content for word in ["åº”ç”¨", "æ¡ˆä¾‹", "å®ä¾‹"]):
        needs.append("ç¤ºä¾‹å›¾ç‰‡")
    
    # åŸºç¡€éœ€æ±‚
    needs.append("æ ‡é¢˜è®¾è®¡")
    needs.append("æ’ç‰ˆä¼˜åŒ–")
    
    return needs

def generate_visual_elements(needs: List[str], topic: str) -> List[str]:
    """ç”Ÿæˆè§†è§‰å…ƒç´ ï¼ˆæ¨¡æ‹Ÿå·¥å…·è°ƒç”¨ï¼‰"""
    elements = []
    
    for need in needs:
        if need == "æ•°æ®å›¾è¡¨":
            elements.append(f"chart_{topic}_data.png")
        elif need == "æµç¨‹å›¾":
            elements.append(f"flowchart_{topic}_process.png")
        elif need == "ç¤ºä¾‹å›¾ç‰‡":
            elements.append(f"example_{topic}_case.jpg")
        elif need == "æ ‡é¢˜è®¾è®¡":
            elements.append(f"title_{topic}_header.png")
        elif need == "æ’ç‰ˆä¼˜åŒ–":
            elements.append("layout_template.css")
    
    return elements

def format_content_with_visuals(content: str, visual_elements: List[str]) -> str:
    """æ ¼å¼åŒ–å†…å®¹å¹¶æ’å…¥è§†è§‰å…ƒç´ """
    formatted = content
    
    # åœ¨æ¯ä¸ªä¸»è¦ç« èŠ‚åæ’å…¥ç›¸å…³å›¾ç‰‡
    sections = formatted.split("## ")
    enhanced_sections = []
    
    for i, section in enumerate(sections):
        enhanced_sections.append(section)
        
        # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ åˆé€‚çš„è§†è§‰å…ƒç´ 
        if i < len(visual_elements):
            visual_element = visual_elements[i]
            enhanced_sections.append(f"\n\n![ç›¸å…³å›¾ç‰‡]({visual_element})\n\n")
    
    # æ·»åŠ æ ·å¼å’Œå¸ƒå±€
    formatted = "## ".join(enhanced_sections)
    formatted = f"""
<style>
body {{ font-family: Arial, sans-serif; line-height: 1.6; }}
h2 {{ color: #2c3e50; border-bottom: 2px solid #3498db; }}
img {{ max-width: 100%; height: auto; margin: 20px 0; }}
</style>

{formatted}

<footer>
<p><em>æœ¬å†…å®¹ç”±AIå¤šä»£ç†åä½œç³»ç»Ÿç”Ÿæˆ</em></p>
</footer>
"""
    
    return formatted

# æ„å»ºå®Œæ•´çš„å†…å®¹åˆ›ä½œå·¥ä½œå®¤
def create_content_studio():
    """åˆ›å»ºæ™ºèƒ½å†…å®¹åˆ›ä½œå·¥ä½œå®¤"""
    workflow = StateGraph(ContentStudioState)
    
    # æ·»åŠ æ‰€æœ‰ä»£ç†
    workflow.add_node("researcher", content_researcher)
    workflow.add_node("writer", content_writer)
    workflow.add_node("editor", content_editor)
    workflow.add_node("quality_checker", quality_checker)
    workflow.add_node("designer", content_designer)
    
    # å®šä¹‰æµæ°´çº¿åä½œæµç¨‹
    workflow.set_entry_point("researcher")
    workflow.add_edge("researcher", "writer")
    workflow.add_edge("writer", "editor")
    workflow.add_edge("editor", "quality_checker")
    workflow.add_edge("quality_checker", "designer")
    workflow.add_edge("designer", END)
    
    return workflow.compile()

# æµ‹è¯•å®Œæ•´çš„å¤šä»£ç†åä½œç³»ç»Ÿ
if __name__ == "__main__":
    app = create_content_studio()
    
    # æµ‹è¯•å†…å®¹åˆ›ä½œä»»åŠ¡
    result = app.invoke({
        "content_topic": "åŒºå—é“¾æŠ€æœ¯åœ¨ä¾›åº”é“¾ç®¡ç†ä¸­çš„åº”ç”¨",
        "content_type": "article",
        "target_audience": "professional",
        "word_count_target": 1500
    })
    
    print("\n" + "="*60)
    print("ğŸ¯ å¤šä»£ç†å†…å®¹åˆ›ä½œå·¥ä½œå®¤ç»“æœ")
    print("="*60)
    print(f"ä¸»é¢˜: {result['content_topic']}")
    print(f"å®Œæˆä»£ç†: {result['completed_agents']}")
    print(f"æ•´ä½“è¿›åº¦: {result['overall_progress']*100}%")
    print(f"è´¨é‡è¯„ä¼°: {result['quality_assessment']}")
    
    print(f"\nğŸ“Š å„ä»£ç†å·¥ä½œæˆæœ:")
    print(f"ğŸ” ç ”ç©¶å…³é”®è¯: {result['research_keywords']}")
    print(f"âœï¸ å†…å®¹å¤§çº²: {result['content_outline']}")
    print(f"âœï¸ ç¼–è¾‘åé¦ˆ: {result['editing_feedback']}")
    print(f"ğŸ” è´¨æ£€å»ºè®®: {result['improvement_suggestions']}")
    print(f"ğŸ¨ è§†è§‰å…ƒç´ : {result['visual_elements']}")
    
    print(f"\nğŸ“„ æœ€ç»ˆæˆå“é¢„è§ˆ:")
    print(result['final_output'][:500] + "..." if len(result['final_output']) > 500 else result['final_output'])
```

## ğŸ› ï¸ å·¥å…·é›†æˆï¼šæ‰©å±•ä»£ç†èƒ½åŠ›

å·¥å…·é›†æˆæ˜¯å¤šä»£ç†ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå®ƒè®©ä»£ç†èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨æœåŠ¡å’Œå·¥å…·æ¥æ‰©å±•è‡ªå·±çš„èƒ½åŠ›ã€‚

### å·¥å…·ç³»ç»Ÿæ¶æ„

```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

# å·¥å…·åŸºç¡€æ¥å£
class Tool(ABC):
    """å·¥å…·åŸºç¡€ç±»"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """å·¥å…·åç§°"""
        pass
    
    @property
    @abstractmethod 
    def description(self) -> str:
        """å·¥å…·æè¿°"""
        pass
    
    @abstractmethod
    def execute(self, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·"""
        pass

# æœç´¢å·¥å…·å®ç°
class WebSearchTool(Tool):
    """ç½‘ç»œæœç´¢å·¥å…·"""
    
    @property
    def name(self) -> str:
        return "web_search"
    
    @property
    def description(self) -> str:
        return "æœç´¢äº’è”ç½‘è·å–æœ€æ–°ä¿¡æ¯"
    
    def execute(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­ä¼šè°ƒç”¨çœŸå®çš„æœç´¢API
        print(f"ğŸ” æœç´¢: {query}")
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        results = [
            {
                "title": f"å…³äº{query}çš„æœ€æ–°ç ”ç©¶",
                "url": f"https://example.com/research/{query}",
                "snippet": f"æœ€æ–°ç ”ç©¶è¡¨æ˜ï¼Œ{query}åœ¨ç›¸å…³é¢†åŸŸæ˜¾ç¤ºå‡ºé‡è¦è¿›å±•...",
                "relevance": 0.9
            },
            {
                "title": f"{query}çš„å®é™…åº”ç”¨æ¡ˆä¾‹",
                "url": f"https://example.com/cases/{query}",
                "snippet": f"åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ{query}å·²ç»è¢«æˆåŠŸè¿ç”¨åˆ°...",
                "relevance": 0.8
            }
        ][:max_results]
        
        return {
            "success": True,
            "results": results,
            "total_found": len(results),
            "query": query
        }

# æ•°æ®åˆ†æå·¥å…·
class DataAnalysisTool(Tool):
    """æ•°æ®åˆ†æå·¥å…·"""
    
    @property
    def name(self) -> str:
        return "data_analysis"
    
    @property
    def description(self) -> str:
        return "åˆ†ææ•°æ®å¹¶ç”Ÿæˆå›¾è¡¨"
    
    def execute(self, data: List[Dict[str, Any]], analysis_type: str = "summary") -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åˆ†æ"""
        print(f"ğŸ“Š åˆ†ææ•°æ®: {len(data)} æ¡è®°å½•, ç±»å‹: {analysis_type}")
        
        if analysis_type == "summary":
            return self._generate_summary(data)
        elif analysis_type == "trend":
            return self._analyze_trend(data)
        else:
            return {"success": False, "error": "ä¸æ”¯æŒçš„åˆ†æç±»å‹"}
    
    def _generate_summary(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®æ‘˜è¦"""
        return {
            "success": True,
            "summary": {
                "total_records": len(data),
                "avg_relevance": sum(item.get("relevance", 0) for item in data) / len(data) if data else 0,
                "top_sources": list(set(item.get("source", "unknown") for item in data))[:3]
            },
            "chart_url": "data_summary_chart.png"
        }
    
    def _analyze_trend(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿"""
        return {
            "success": True,
            "trend": {
                "direction": "ä¸Šå‡",
                "confidence": 0.85,
                "key_factors": ["æŠ€æœ¯è¿›æ­¥", "å¸‚åœºéœ€æ±‚", "æ”¿ç­–æ”¯æŒ"]
            },
            "chart_url": "trend_analysis_chart.png"
        }

# å›¾ç‰‡ç”Ÿæˆå·¥å…·
class ImageGenerationTool(Tool):
    """å›¾ç‰‡ç”Ÿæˆå·¥å…·"""
    
    @property
    def name(self) -> str:
        return "image_generation"
    
    @property
    def description(self) -> str:
        return "æ ¹æ®æè¿°ç”Ÿæˆå›¾ç‰‡"
    
    def execute(self, description: str, style: str = "realistic") -> Dict[str, Any]:
        """ç”Ÿæˆå›¾ç‰‡"""
        print(f"ğŸ¨ ç”Ÿæˆå›¾ç‰‡: {description}, é£æ ¼: {style}")
        
        # æ¨¡æ‹Ÿå›¾ç‰‡ç”Ÿæˆ
        filename = f"generated_{hash(description) % 10000}.png"
        
        return {
            "success": True,
            "image_url": filename,
            "description": description,
            "style": style,
            "dimensions": "1024x768"
        }

# å·¥å…·ç®¡ç†å™¨
class ToolManager:
    """å·¥å…·ç®¡ç†å™¨"""
    
    def __init__(self):
        self.tools: Dict[str, Tool] = {}
    
    def register_tool(self, tool: Tool):
        """æ³¨å†Œå·¥å…·"""
        self.tools[tool.name] = tool
        print(f"âœ… æ³¨å†Œå·¥å…·: {tool.name}")
    
    def get_tool(self, name: str) -> Optional[Tool]:
        """è·å–å·¥å…·"""
        return self.tools.get(name)
    
    def list_tools(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·¥å…·"""
        return list(self.tools.keys())
    
    def execute_tool(self, tool_name: str, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·"""
        tool = self.get_tool(tool_name)
        if not tool:
            return {"success": False, "error": f"å·¥å…· {tool_name} ä¸å­˜åœ¨"}
        
        try:
            return tool.execute(**kwargs)
        except Exception as e:
            return {"success": False, "error": str(e)}

# å¸¦å·¥å…·é›†æˆçš„ä»£ç†çŠ¶æ€
class ToolIntegratedState(TypedDict):
    user_query: str
    search_results: NotRequired[List[Dict[str, Any]]]
    analysis_results: NotRequired[Dict[str, Any]]
    generated_images: NotRequired[List[str]]
    tool_usage_log: NotRequired[List[Dict[str, Any]]]
    final_response: NotRequired[str]

# å¸¦å·¥å…·çš„æœç´¢ä»£ç†
def tool_enhanced_researcher(state: ToolIntegratedState):
    """ğŸ” å·¥å…·å¢å¼ºçš„ç ”ç©¶ä»£ç†"""
    query = state["user_query"]
    
    print(f"ğŸ” å¯åŠ¨å·¥å…·å¢å¼ºç ”ç©¶ä»£ç†...")
    
    # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨
    tool_manager = ToolManager()
    tool_manager.register_tool(WebSearchTool())
    tool_manager.register_tool(DataAnalysisTool())
    
    # æ‰§è¡Œæœç´¢
    search_result = tool_manager.execute_tool("web_search", query=query, max_results=3)
    
    tool_log = [{
        "tool": "web_search",
        "input": {"query": query},
        "output": search_result,
        "timestamp": "2024-01-01T10:00:00"
    }]
    
    if search_result["success"]:
        # åˆ†ææœç´¢ç»“æœ
        analysis_result = tool_manager.execute_tool(
            "data_analysis", 
            data=search_result["results"], 
            analysis_type="summary"
        )
        
        tool_log.append({
            "tool": "data_analysis", 
            "input": {"data": search_result["results"]},
            "output": analysis_result,
            "timestamp": "2024-01-01T10:01:00"
        })
        
        return {
            "search_results": search_result["results"],
            "analysis_results": analysis_result if analysis_result["success"] else None,
            "tool_usage_log": tool_log
        }
    else:
        return {
            "search_results": [],
            "tool_usage_log": tool_log,
            "error": "æœç´¢å¤±è´¥"
        }

# å¸¦å·¥å…·çš„å†…å®¹åˆ›ä½œä»£ç†
def tool_enhanced_creator(state: ToolIntegratedState):
    """âœï¸ å·¥å…·å¢å¼ºçš„å†…å®¹åˆ›ä½œä»£ç†"""
    search_results = state.get("search_results", [])
    analysis = state.get("analysis_results", {})
    
    print(f"âœï¸ å¯åŠ¨å·¥å…·å¢å¼ºåˆ›ä½œä»£ç†...")
    
    # åˆå§‹åŒ–å·¥å…·
    tool_manager = ToolManager()
    tool_manager.register_tool(ImageGenerationTool())
    
    # ç”Ÿæˆå†…å®¹
    content = create_enhanced_content(search_results, analysis)
    
    # ç”Ÿæˆé…å›¾
    images = []
    if search_results:
        for result in search_results[:2]:  # ä¸ºå‰ä¸¤ä¸ªç»“æœç”Ÿæˆé…å›¾
            img_result = tool_manager.execute_tool(
                "image_generation",
                description=f"æ’å›¾: {result['title']}",
                style="professional"
            )
            if img_result["success"]:
                images.append(img_result["image_url"])
    
    tool_log = state.get("tool_usage_log", [])
    tool_log.append({
        "tool": "image_generation",
        "input": {"count": len(images)},
        "output": {"generated_images": images},
        "timestamp": "2024-01-01T10:02:00"
    })
    
    return {
        "generated_images": images,
        "final_response": content,
        "tool_usage_log": tool_log
    }

def create_enhanced_content(search_results: List[Dict[str, Any]], 
                          analysis: Dict[str, Any]) -> str:
    """åˆ›å»ºå¢å¼ºå†…å®¹"""
    content = "# ç ”ç©¶æŠ¥å‘Š\n\n"
    
    if analysis and "summary" in analysis:
        summary = analysis["summary"]
        content += f"## æ•°æ®æ¦‚è§ˆ\n"
        content += f"- æ€»è®¡åˆ†æäº† {summary['total_records']} ä¸ªä¿¡æ¯æº\n"
        content += f"- å¹³å‡ç›¸å…³åº¦: {summary['avg_relevance']:.2f}\n"
        content += f"- ä¸»è¦æ¥æº: {', '.join(summary['top_sources'])}\n\n"
    
    content += "## è¯¦ç»†å‘ç°\n\n"
    for i, result in enumerate(search_results, 1):
        content += f"### {i}. {result['title']}\n"
        content += f"{result['snippet']}\n"
        content += f"*æ¥æº: {result['url']}*\n\n"
    
    return content

# åˆ›å»ºå·¥å…·é›†æˆçš„å·¥ä½œæµ
def create_tool_integrated_workflow():
    """åˆ›å»ºå·¥å…·é›†æˆå·¥ä½œæµ"""
    workflow = StateGraph(ToolIntegratedState)
    
    workflow.add_node("researcher", tool_enhanced_researcher)
    workflow.add_node("creator", tool_enhanced_creator)
    
    workflow.set_entry_point("researcher")
    workflow.add_edge("researcher", "creator")
    workflow.add_edge("creator", END)
    
    return workflow.compile()

# æµ‹è¯•å·¥å…·é›†æˆ
if __name__ == "__main__":
    app = create_tool_integrated_workflow()
    
    result = app.invoke({
        "user_query": "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„æœ€æ–°åº”ç”¨"
    })
    
    print("\n" + "="*50)
    print("ğŸ› ï¸ å·¥å…·é›†æˆæµ‹è¯•ç»“æœ")
    print("="*50)
    print(f"æŸ¥è¯¢: {result['user_query']}")
    print(f"æœç´¢ç»“æœ: {len(result.get('search_results', []))} æ¡")
    print(f"ç”Ÿæˆå›¾ç‰‡: {len(result.get('generated_images', []))} å¼ ")
    print(f"å·¥å…·ä½¿ç”¨è®°å½•: {len(result.get('tool_usage_log', []))} æ¬¡")
    
    print(f"\nğŸ› ï¸ å·¥å…·ä½¿ç”¨è¯¦æƒ…:")
    for log in result.get('tool_usage_log', []):
        print(f"- {log['tool']}: {log.get('timestamp', 'N/A')}")
    
    print(f"\nğŸ“„ æœ€ç»ˆå†…å®¹:")
    print(result.get('final_response', 'æ— å†…å®¹'))
```

## ğŸ”„ é«˜çº§åä½œæ¨¡å¼

### å¹¶è¡Œåä½œæ¨¡å¼

æœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦å¤šä¸ªä»£ç†åŒæ—¶å·¥ä½œä»¥æé«˜æ•ˆç‡ï¼š

```python
class ParallelResearchState(TypedDict):
    research_topic: str
    
    # å¹¶è¡Œç ”ç©¶ç»“æœ
    tech_research: NotRequired[Dict[str, Any]]
    market_research: NotRequired[Dict[str, Any]]
    academic_research: NotRequired[Dict[str, Any]]
    
    # åˆå¹¶ç»“æœ
    combined_insights: NotRequired[str]
    final_report: NotRequired[str]

def tech_researcher(state: ParallelResearchState):
    """ğŸ”§ æŠ€æœ¯ç ”ç©¶å‘˜"""
    topic = state["research_topic"]
    print(f"ğŸ”§ æŠ€æœ¯ç ”ç©¶å‘˜åˆ†æ: {topic}")
    
    # æ¨¡æ‹ŸæŠ€æœ¯è§’åº¦çš„ç ”ç©¶
    tech_insights = {
        "technical_feasibility": 0.8,
        "implementation_complexity": "ä¸­ç­‰",
        "required_technologies": ["AI", "äº‘è®¡ç®—", "å¤§æ•°æ®"],
        "technical_challenges": ["æ•°æ®å®‰å…¨", "ç³»ç»Ÿé›†æˆ", "æ€§èƒ½ä¼˜åŒ–"]
    }
    
    return {"tech_research": tech_insights}

def market_researcher(state: ParallelResearchState):
    """ğŸ“ˆ å¸‚åœºç ”ç©¶å‘˜"""
    topic = state["research_topic"]
    print(f"ğŸ“ˆ å¸‚åœºç ”ç©¶å‘˜åˆ†æ: {topic}")
    
    # æ¨¡æ‹Ÿå¸‚åœºè§’åº¦çš„ç ”ç©¶
    market_insights = {
        "market_size": "100äº¿ç¾å…ƒ",
        "growth_rate": "15%å¹´å¢é•¿",
        "key_players": ["å…¬å¸A", "å…¬å¸B", "å…¬å¸C"],
        "market_trends": ["è‡ªåŠ¨åŒ–", "ä¸ªæ€§åŒ–", "ç§»åŠ¨åŒ–"]
    }
    
    return {"market_research": market_insights}

def academic_researcher(state: ParallelResearchState):
    """ğŸ“ å­¦æœ¯ç ”ç©¶å‘˜"""
    topic = state["research_topic"]
    print(f"ğŸ“ å­¦æœ¯ç ”ç©¶å‘˜åˆ†æ: {topic}")
    
    # æ¨¡æ‹Ÿå­¦æœ¯è§’åº¦çš„ç ”ç©¶
    academic_insights = {
        "research_papers": 150,
        "key_theories": ["ç†è®ºA", "ç†è®ºB"],
        "research_gaps": ["ç¼ºä¹é•¿æœŸç ”ç©¶", "æ ·æœ¬è§„æ¨¡æœ‰é™"],
        "future_directions": ["è·¨å­¦ç§‘ç ”ç©¶", "å®è¯ç ”ç©¶"]
    }
    
    return {"academic_research": academic_insights}

def insight_synthesizer(state: ParallelResearchState):
    """ğŸ§  æ´å¯Ÿç»¼åˆå‘˜"""
    tech = state.get("tech_research", {})
    market = state.get("market_research", {})
    academic = state.get("academic_research", {})
    
    print(f"ğŸ§  ç»¼åˆå¤šç»´åº¦ç ”ç©¶ç»“æœ...")
    
    # ç»¼åˆä¸‰ä¸ªç»´åº¦çš„ç ”ç©¶ç»“æœ
    combined = f"""
# ç»¼åˆç ”ç©¶æ´å¯Ÿ

## æŠ€æœ¯ç»´åº¦
- æŠ€æœ¯å¯è¡Œæ€§: {tech.get('technical_feasibility', 'N/A')}
- å®ç°å¤æ‚åº¦: {tech.get('implementation_complexity', 'N/A')}
- å…³é”®æŠ€æœ¯: {', '.join(tech.get('required_technologies', []))}

## å¸‚åœºç»´åº¦  
- å¸‚åœºè§„æ¨¡: {market.get('market_size', 'N/A')}
- å¢é•¿ç‡: {market.get('growth_rate', 'N/A')}
- å¸‚åœºè¶‹åŠ¿: {', '.join(market.get('market_trends', []))}

## å­¦æœ¯ç»´åº¦
- ç›¸å…³è®ºæ–‡: {academic.get('research_papers', 'N/A')} ç¯‡
- æ ¸å¿ƒç†è®º: {', '.join(academic.get('key_theories', []))}
- ç ”ç©¶æ–¹å‘: {', '.join(academic.get('future_directions', []))}

## ç»¼åˆå»ºè®®
åŸºäºæŠ€æœ¯ã€å¸‚åœºå’Œå­¦æœ¯ä¸‰ä¸ªç»´åº¦çš„åˆ†æï¼Œå»ºè®®é‡‡ç”¨æ¸è¿›å¼å‘å±•ç­–ç•¥...
"""
    
    return {
        "combined_insights": combined,
        "final_report": combined
    }

def create_parallel_research_workflow():
    """åˆ›å»ºå¹¶è¡Œç ”ç©¶å·¥ä½œæµ"""
    workflow = StateGraph(ParallelResearchState)
    
    # æ·»åŠ å¹¶è¡Œç ”ç©¶èŠ‚ç‚¹
    workflow.add_node("tech_researcher", tech_researcher)
    workflow.add_node("market_researcher", market_researcher)
    workflow.add_node("academic_researcher", academic_researcher)
    workflow.add_node("synthesizer", insight_synthesizer)
    
    # è®¾ç½®å¹¶è¡Œæ‰§è¡Œ
    workflow.set_entry_point("tech_researcher")
    workflow.set_entry_point("market_researcher")
    workflow.set_entry_point("academic_researcher")
    
    # å¹¶è¡Œç»“æœæ±‡æ€»
    workflow.add_edge("tech_researcher", "synthesizer")
    workflow.add_edge("market_researcher", "synthesizer") 
    workflow.add_edge("academic_researcher", "synthesizer")
    workflow.add_edge("synthesizer", END)
    
    return workflow.compile()
```

### åé¦ˆå¾ªç¯åä½œæ¨¡å¼

æœ‰äº›ä»»åŠ¡éœ€è¦ä»£ç†ä¹‹é—´ç›¸äº’åé¦ˆå’Œè¿­ä»£æ”¹è¿›ï¼š

```python
class FeedbackLoopState(TypedDict):
    original_content: str
    
    # è¿­ä»£ç‰ˆæœ¬
    current_version: NotRequired[str]
    iteration_count: NotRequired[int]
    
    # ç¼–è¾‘åé¦ˆ
    editor_feedback: NotRequired[List[str]]
    writer_revisions: NotRequired[List[str]]
    
    # è´¨é‡è·Ÿè¸ª
    quality_scores: NotRequired[List[float]]
    improvement_threshold: NotRequired[float]
    
    # æœ€ç»ˆç»“æœ
    final_version: NotRequired[str]
    improvement_log: NotRequired[List[str]]

def content_writer_v2(state: FeedbackLoopState):
    """âœï¸ å†…å®¹å†™ä½œå‘˜ï¼ˆæ”¯æŒè¿­ä»£ï¼‰"""
    current_content = state.get("current_version", state["original_content"])
    feedback = state.get("editor_feedback", [])
    iteration = state.get("iteration_count", 0)
    
    print(f"âœï¸ å†™ä½œå‘˜å¼€å§‹ç¬¬ {iteration + 1} æ¬¡è¿­ä»£...")
    
    if feedback:
        print(f"   å¤„ç† {len(feedback)} æ¡ç¼–è¾‘åé¦ˆ")
        # æ ¹æ®åé¦ˆæ”¹è¿›å†…å®¹
        revised_content = apply_feedback(current_content, feedback)
    else:
        # é¦–æ¬¡åˆ›ä½œ
        revised_content = create_initial_content(state["original_content"])
    
    revisions = state.get("writer_revisions", [])
    revisions.append(f"ç¬¬{iteration + 1}æ¬¡è¿­ä»£ï¼šåº”ç”¨ç¼–è¾‘åé¦ˆï¼Œæ”¹è¿›å†…å®¹è´¨é‡")
    
    return {
        "current_version": revised_content,
        "iteration_count": iteration + 1,
        "writer_revisions": revisions
    }

def content_editor_v2(state: FeedbackLoopState):
    """âœï¸ å†…å®¹ç¼–è¾‘ï¼ˆæ”¯æŒè¿­ä»£ï¼‰"""
    content = state.get("current_version", "")
    iteration = state.get("iteration_count", 0)
    
    print(f"âœï¸ ç¼–è¾‘å¼€å§‹ç¬¬ {iteration} æ¬¡å®¡æ ¸...")
    
    # åˆ†æå†…å®¹è´¨é‡
    quality_score = evaluate_content_quality(content)
    feedback = generate_editing_feedback(content, iteration)
    
    quality_scores = state.get("quality_scores", [])
    quality_scores.append(quality_score)
    
    print(f"   è´¨é‡è¯„åˆ†: {quality_score:.2f}")
    print(f"   åé¦ˆæ¡æ•°: {len(feedback)}")
    
    return {
        "editor_feedback": feedback,
        "quality_scores": quality_scores
    }

def evaluate_content_quality(content: str) -> float:
    """è¯„ä¼°å†…å®¹è´¨é‡"""
    # åŸºç¡€è´¨é‡æŒ‡æ ‡
    length_score = min(1.0, len(content) / 1000)
    structure_score = 1.0 if content.count("\n\n") >= 3 else 0.5
    
    # è¯­è¨€è´¨é‡ï¼ˆç®€åŒ–è¯„ä¼°ï¼‰
    sentences = content.count("ã€‚")
    words = len(content.split())
    language_score = 0.8 if sentences > 5 and words > 100 else 0.4
    
    return (length_score + structure_score + language_score) / 3

def generate_editing_feedback(content: str, iteration: int) -> List[str]:
    """ç”Ÿæˆç¼–è¾‘åé¦ˆ"""
    feedback = []
    
    # ä¸åŒè¿­ä»£å…³æ³¨ä¸åŒæ–¹é¢
    if iteration == 1:
        # ç¬¬ä¸€æ¬¡è¿­ä»£å…³æ³¨ç»“æ„
        if content.count("##") < 3:
            feedback.append("å¢åŠ ç« èŠ‚æ ‡é¢˜ï¼Œæ”¹å–„æ–‡ç« ç»“æ„")
        if len(content.split("\n\n")) < 5:
            feedback.append("å¢åŠ æ®µè½åˆ†éš”ï¼Œæé«˜å¯è¯»æ€§")
    
    elif iteration == 2:
        # ç¬¬äºŒæ¬¡è¿­ä»£å…³æ³¨å†…å®¹
        if "ä¾‹å¦‚" not in content and "æ¯”å¦‚" not in content:
            feedback.append("æ·»åŠ å…·ä½“ä¾‹å­å’Œæ¡ˆä¾‹")
        if "å› æ­¤" not in content and "æ‰€ä»¥" not in content:
            feedback.append("åŠ å¼ºé€»è¾‘è¿æ¥å’Œè®ºè¯")
    
    elif iteration >= 3:
        # åç»­è¿­ä»£å…³æ³¨ç»†èŠ‚
        if content.count("ï¼Œ") > content.count("ã€‚") * 2:
            feedback.append("ç®€åŒ–å¥å­ç»“æ„ï¼Œæé«˜è¡¨è¾¾æ¸…æ™°åº¦")
        feedback.append("æœ€ç»ˆæ¶¦è‰²ï¼Œæ£€æŸ¥è¯­æ³•å’Œç”¨è¯")
    
    return feedback

def apply_feedback(content: str, feedback: List[str]) -> str:
    """åº”ç”¨ç¼–è¾‘åé¦ˆ"""
    revised = content
    
    for item in feedback:
        if "ç« èŠ‚æ ‡é¢˜" in item:
            # æ·»åŠ ç« èŠ‚æ ‡é¢˜
            if "## " not in revised:
                parts = revised.split("\n\n")
                if len(parts) >= 3:
                    parts[0] = "## å¼•è¨€\n\n" + parts[0]
                    parts[len(parts)//2] = "## ä¸»è¦å†…å®¹\n\n" + parts[len(parts)//2]
                    parts[-1] = "## ç»“è®º\n\n" + parts[-1]
                    revised = "\n\n".join(parts)
        
        elif "æ®µè½åˆ†éš”" in item:
            # å¢åŠ æ®µè½åˆ†éš”
            revised = revised.replace("ã€‚", "ã€‚\n\n", 2)
        
        elif "å…·ä½“ä¾‹å­" in item:
            # æ·»åŠ ä¾‹å­
            revised += "\n\nä¾‹å¦‚ï¼Œåœ¨å®é™…åº”ç”¨ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç›¸å…³çš„æˆåŠŸæ¡ˆä¾‹å’Œå®è·µç»éªŒã€‚"
        
        elif "é€»è¾‘è¿æ¥" in item:
            # åŠ å¼ºé€»è¾‘è¿æ¥
            revised = revised.replace("ã€‚\n\n", "ã€‚å› æ­¤ï¼Œ", 1)
            revised = revised.replace("ã€‚\n\n", "ã€‚æ­¤å¤–ï¼Œ", 1)
    
    return revised

def create_initial_content(topic: str) -> str:
    """åˆ›å»ºåˆå§‹å†…å®¹"""
    return f"""
{topic}æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸã€‚

åœ¨å½“å‰çš„å‘å±•ç¯å¢ƒä¸‹ï¼Œ{topic}æ˜¾ç¤ºå‡ºäº†é‡è¦çš„åº”ç”¨ä»·å€¼å’Œå‘å±•å‰æ™¯ã€‚

ç›¸å…³ç ”ç©¶è¡¨æ˜ï¼Œ{topic}åœ¨å¤šä¸ªæ–¹é¢éƒ½æœ‰æ˜¾è‘—çš„è¿›å±•ã€‚

é€šè¿‡æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°{topic}çš„é‡è¦æ„ä¹‰ã€‚
"""

def should_continue_iteration(state: FeedbackLoopState):
    """åˆ¤æ–­æ˜¯å¦ç»§ç»­è¿­ä»£"""
    iteration = state.get("iteration_count", 0)
    quality_scores = state.get("quality_scores", [])
    threshold = state.get("improvement_threshold", 0.8)
    
    # æœ€å¤šè¿­ä»£4æ¬¡
    if iteration >= 4:
        return False
    
    # è´¨é‡è¾¾æ ‡åˆ™åœæ­¢
    if quality_scores and quality_scores[-1] >= threshold:
        return False
    
    # è´¨é‡æ²¡æœ‰æ˜æ˜¾æ”¹å–„åˆ™åœæ­¢
    if len(quality_scores) >= 2:
        improvement = quality_scores[-1] - quality_scores[-2]
        if improvement < 0.05:
            return False
    
    return True

def finalize_content(state: FeedbackLoopState):
    """æœ€ç»ˆç¡®å®šå†…å®¹"""
    final_content = state.get("current_version", "")
    quality_scores = state.get("quality_scores", [])
    
    improvement_log = []
    if quality_scores:
        initial_quality = quality_scores[0] if len(quality_scores) > 0 else 0
        final_quality = quality_scores[-1]
        improvement = final_quality - initial_quality
        
        improvement_log.append(f"åˆå§‹è´¨é‡: {initial_quality:.2f}")
        improvement_log.append(f"æœ€ç»ˆè´¨é‡: {final_quality:.2f}")
        improvement_log.append(f"è´¨é‡æå‡: {improvement:.2f}")
        improvement_log.append(f"è¿­ä»£æ¬¡æ•°: {len(quality_scores)}")
    
    return {
        "final_version": final_content,
        "improvement_log": improvement_log
    }

def create_feedback_loop_workflow():
    """åˆ›å»ºåé¦ˆå¾ªç¯å·¥ä½œæµ"""
    workflow = StateGraph(FeedbackLoopState)
    
    workflow.add_node("writer", content_writer_v2)
    workflow.add_node("editor", content_editor_v2)
    workflow.add_node("finalizer", finalize_content)
    
    # è®¾ç½®å¾ªç¯æµç¨‹
    workflow.set_entry_point("writer")
    workflow.add_edge("writer", "editor")
    
    # æ¡ä»¶å¾ªç¯ï¼šæ ¹æ®è´¨é‡å†³å®šæ˜¯å¦ç»§ç»­
    workflow.add_conditional_edges(
        "editor",
        should_continue_iteration,
        {
            True: "writer",    # ç»§ç»­è¿­ä»£
            False: "finalizer" # ç»“æŸè¿­ä»£
        }
    )
    
    workflow.add_edge("finalizer", END)
    
    return workflow.compile()

# æµ‹è¯•åé¦ˆå¾ªç¯
if __name__ == "__main__":
    app = create_feedback_loop_workflow()
    
    result = app.invoke({
        "original_content": "äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿",
        "improvement_threshold": 0.75
    })
    
    print("\n" + "="*50)
    print("ğŸ”„ åé¦ˆå¾ªç¯åä½œç»“æœ")
    print("="*50)
    print(f"è¿­ä»£æ¬¡æ•°: {result['iteration_count']}")
    print(f"è´¨é‡å˜åŒ–: {result['quality_scores']}")
    print(f"æ”¹è¿›è®°å½•: {result['improvement_log']}")
    
    print(f"\nğŸ“„ æœ€ç»ˆå†…å®¹:")
    print(result['final_version'])
```

## ğŸ›¡ï¸ ç³»ç»Ÿå®¹é”™ä¸æ¢å¤

å¤šä»£ç†ç³»ç»Ÿéœ€è¦å¼ºå¤§çš„å®¹é”™æœºåˆ¶æ¥å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µï¼š

```python
class RobustAgentState(TypedDict):
    task_description: str
    
    # æ‰§è¡ŒçŠ¶æ€
    current_agent: NotRequired[str]
    completed_agents: NotRequired[List[str]]
    failed_agents: NotRequired[List[str]]
    
    # ç»“æœå­˜å‚¨
    agent_results: NotRequired[Dict[str, Any]]
    
    # é”™è¯¯å¤„ç†
    errors: NotRequired[List[Dict[str, Any]]]
    retry_count: NotRequired[Dict[str, int]]
    fallback_mode: NotRequired[bool]
    
    # æœ€ç»ˆè¾“å‡º
    final_result: NotRequired[str]
    execution_summary: NotRequired[Dict[str, Any]]

class AgentError(Exception):
    """ä»£ç†æ‰§è¡Œé”™è¯¯"""
    def __init__(self, agent_name: str, error_type: str, message: str):
        self.agent_name = agent_name
        self.error_type = error_type
        self.message = message
        super().__init__(f"{agent_name} {error_type}: {message}")

def resilient_researcher(state: RobustAgentState):
    """ğŸ” å…·æœ‰å®¹é”™èƒ½åŠ›çš„ç ”ç©¶ä»£ç†"""
    task = state["task_description"]
    retry_count = state.get("retry_count", {}).get("researcher", 0)
    
    print(f"ğŸ” ç ”ç©¶ä»£ç†å¼€å§‹å·¥ä½œ (å°è¯• {retry_count + 1}/3)...")
    
    try:
        # æ¨¡æ‹Ÿå¯èƒ½å¤±è´¥çš„æ“ä½œ
        if retry_count == 0:
            # ç¬¬ä¸€æ¬¡å°è¯•ï¼šæ¨¡æ‹Ÿç½‘ç»œé”™è¯¯
            raise AgentError("researcher", "NetworkError", "ç½‘ç»œè¿æ¥è¶…æ—¶")
        elif retry_count == 1:
            # ç¬¬äºŒæ¬¡å°è¯•ï¼šæ¨¡æ‹ŸAPIé™åˆ¶
            raise AgentError("researcher", "APIError", "APIè°ƒç”¨é¢‘ç‡é™åˆ¶")
        else:
            # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šæˆåŠŸ
            research_data = {"findings": f"å…³äº{task}çš„ç ”ç©¶ç»“æœ", "confidence": 0.8}
            
            # è®°å½•æˆåŠŸç»“æœ
            agent_results = state.get("agent_results", {})
            agent_results["researcher"] = research_data
            
            completed = state.get("completed_agents", [])
            completed.append("researcher")
            
            return {
                "agent_results": agent_results,
                "completed_agents": completed,
                "current_agent": "researcher"
            }
    
    except AgentError as e:
        print(f"âŒ {e}")
        
        # è®°å½•é”™è¯¯
        errors = state.get("errors", [])
        errors.append({
            "agent": e.agent_name,
            "type": e.error_type,
            "message": e.message,
            "retry_count": retry_count,
            "timestamp": "2024-01-01T10:00:00"
        })
        
        # æ›´æ–°é‡è¯•è®¡æ•°
        retry_counts = state.get("retry_count", {})
        retry_counts["researcher"] = retry_count + 1
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
        if retry_count >= 2:
            failed = state.get("failed_agents", [])
            failed.append("researcher")
            
            return {
                "errors": errors,
                "retry_count": retry_counts,
                "failed_agents": failed,
                "fallback_mode": True
            }
        else:
            return {
                "errors": errors,
                "retry_count": retry_counts
            }

def resilient_analyzer(state: RobustAgentState):
    """ğŸ“Š å…·æœ‰å®¹é”™èƒ½åŠ›çš„åˆ†æä»£ç†"""
    agent_results = state.get("agent_results", {})
    fallback_mode = state.get("fallback_mode", False)
    
    print(f"ğŸ“Š åˆ†æä»£ç†å¼€å§‹å·¥ä½œ...")
    
    if fallback_mode:
        print("âš ï¸ è¿›å…¥é™çº§æ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–åˆ†æ")
        # é™çº§å¤„ç†ï¼šä½¿ç”¨ç®€åŒ–çš„åˆ†æé€»è¾‘
        analysis = {
            "summary": "åŸºäºå¯ç”¨ä¿¡æ¯çš„ç®€åŒ–åˆ†æ",
            "confidence": 0.5,
            "mode": "fallback"
        }
    else:
        # æ­£å¸¸å¤„ç†
        research_data = agent_results.get("researcher", {})
        analysis = {
            "summary": f"æ·±åº¦åˆ†æï¼š{research_data.get('findings', 'æ— æ•°æ®')}",
            "confidence": research_data.get("confidence", 0) * 0.9,
            "mode": "normal"
        }
    
    # æ›´æ–°ç»“æœ
    agent_results = state.get("agent_results", {})
    agent_results["analyzer"] = analysis
    
    completed = state.get("completed_agents", [])
    completed.append("analyzer")
    
    return {
        "agent_results": agent_results,
        "completed_agents": completed,
        "current_agent": "analyzer"
    }

def error_recovery_router(state: RobustAgentState):
    """ğŸ”§ é”™è¯¯æ¢å¤è·¯ç”±å™¨"""
    errors = state.get("errors", [])
    failed_agents = state.get("failed_agents", [])
    current_agent = state.get("current_agent", "")
    
    print(f"ğŸ”§ é”™è¯¯æ¢å¤è·¯ç”±å™¨æ£€æŸ¥çŠ¶æ€...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„ä»£ç†
    if failed_agents:
        print(f"âš ï¸ æ£€æµ‹åˆ°å¤±è´¥ä»£ç†: {failed_agents}")
        return "fallback_mode"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
    retry_count = state.get("retry_count", {})
    for agent, count in retry_count.items():
        if count > 0 and count < 3 and agent not in state.get("completed_agents", []):
            print(f"ğŸ”„ ä»£ç† {agent} éœ€è¦é‡è¯•")
            return "retry"
    
    # æ­£å¸¸ç»§ç»­
    if current_agent == "researcher":
        return "continue_to_analyzer"
    else:
        return "finalize"

def generate_execution_summary(state: RobustAgentState):
    """ğŸ“‹ ç”Ÿæˆæ‰§è¡Œæ€»ç»“"""
    completed = state.get("completed_agents", [])
    failed = state.get("failed_agents", [])
    errors = state.get("errors", [])
    agent_results = state.get("agent_results", {})
    
    # ç”Ÿæˆæœ€ç»ˆç»“æœ
    if agent_results:
        final_result = "æ‰§è¡Œå®Œæˆ\n\n"
        for agent, result in agent_results.items():
            final_result += f"## {agent}ç»“æœ\n"
            final_result += f"{result.get('summary', 'æ— ç»“æœ')}\n"
            final_result += f"ç½®ä¿¡åº¦: {result.get('confidence', 0):.2f}\n\n"
    else:
        final_result = "æ‰§è¡Œå¤±è´¥ï¼šæ‰€æœ‰ä»£ç†éƒ½æ— æ³•å®Œæˆä»»åŠ¡"
    
    # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
    summary = {
        "total_agents": len(completed) + len(failed),
        "completed_agents": completed,
        "failed_agents": failed,
        "total_errors": len(errors),
        "fallback_used": state.get("fallback_mode", False),
        "overall_success": len(completed) > 0
    }
    
    return {
        "final_result": final_result,
        "execution_summary": summary
    }

def should_retry(state: RobustAgentState):
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
    retry_count = state.get("retry_count", {})
    failed_agents = state.get("failed_agents", [])
    
    # å¦‚æœæœ‰ä»£ç†å¤±è´¥ä½†é‡è¯•æ¬¡æ•°æœªè¾¾ä¸Šé™ï¼Œåˆ™é‡è¯•
    for agent, count in retry_count.items():
        if count > 0 and count < 3 and agent not in failed_agents:
            return True
    
    return False

def create_robust_agent_system():
    """åˆ›å»ºå…·æœ‰å®¹é”™èƒ½åŠ›çš„ä»£ç†ç³»ç»Ÿ"""
    workflow = StateGraph(RobustAgentState)
    
    # æ·»åŠ ä»£ç†èŠ‚ç‚¹
    workflow.add_node("researcher", resilient_researcher)
    workflow.add_node("analyzer", resilient_analyzer)
    workflow.add_node("summarizer", generate_execution_summary)
    
    # è®¾ç½®å…¥å£
    workflow.set_entry_point("researcher")
    
    # å¤æ‚çš„æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "researcher",
        error_recovery_router,
        {
            "retry": "researcher",           # é‡è¯•ç ”ç©¶ä»£ç†
            "fallback_mode": "analyzer",     # è¿›å…¥é™çº§æ¨¡å¼
            "continue_to_analyzer": "analyzer" # æ­£å¸¸ç»§ç»­
        }
    )
    
    workflow.add_conditional_edges(
        "analyzer",
        lambda state: "summarizer",  # åˆ†æå®Œæˆåç›´æ¥æ€»ç»“
        {
            "summarizer": "summarizer"
        }
    )
    
    workflow.add_edge("summarizer", END)
    
    return workflow.compile()

# æµ‹è¯•å®¹é”™ç³»ç»Ÿ
if __name__ == "__main__":
    app = create_robust_agent_system()
    
    result = app.invoke({
        "task_description": "åˆ†æäº‘è®¡ç®—æŠ€æœ¯çš„å‘å±•è¶‹åŠ¿"
    })
    
    print("\n" + "="*60)
    print("ğŸ›¡ï¸ å®¹é”™ä»£ç†ç³»ç»Ÿæ‰§è¡Œç»“æœ")
    print("="*60)
    
    summary = result["execution_summary"]
    print(f"æ€»ä»£ç†æ•°: {summary['total_agents']}")
    print(f"æˆåŠŸä»£ç†: {summary['completed_agents']}")
    print(f"å¤±è´¥ä»£ç†: {summary['failed_agents']}")
    print(f"é”™è¯¯æ€»æ•°: {summary['total_errors']}")
    print(f"ä½¿ç”¨é™çº§æ¨¡å¼: {summary['fallback_used']}")
    print(f"æ•´ä½“æˆåŠŸ: {summary['overall_success']}")
    
    print(f"\nğŸ“‹ æ‰§è¡Œç»“æœ:")
    print(result["final_result"])
    
    if result.get("errors"):
        print(f"\nâŒ é”™è¯¯è®°å½•:")
        for error in result["errors"]:
            print(f"- {error['agent']}: {error['type']} - {error['message']}")
```

## ğŸ“ å­¦ä¹ æ£€éªŒæ¸…å•

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€èƒ½åŠ›**ï¼š
- [ ] è§£é‡Šå¤šä»£ç†åä½œçš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯
- [ ] è®¾è®¡åˆç†çš„ä»£ç†åˆ†å·¥å’ŒèŒè´£
- [ ] å®ç°ç®€å•çš„åŒä»£ç†åä½œç³»ç»Ÿ
- [ ] é›†æˆåŸºæœ¬çš„å¤–éƒ¨å·¥å…·æ‰©å±•ä»£ç†èƒ½åŠ›

**è¿›é˜¶èƒ½åŠ›**ï¼š
- [ ] æ„å»ºå¤æ‚çš„å¤šä»£ç†æµæ°´çº¿ç³»ç»Ÿ
- [ ] å®ç°å¹¶è¡Œåä½œå’Œåé¦ˆå¾ªç¯åä½œ
- [ ] è®¾è®¡å·¥å…·ç®¡ç†å’Œè°ƒç”¨æœºåˆ¶
- [ ] å¤„ç†å¤šä»£ç†ç³»ç»Ÿçš„çŠ¶æ€ç®¡ç†å¤æ‚æ€§

**é«˜çº§èƒ½åŠ›**ï¼š
- [ ] å®ç°å¤šä»£ç†ç³»ç»Ÿçš„å®¹é”™å’Œæ¢å¤æœºåˆ¶
- [ ] ä¼˜åŒ–ä»£ç†é—´çš„åä½œæ•ˆç‡
- [ ] è®¾è®¡å¯æ‰©å±•çš„å¤šä»£ç†æ¶æ„
- [ ] è§£å†³å¤æ‚çš„ä»£ç†åè°ƒå’Œå†²çªé—®é¢˜

**å®æˆ˜èƒ½åŠ›**ï¼š
- [ ] åˆ†æå¤æ‚ä¸šåŠ¡éœ€æ±‚ï¼Œè®¾è®¡å¤šä»£ç†è§£å†³æ–¹æ¡ˆ
- [ ] æ„å»ºç”Ÿäº§çº§çš„å¤šä»£ç†åä½œç³»ç»Ÿ
- [ ] è°ƒè¯•å’Œä¼˜åŒ–å¤šä»£ç†ç³»ç»Ÿçš„æ€§èƒ½
- [ ] å‘å›¢é˜Ÿè§£é‡Šå¤šä»£ç†ç³»ç»Ÿçš„æ¶æ„è®¾è®¡

## ğŸ”® æ€è€ƒé¢˜å’Œç»ƒä¹ 

**åŸºç¡€ç»ƒä¹ **ï¼š
1. è®¾è®¡ä¸€ä¸ªç®€å•çš„å¤šä»£ç†ç¿»è¯‘ç³»ç»Ÿï¼šæ£€æµ‹è¯­è¨€â†’ç¿»è¯‘â†’è´¨æ£€â†’æ ¼å¼åŒ–
2. å®ç°ä¸€ä¸ªä»£ç å®¡æŸ¥åä½œç³»ç»Ÿï¼šä»£ç åˆ†æâ†’å®‰å…¨æ£€æŸ¥â†’æ€§èƒ½åˆ†æâ†’æŠ¥å‘Šç”Ÿæˆ
3. æ„å»ºä¸€ä¸ªæ™ºèƒ½å®¢æœç³»ç»Ÿï¼šæ„å›¾è¯†åˆ«â†’çŸ¥è¯†æ£€ç´¢â†’å›å¤ç”Ÿæˆâ†’æ»¡æ„åº¦è¯„ä¼°

**è¿›é˜¶ç»ƒä¹ **ï¼š
1. è®¾è®¡ä¸€ä¸ªå¤šä»£ç†æ•°æ®åˆ†æå¹³å°ï¼šæ•°æ®æ”¶é›†â†’æ¸…æ´—â†’åˆ†æâ†’å¯è§†åŒ–â†’æŠ¥å‘Š
2. å®ç°ä¸€ä¸ªå†…å®¹è¥é”€åä½œç³»ç»Ÿï¼šè¶‹åŠ¿åˆ†æâ†’å†…å®¹ç­–åˆ’â†’åˆ›ä½œâ†’ä¼˜åŒ–â†’å‘å¸ƒ
3. æ„å»ºä¸€ä¸ªæ™ºèƒ½æ‹›è˜ç³»ç»Ÿï¼šç®€å†ç­›é€‰â†’æŠ€èƒ½è¯„ä¼°â†’é¢è¯•å®‰æ’â†’å†³ç­–å»ºè®®

**é«˜çº§ç»ƒä¹ **ï¼š
1. è®¾è®¡ä¸€ä¸ªå®¹é”™çš„åˆ†å¸ƒå¼çˆ¬è™«ç³»ç»Ÿï¼šå¤šä»£ç†å¹¶è¡Œçˆ¬å–ï¼Œæ”¯æŒæ•…éšœæ¢å¤
2. å®ç°ä¸€ä¸ªæ™ºèƒ½æŠ•èµ„é¡¾é—®ï¼šå¸‚åœºåˆ†æâ†’é£é™©è¯„ä¼°â†’ç­–ç•¥æ¨èâ†’æ‰§è¡Œç›‘æ§
3. æ„å»ºä¸€ä¸ªè‡ªé€‚åº”å­¦ä¹ ç³»ç»Ÿï¼šå†…å®¹æ¨èâ†’å­¦ä¹ ç›‘æ§â†’æ•ˆæœè¯„ä¼°â†’ç­–ç•¥è°ƒæ•´

**æ¶æ„æ€è€ƒ**ï¼š
1. å¦‚ä½•è®¾è®¡å¤šä»£ç†ç³»ç»Ÿçš„é€šä¿¡åè®®å’Œæ¶ˆæ¯æ ¼å¼ï¼Ÿ
2. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥é€‰æ‹©é¡ºåºåä½œvså¹¶è¡Œåä½œvsåé¦ˆåä½œï¼Ÿ
3. å¦‚ä½•å¹³è¡¡ä»£ç†çš„ä¸“ä¸šåŒ–ç¨‹åº¦å’Œç³»ç»Ÿçš„çµæ´»æ€§ï¼Ÿ
4. å¤šä»£ç†ç³»ç»Ÿçš„æ€§èƒ½ç“¶é¢ˆé€šå¸¸åœ¨å“ªé‡Œï¼Œå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

---

**ğŸ¯ æ­å–œï¼** ä½ å·²ç»æŒæ¡äº†LangGraphçš„ç²¾é«“â€”â€”å¤šä»£ç†åä½œã€‚ç°åœ¨ä½ å¯ä»¥æ„å»ºç”±ä¸“ä¸šåŒ–AIä»£ç†ç»„æˆçš„å¼ºå¤§å›¢é˜Ÿï¼Œè§£å†³å¤æ‚çš„ç°å®é—®é¢˜ï¼

**ğŸ‘‰ ä¸‹ä¸€æ­¥**: [L4: é«˜çº§ç‰¹æ€§ä¸æ€§èƒ½ä¼˜åŒ–](./04-é«˜çº§ç‰¹æ€§ä¸æ€§èƒ½ä¼˜åŒ–.md) - è®©ä½ çš„å¤šä»£ç†ç³»ç»Ÿè¾¾åˆ°ç”Ÿäº§çº§æ€§èƒ½ï¼