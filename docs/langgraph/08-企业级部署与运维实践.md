# ä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´å®è·µ

*ä»å®éªŒå®¤åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œè¿™æ˜¯æ¯ä¸ªLangGraphé¡¹ç›®å¿…é¡»è·¨è¶Šçš„å…³é”®ä¸€æ­¥ã€‚æœ¬æ–‡æ¡£å°†æä¾›ç«¯åˆ°ç«¯çš„ä¼ä¸šçº§éƒ¨ç½²æŒ‡å—ï¼Œå¸®åŠ©ä½ æ„å»ºç¨³å®šã€å®‰å…¨ã€å¯æ‰©å±•çš„ç”Ÿäº§ç³»ç»Ÿã€‚*

---

## ğŸ¯ ä¼ä¸šçº§éƒ¨ç½²æ ¸å¿ƒæŒ‘æˆ˜

### ç”Ÿäº§ç¯å¢ƒçš„ç°å®çº¦æŸ

å½“æˆ‘ä»¬å°†LangGraphä»å¼€å‘ç¯å¢ƒæ¨å‘ç”Ÿäº§æ—¶ï¼Œé¢ä¸´çš„æŒ‘æˆ˜è¿œè¶…æŠ€æœ¯å®ç°ï¼š

**è§„æ¨¡æŒ‘æˆ˜**ï¼š
- æ”¯æŒä¸‡çº§å¹¶å‘ç”¨æˆ·è®¿é—®
- å¤„ç†TBçº§åˆ«çš„å¯¹è¯å†å²æ•°æ®
- ç®¡ç†æ•°ç™¾ä¸ªä¸åŒçš„AIå·¥ä½œæµ

**å¯é æ€§è¦æ±‚**ï¼š
- 99.9%+ å¯ç”¨æ€§ä¿éšœï¼ˆå¹´åœæœºæ—¶é—´ < 8.7å°æ—¶ï¼‰
- æ•…éšœè‡ªåŠ¨æ¢å¤å’Œä¼˜é›…é™çº§
- æ•°æ®ä¸€è‡´æ€§å’Œäº‹åŠ¡å®Œæ•´æ€§

**å®‰å…¨åˆè§„**ï¼š
- ä¼ä¸šçº§èº«ä»½è®¤è¯å’Œæƒé™æ§åˆ¶
- æ•æ„Ÿæ•°æ®åŠ å¯†å’Œéšç§ä¿æŠ¤
- å®¡è®¡æ—¥å¿—å’Œåˆè§„æŠ¥å‘Š

**æˆæœ¬æ§åˆ¶**ï¼š
- æ™ºèƒ½èµ„æºè°ƒåº¦å’Œå¼¹æ€§æ‰©ç¼©å®¹
- è®¡ç®—èµ„æºä¼˜åŒ–å’Œæˆæœ¬ç›‘æ§
- å¤šäº‘ç¯å¢ƒçš„æˆæœ¬ç®¡ç†

### ä¼ä¸šçº§æ¶æ„è®¾è®¡åŸåˆ™

```mermaid
graph TB
    A[API Gateway<br/>è´Ÿè½½å‡è¡¡] --> B[Service Mesh<br/>æœåŠ¡æ²»ç†]
    B --> C[LangGraphé›†ç¾¤<br/>å·¥ä½œæµå¼•æ“]
    C --> D[æ•°æ®å­˜å‚¨å±‚<br/>çŠ¶æ€æŒä¹…åŒ–]
    C --> E[æ¶ˆæ¯é˜Ÿåˆ—<br/>å¼‚æ­¥å¤„ç†]
    C --> F[å¤–éƒ¨æœåŠ¡<br/>AIæ¨¡å‹API]
    
    G[ç›‘æ§å‘Šè­¦<br/>å¯è§‚æµ‹æ€§] --> C
    H[å®‰å…¨ç½‘å…³<br/>è®¤è¯æˆæƒ] --> A
    I[CI/CDæµæ°´çº¿<br/>è‡ªåŠ¨åŒ–éƒ¨ç½²] --> C
```

**è®¾è®¡åŸåˆ™**ï¼š
1. **å¼¹æ€§è®¾è®¡**ï¼šç»„ä»¶å¤±è´¥æ—¶ç³»ç»Ÿä¾ç„¶å¯ç”¨
2. **å¯è§‚æµ‹æ€§**ï¼šå…¨é“¾è·¯ç›‘æ§å’Œæ•…éšœå¿«é€Ÿå®šä½
3. **å®‰å…¨ä¼˜å…ˆ**ï¼šé›¶ä¿¡ä»»æ¶æ„å’Œæ•°æ®ä¿æŠ¤
4. **è‡ªåŠ¨åŒ–**ï¼šå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜è¿ç»´æ•ˆç‡
5. **æˆæœ¬ä¼˜åŒ–**ï¼šæŒ‰éœ€åˆ†é…èµ„æºï¼Œé¿å…è¿‡åº¦é…ç½®

---

## ğŸ³ ç”Ÿäº§çº§å®¹å™¨åŒ–éƒ¨ç½²

### å¤šé˜¶æ®µDockeræ„å»ºä¼˜åŒ–

```dockerfile
# Dockerfile - ç”Ÿäº§çº§å¤šé˜¶æ®µæ„å»º
FROM python:3.11-slim as builder

# å®‰è£…æ„å»ºä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å®‰è£…Pythonä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ç”Ÿäº§é˜¶æ®µ
FROM python:3.11-slim as production

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r langraph && useradd -r -g langraph langraph

# å¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app
COPY --chown=langraph:langraph . .

# å®‰å…¨é…ç½®
USER langraph
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "app:app"]
```

### Kubernetesé›†ç¾¤é…ç½®

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: langgraph-prod
  labels:
    name: langgraph-prod
    tier: production

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: langgraph-api
  namespace: langgraph-prod
  labels:
    app: langgraph-api
    version: v1.0.0
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: langgraph-api
  template:
    metadata:
      labels:
        app: langgraph-api
        version: v1.0.0
    spec:
      serviceAccountName: langgraph-service-account
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: langgraph-api
        image: langgraph/api:v1.0.0
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: langgraph-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: langgraph-secrets
              key: redis-url
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: langgraph-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: logs-volume
          mountPath: /app/logs
      volumes:
      - name: config-volume
        configMap:
          name: langgraph-config
      - name: logs-volume
        emptyDir: {}

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: langgraph-api-service
  namespace: langgraph-prod
  labels:
    app: langgraph-api
spec:
  selector:
    app: langgraph-api
  ports:
  - name: http
    port: 80
    targetPort: 8000
    protocol: TCP
  type: ClusterIP

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langgraph-api-hpa
  namespace: langgraph-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

### Helm Chartsæ¨¡æ¿åŒ–éƒ¨ç½²

```yaml
# helm/langgraph/Chart.yaml
apiVersion: v2
name: langgraph
description: Enterprise LangGraph deployment
version: 1.0.0
appVersion: "1.0.0"

# helm/langgraph/values.yaml
image:
  repository: langgraph/api
  tag: v1.0.0
  pullPolicy: IfNotPresent

replicaCount: 3

service:
  type: ClusterIP
  port: 80
  targetPort: 8000

ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
  hosts:
    - host: langgraph-api.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: langgraph-tls
      hosts:
        - langgraph-api.example.com

resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

postgresql:
  enabled: true
  global:
    postgresql:
      auth:
        database: "langgraph"
        username: "langgraph"
        existingSecret: "langgraph-db-secret"
  primary:
    persistence:
      enabled: true
      size: 20Gi
    resources:
      requests:
        memory: "512Mi"
        cpu: "500m"

redis:
  enabled: true
  auth:
    enabled: true
    existingSecret: "langgraph-redis-secret"
  master:
    persistence:
      enabled: true
      size: 10Gi

monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    additionalLabels:
      release: prometheus
```

```bash
# éƒ¨ç½²å‘½ä»¤
helm install langgraph ./helm/langgraph \
  --namespace langgraph-prod \
  --create-namespace \
  --values values-prod.yaml
```

---

## ğŸš€ CI/CDè‡ªåŠ¨åŒ–æµæ°´çº¿

### GitOpså·¥ä½œæµè®¾è®¡

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production
on:
  push:
    branches: [main]
    tags: ['v*']

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest tests/ --cov=langgraph --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  build-and-push:
    needs: [test, security-scan]
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ secrets.CONTAINER_REGISTRY }}
        username: ${{ secrets.CONTAINER_USERNAME }}
        password: ${{ secrets.CONTAINER_PASSWORD }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ secrets.CONTAINER_REGISTRY }}/langgraph/api
        tags: |
          type=ref,event=branch
          type=ref,event=tag
          type=sha,prefix=commit-
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    environment: staging
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Staging
      run: |
        # ä½¿ç”¨Helmæˆ–ArgoCDéƒ¨ç½²åˆ°stagingç¯å¢ƒ
        helm upgrade --install langgraph-staging ./helm/langgraph \
          --namespace langgraph-staging \
          --set image.tag=${{ needs.build-and-push.outputs.image-tag }} \
          --values values-staging.yaml

  integration-tests:
    needs: deploy-staging
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run E2E tests
      run: |
        # è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
        pytest tests/e2e/ --base-url=https://staging.langgraph.example.com

  deploy-production:
    needs: [deploy-staging, integration-tests]
    runs-on: ubuntu-latest
    environment: production
    if: startsWith(github.ref, 'refs/tags/v')
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to Production
      run: |
        # è“ç»¿éƒ¨ç½²æˆ–é‡‘ä¸é›€éƒ¨ç½²
        ./scripts/blue-green-deploy.sh ${{ needs.build-and-push.outputs.image-tag }}
```

### é‡‘ä¸é›€å‘å¸ƒç­–ç•¥

```yaml
# k8s/canary-deployment.yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: langgraph-api-rollout
  namespace: langgraph-prod
spec:
  replicas: 10
  strategy:
    canary:
      steps:
      - setWeight: 10  # 10%æµé‡å¯¼å‘æ–°ç‰ˆæœ¬
      - pause: {duration: 300s}  # æš‚åœ5åˆ†é’Ÿè§‚å¯ŸæŒ‡æ ‡
      - setWeight: 25
      - pause: {duration: 300s}
      - setWeight: 50
      - pause: {duration: 600s}  # åœ¨50%æ—¶å¤šæš‚åœè§‚å¯Ÿ
      - setWeight: 75
      - pause: {duration: 300s}
      - setWeight: 100
      trafficRouting:
        istio:
          virtualService:
            name: langgraph-api-vs
          destinationRule:
            name: langgraph-api-dr
      analysis:
        templates:
        - templateName: success-rate
        args:
        - name: service-name
          value: langgraph-api
        - name: namespace
          value: langgraph-prod
        successCondition: result[0] >= 0.95  # æˆåŠŸç‡ >= 95%
        failureLimit: 3
        interval: 60s
  selector:
    matchLabels:
      app: langgraph-api
  template:
    metadata:
      labels:
        app: langgraph-api
    spec:
      containers:
      - name: langgraph-api
        image: langgraph/api:latest
        ports:
        - containerPort: 8000
          name: http
```

### è‡ªåŠ¨å›æ»šæœºåˆ¶

```bash
#!/bin/bash
# scripts/auto-rollback.sh

# ç›‘æ§å…³é”®æŒ‡æ ‡
ERROR_RATE_THRESHOLD=0.05  # 5%é”™è¯¯ç‡é˜ˆå€¼
RESPONSE_TIME_THRESHOLD=2000  # 2ç§’å“åº”æ—¶é—´é˜ˆå€¼

# è·å–å½“å‰æŒ‡æ ‡
current_error_rate=$(kubectl get --raw /api/v1/namespaces/langgraph-prod/pods | jq '.items[].status' | jq -r '.containerStatuses[].restartCount' | awk '{sum+=$1} END {print sum/NR}')

current_response_time=$(kubectl logs -l app=langgraph-api -n langgraph-prod --tail=1000 | grep "response_time" | awk '{sum+=$NF; count++} END {print sum/count}')

# æ£€æŸ¥æ˜¯å¦éœ€è¦å›æ»š
if (( $(echo "$current_error_rate > $ERROR_RATE_THRESHOLD" | bc -l) )) || (( $(echo "$current_response_time > $RESPONSE_TIME_THRESHOLD" | bc -l) )); then
    echo "æŒ‡æ ‡å¼‚å¸¸ï¼Œæ‰§è¡Œè‡ªåŠ¨å›æ»š..."
    
    # è·å–ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬
    previous_version=$(kubectl rollout history deployment/langgraph-api -n langgraph-prod | tail -n 2 | head -n 1 | awk '{print $1}')
    
    # æ‰§è¡Œå›æ»š
    kubectl rollout undo deployment/langgraph-api -n langgraph-prod --to-revision=$previous_version
    
    # å‘é€å‘Šè­¦é€šçŸ¥
    curl -X POST $SLACK_WEBHOOK_URL \
      -H 'Content-type: application/json' \
      --data "{\"text\":\"ğŸš¨ LangGraphç”Ÿäº§ç¯å¢ƒè‡ªåŠ¨å›æ»šåˆ°ç‰ˆæœ¬ $previous_version\"}"
else
    echo "æ‰€æœ‰æŒ‡æ ‡æ­£å¸¸"
fi
```

---

## ğŸ“Š ç›‘æ§å‘Šè­¦ä¸å¯è§‚æµ‹æ€§

### Prometheusç›‘æ§é…ç½®

```yaml
# monitoring/prometheus-config.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "langgraph-rules.yml"

scrape_configs:
  - job_name: 'langgraph-api'
    kubernetes_sd_configs:
    - role: endpoints
      namespaces:
        names:
        - langgraph-prod
    relabel_configs:
    - source_labels: [__meta_kubernetes_service_name]
      action: keep
      regex: langgraph-api-service
    - source_labels: [__meta_kubernetes_endpoint_port_name]
      action: keep
      regex: http

  - job_name: 'langgraph-postgresql'
    static_configs:
    - targets: ['postgres-exporter.langgraph-prod.svc.cluster.local:9187']

  - job_name: 'langgraph-redis'
    static_configs:
    - targets: ['redis-exporter.langgraph-prod.svc.cluster.local:9121']

alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - alertmanager.monitoring.svc.cluster.local:9093
```

### å‘Šè­¦è§„åˆ™å®šä¹‰

```yaml
# monitoring/langgraph-rules.yml
groups:
- name: langgraph-alerts
  rules:
  # APIå¯ç”¨æ€§å‘Šè­¦
  - alert: LangGraphAPIDown
    expr: up{job="langgraph-api"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "LangGraph API is down"
      description: "LangGraph API has been down for more than 1 minute"

  # é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }} for 2 minutes"

  # å“åº”æ—¶é—´å‘Šè­¦
  - alert: HighResponseTime
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "High response time"
      description: "95th percentile response time is {{ $value }}s"

  # å†…å­˜ä½¿ç”¨å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (container_memory_usage_bytes{pod=~"langgraph-api-.*"} / container_spec_memory_limit_bytes) > 0.9
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage"
      description: "Memory usage is {{ $value | humanizePercentage }} for pod {{ $labels.pod }}"

  # æ•°æ®åº“è¿æ¥å‘Šè­¦
  - alert: DatabaseConnectionIssue
    expr: postgresql_up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Database connection issue"
      description: "PostgreSQL database is unreachable"

  # Redisè¿æ¥å‘Šè­¦
  - alert: RedisConnectionIssue
    expr: redis_up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Redis connection issue"
      description: "Redis server is unreachable"
```

### Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "LangGraph Enterprise Dashboard",
    "tags": ["langgraph", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "APIè¯·æ±‚æ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{job=\"langgraph-api\"}[5m]))",
            "legendFormat": "RPS"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 100},
                {"color": "red", "value": 500}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (status) (langgraph_workflow_executions_total)",
            "legendFormat": "{{status}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "ç³»ç»Ÿèµ„æºä½¿ç”¨",
        "type": "timeseries",
        "targets": [
          {
            "expr": "avg(rate(container_cpu_usage_seconds_total{pod=~\"langgraph-api-.*\"}[5m])) * 100",
            "legendFormat": "CPUä½¿ç”¨ç‡(%)"
          },
          {
            "expr": "avg(container_memory_usage_bytes{pod=~\"langgraph-api-.*\"}) / 1024 / 1024",
            "legendFormat": "å†…å­˜ä½¿ç”¨(MB)"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

### è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†

```python
# monitoring/metrics_collector.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import functools

# å®šä¹‰ä¸šåŠ¡æŒ‡æ ‡
workflow_executions = Counter(
    'langgraph_workflow_executions_total',
    'Total number of workflow executions',
    ['workflow_type', 'status']
)

workflow_duration = Histogram(
    'langgraph_workflow_duration_seconds',
    'Duration of workflow execution',
    ['workflow_type'],
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30, 60, float('inf')]
)

active_sessions = Gauge(
    'langgraph_active_sessions',
    'Number of active user sessions'
)

llm_api_calls = Counter(
    'langgraph_llm_api_calls_total',
    'Total number of LLM API calls',
    ['provider', 'model', 'status']
)

def monitor_workflow(workflow_type: str):
    """å·¥ä½œæµç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                workflow_executions.labels(
                    workflow_type=workflow_type,
                    status='success'
                ).inc()
                return result
            except Exception as e:
                workflow_executions.labels(
                    workflow_type=workflow_type,
                    status='error'
                ).inc()
                raise
            finally:
                duration = time.time() - start_time
                workflow_duration.labels(
                    workflow_type=workflow_type
                ).observe(duration)
        return wrapper
    return decorator

# åœ¨LangGraphå·¥ä½œæµä¸­ä½¿ç”¨
@monitor_workflow('customer_service')
async def customer_service_workflow(state: dict):
    """å®¢æœå·¥ä½œæµ"""
    # å·¥ä½œæµé€»è¾‘
    pass

# å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨
if __name__ == '__main__':
    start_http_server(8001)  # åœ¨8001ç«¯å£æš´éœ²æŒ‡æ ‡
```

---

## ğŸ”’ å®‰å…¨åˆè§„ä¸æ•°æ®ä¿æŠ¤

### é›¶ä¿¡ä»»ç½‘ç»œæ¶æ„

```yaml
# security/network-policies.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: langgraph-network-policy
  namespace: langgraph-prod
spec:
  podSelector:
    matchLabels:
      app: langgraph-api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: istio-system
    - podSelector:
        matchLabels:
          app: nginx-ingress
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgresql
    ports:
    - protocol: TCP
      port: 5432
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379
  - to: []  # å…è®¸è®¿é—®å¤–éƒ¨LLM API
    ports:
    - protocol: TCP
      port: 443

---
# security/pod-security-policy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: langgraph-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

### RBACæƒé™æ§åˆ¶

```yaml
# security/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: langgraph-service-account
  namespace: langgraph-prod

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: langgraph-prod
  name: langgraph-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: langgraph-rolebinding
  namespace: langgraph-prod
subjects:
- kind: ServiceAccount
  name: langgraph-service-account
  namespace: langgraph-prod
roleRef:
  kind: Role
  name: langgraph-role
  apiGroup: rbac.authorization.k8s.io
```

### æ•°æ®åŠ å¯†ä¸éšç§ä¿æŠ¤

```python
# security/data_protection.py
import hashlib
import hmac
import base64
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import os
import logging

class DataProtectionManager:
    """ä¼ä¸šçº§æ•°æ®ä¿æŠ¤ç®¡ç†å™¨"""
    
    def __init__(self, master_key: str):
        self.master_key = master_key.encode()
        self.fernet = self._create_fernet()
        self.logger = logging.getLogger(__name__)
    
    def _create_fernet(self) -> Fernet:
        """åˆ›å»ºåŠ å¯†å®ä¾‹"""
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=b'langgraph_salt',  # ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨éšæœºsalt
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(self.master_key))
        return Fernet(key)
    
    def encrypt_sensitive_data(self, data: str) -> str:
        """åŠ å¯†æ•æ„Ÿæ•°æ®"""
        try:
            encrypted_data = self.fernet.encrypt(data.encode())
            return base64.urlsafe_b64encode(encrypted_data).decode()
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ å¯†å¤±è´¥: {e}")
            raise
    
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        """è§£å¯†æ•æ„Ÿæ•°æ®"""
        try:
            decoded_data = base64.urlsafe_b64decode(encrypted_data.encode())
            decrypted_data = self.fernet.decrypt(decoded_data)
            return decrypted_data.decode()
        except Exception as e:
            self.logger.error(f"æ•°æ®è§£å¯†å¤±è´¥: {e}")
            raise
    
    def hash_pii(self, pii: str, salt: str = None) -> str:
        """å¯¹PIIè¿›è¡Œä¸å¯é€†å“ˆå¸Œ"""
        if salt is None:
            salt = os.urandom(32)
        else:
            salt = salt.encode()
        
        pwdhash = hashlib.pbkdf2_hmac(
            'sha256',
            pii.encode('utf-8'),
            salt,
            100000
        )
        return base64.b64encode(salt + pwdhash).decode('ascii')
    
    def anonymize_text(self, text: str) -> str:
        """æ–‡æœ¬åŒ¿ååŒ–å¤„ç†"""
        import re
        
        # æ›¿æ¢å¸¸è§çš„PIIæ¨¡å¼
        patterns = [
            (r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]'),
            (r'\b\d{3}-\d{2}-\d{4}\b', '[SSN]'),
            (r'\b\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}\b', '[CARD]'),
            (r'\b\d{3}[- ]?\d{3}[- ]?\d{4}\b', '[PHONE]'),
        ]
        
        anonymized_text = text
        for pattern, replacement in patterns:
            anonymized_text = re.sub(pattern, replacement, anonymized_text)
        
        return anonymized_text

# åœ¨LangGraphçŠ¶æ€ä¸­é›†æˆæ•°æ®ä¿æŠ¤
class SecureStateManager:
    """å®‰å…¨çš„çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self, protection_manager: DataProtectionManager):
        self.protection_manager = protection_manager
    
    def store_state(self, state: dict) -> dict:
        """å­˜å‚¨çŠ¶æ€æ—¶è‡ªåŠ¨åŠ å¯†æ•æ„Ÿæ•°æ®"""
        secure_state = {}
        
        for key, value in state.items():
            if self._is_sensitive_field(key):
                # æ•æ„Ÿå­—æ®µåŠ å¯†å­˜å‚¨
                secure_state[key] = self.protection_manager.encrypt_sensitive_data(str(value))
                secure_state[f"{key}_encrypted"] = True
            else:
                # æ™®é€šå­—æ®µç›´æ¥å­˜å‚¨
                secure_state[key] = value
        
        return secure_state
    
    def load_state(self, state: dict) -> dict:
        """åŠ è½½çŠ¶æ€æ—¶è‡ªåŠ¨è§£å¯†æ•æ„Ÿæ•°æ®"""
        decrypted_state = {}
        
        for key, value in state.items():
            if key.endswith('_encrypted'):
                continue
            
            if state.get(f"{key}_encrypted", False):
                # è§£å¯†æ•æ„Ÿå­—æ®µ
                decrypted_state[key] = self.protection_manager.decrypt_sensitive_data(value)
            else:
                # æ™®é€šå­—æ®µç›´æ¥è¿”å›
                decrypted_state[key] = value
        
        return decrypted_state
    
    def _is_sensitive_field(self, field_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•æ„Ÿå­—æ®µ"""
        sensitive_fields = [
            'email', 'phone', 'ssn', 'credit_card',
            'password', 'token', 'api_key', 'personal_info'
        ]
        return any(sensitive in field_name.lower() for sensitive in sensitive_fields)
```

### å®¡è®¡æ—¥å¿—ç³»ç»Ÿ

```python
# security/audit_logger.py
import json
import uuid
from datetime import datetime
from typing import Dict, Any, Optional
import logging
from contextlib import contextmanager

class AuditLogger:
    """ä¼ä¸šçº§å®¡è®¡æ—¥å¿—ç³»ç»Ÿ"""
    
    def __init__(self, service_name: str = "langgraph"):
        self.service_name = service_name
        self.logger = logging.getLogger(f"audit.{service_name}")
        
        # é…ç½®å®¡è®¡æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # å®¡è®¡æ—¥å¿—å•ç‹¬è¾“å‡º
        audit_handler = logging.FileHandler('/var/log/langgraph/audit.log')
        audit_handler.setFormatter(formatter)
        self.logger.addHandler(audit_handler)
        self.logger.setLevel(logging.INFO)
    
    def log_workflow_execution(
        self, 
        workflow_id: str,
        user_id: str,
        workflow_type: str,
        input_data: Dict[str, Any],
        output_data: Dict[str, Any],
        execution_time: float,
        status: str
    ):
        """è®°å½•å·¥ä½œæµæ‰§è¡Œæ—¥å¿—"""
        audit_event = {
            "event_id": str(uuid.uuid4()),
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": "workflow_execution",
            "service": self.service_name,
            "workflow_id": workflow_id,
            "user_id": user_id,
            "workflow_type": workflow_type,
            "execution_time": execution_time,
            "status": status,
            "input_summary": self._summarize_data(input_data),
            "output_summary": self._summarize_data(output_data),
            "ip_address": self._get_client_ip(),
            "user_agent": self._get_user_agent()
        }
        
        self.logger.info(json.dumps(audit_event))
    
    def log_data_access(
        self,
        user_id: str,
        resource_type: str,
        resource_id: str,
        action: str,
        success: bool,
        reason: Optional[str] = None
    ):
        """è®°å½•æ•°æ®è®¿é—®æ—¥å¿—"""
        audit_event = {
            "event_id": str(uuid.uuid4()),
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": "data_access",
            "service": self.service_name,
            "user_id": user_id,
            "resource_type": resource_type,
            "resource_id": resource_id,
            "action": action,
            "success": success,
            "reason": reason,
            "ip_address": self._get_client_ip()
        }
        
        self.logger.info(json.dumps(audit_event))
    
    def log_security_event(
        self,
        event_type: str,
        severity: str,
        user_id: Optional[str],
        description: str,
        additional_data: Dict[str, Any] = None
    ):
        """è®°å½•å®‰å…¨äº‹ä»¶æ—¥å¿—"""
        audit_event = {
            "event_id": str(uuid.uuid4()),
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": "security_event",
            "service": self.service_name,
            "security_event_type": event_type,
            "severity": severity,
            "user_id": user_id,
            "description": description,
            "additional_data": additional_data or {},
            "ip_address": self._get_client_ip()
        }
        
        self.logger.warning(json.dumps(audit_event))
    
    @contextmanager
    def audit_context(self, user_id: str, operation: str):
        """å®¡è®¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = datetime.utcnow()
        operation_id = str(uuid.uuid4())
        
        try:
            self.log_operation_start(operation_id, user_id, operation)
            yield operation_id
            self.log_operation_success(operation_id, user_id, operation, start_time)
        except Exception as e:
            self.log_operation_failure(operation_id, user_id, operation, start_time, str(e))
            raise
    
    def _summarize_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•°æ®æ‘˜è¦ï¼Œé¿å…è®°å½•æ•æ„Ÿä¿¡æ¯"""
        return {
            "field_count": len(data),
            "fields": list(data.keys()),
            "size_estimate": len(str(data))
        }
    
    def _get_client_ip(self) -> str:
        """è·å–å®¢æˆ·ç«¯IPï¼ˆåœ¨å®é™…å®ç°ä¸­ä»è¯·æ±‚ä¸Šä¸‹æ–‡è·å–ï¼‰"""
        # å®é™…å®ç°ä¸­åº”è¯¥ä»Flask/FastAPIçš„requestå¯¹è±¡è·å–
        return "127.0.0.1"
    
    def _get_user_agent(self) -> str:
        """è·å–ç”¨æˆ·ä»£ç†ï¼ˆåœ¨å®é™…å®ç°ä¸­ä»è¯·æ±‚ä¸Šä¸‹æ–‡è·å–ï¼‰"""
        return "LangGraph-Client/1.0"

# åœ¨LangGraphå·¥ä½œæµä¸­ä½¿ç”¨å®¡è®¡æ—¥å¿—
audit_logger = AuditLogger()

@audit_logger.audit_context
async def secure_customer_service_workflow(state: dict, user_id: str):
    """å¸¦å®¡è®¡çš„å®¢æœå·¥ä½œæµ"""
    workflow_id = state.get('workflow_id')
    
    try:
        # è®°å½•å·¥ä½œæµå¼€å§‹
        audit_logger.log_workflow_execution(
            workflow_id=workflow_id,
            user_id=user_id,
            workflow_type="customer_service",
            input_data=state,
            output_data={},
            execution_time=0,
            status="started"
        )
        
        # æ‰§è¡Œå·¥ä½œæµé€»è¾‘
        result = await execute_workflow_logic(state)
        
        # è®°å½•å·¥ä½œæµå®Œæˆ
        audit_logger.log_workflow_execution(
            workflow_id=workflow_id,
            user_id=user_id,
            workflow_type="customer_service",
            input_data=state,
            output_data=result,
            execution_time=time.time() - start_time,
            status="completed"
        )
        
        return result
        
    except Exception as e:
        # è®°å½•å®‰å…¨äº‹ä»¶
        audit_logger.log_security_event(
            event_type="workflow_execution_failure",
            severity="medium",
            user_id=user_id,
            description=f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}"
        )
        raise
```

---

## ğŸ¯ å¸¸è§æ•…éšœåœºæ™¯ä¸è§£å†³æ–¹æ¡ˆ

### åœºæ™¯1ï¼šçªå‘æµé‡æ¿€å¢

**é—®é¢˜æè¿°**: è¥é”€æ´»åŠ¨å¯¼è‡´ç”¨æˆ·é‡çªç„¶å¢é•¿10å€ï¼Œç³»ç»Ÿå“åº”å˜æ…¢

**ç›‘æ§æŒ‡æ ‡**:
```bash
# æ£€æŸ¥å½“å‰è´Ÿè½½
kubectl top pods -n langgraph-prod
kubectl get hpa -n langgraph-prod

# æŸ¥çœ‹é”™è¯¯ç‡
kubectl logs -l app=langgraph-api -n langgraph-prod | grep ERROR | wc -l
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. ç«‹å³æ‰‹åŠ¨æ‰©å®¹
kubectl scale deployment langgraph-api --replicas=15 -n langgraph-prod

# 2. è°ƒæ•´HPAç­–ç•¥ï¼ˆä¸´æ—¶é™ä½é˜ˆå€¼ï¼‰
kubectl patch hpa langgraph-api-hpa -n langgraph-prod -p '{"spec":{"metrics":[{"type":"Resource","resource":{"name":"cpu","target":{"type":"Utilization","averageUtilization":50}}}]}}'

# 3. å¯ç”¨é™æµæªæ–½
kubectl apply -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: rate-limit
  namespace: langgraph-prod
spec:
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_INBOUND
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.filters.http.local_ratelimit
        typed_config:
          "@type": type.googleapis.com/udpa.type.v1.TypedStruct
          type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
          value:
            stat_prefix: local_rate_limiter
            token_bucket:
              max_tokens: 1000
              tokens_per_fill: 1000
              fill_interval: 60s
EOF
```

### åœºæ™¯2ï¼šæ•°æ®åº“è¿æ¥æ± è€—å°½

**é—®é¢˜æè¿°**: PostgreSQLè¿æ¥æ•°è¾¾åˆ°ä¸Šé™ï¼Œæ–°è¯·æ±‚æ— æ³•å»ºç«‹è¿æ¥

**è¯Šæ–­å‘½ä»¤**:
```sql
-- æŸ¥çœ‹å½“å‰è¿æ¥æ•°
SELECT count(*) FROM pg_stat_activity;

-- æŸ¥çœ‹è¿æ¥è¯¦æƒ…
SELECT 
    state,
    count(*),
    avg(extract(epoch from now() - state_change)) as avg_duration
FROM pg_stat_activity 
WHERE state IS NOT NULL 
GROUP BY state;

-- æŸ¥æ‰¾é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
SELECT 
    pid,
    now() - pg_stat_activity.query_start AS duration,
    query 
FROM pg_stat_activity 
WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes';
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¼˜åŒ–è¿æ¥æ± é…ç½®
DATABASE_CONFIG = {
    "pool_size": 20,              # åŸºç¡€è¿æ¥æ± å¤§å°
    "max_overflow": 30,           # é¢å¤–è¿æ¥æ•°
    "pool_timeout": 30,           # è·å–è¿æ¥è¶…æ—¶æ—¶é—´
    "pool_recycle": 3600,         # è¿æ¥å›æ”¶æ—¶é—´
    "pool_pre_ping": True,        # è¿æ¥å¥åº·æ£€æŸ¥
    "echo": False                 # å…³é—­SQLæ—¥å¿—ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
}

# å®ç°è¿æ¥æ± ç›‘æ§
from sqlalchemy import create_engine, event
from sqlalchemy.pool import QueuePool

class DatabaseMonitor:
    def __init__(self):
        self.connection_count = 0
        self.max_connections = 0
    
    def on_connect(self, dbapi_connection, connection_record):
        self.connection_count += 1
        self.max_connections = max(self.max_connections, self.connection_count)
    
    def on_disconnect(self, dbapi_connection, connection_record):
        self.connection_count -= 1

# åœ¨æ•°æ®åº“å¼•æ“ä¸Šæ³¨å†Œç›‘æ§å™¨
monitor = DatabaseMonitor()
engine = create_engine(DATABASE_URL, **DATABASE_CONFIG)
event.listen(engine, 'connect', monitor.on_connect)
event.listen(engine, 'disconnect', monitor.on_disconnect)
```

### åœºæ™¯3ï¼šLLM APIè°ƒç”¨å¤±è´¥

**é—®é¢˜æè¿°**: OpenAI APIæœåŠ¡ä¸ç¨³å®šï¼Œå¯¼è‡´å·¥ä½œæµé¢‘ç¹å¤±è´¥

**å®¹é”™æœºåˆ¶**:
```python
import asyncio
import aiohttp
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

class ResilientLLMClient:
    """å¼¹æ€§LLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.providers = [
            {"name": "openai", "api_key": "...", "base_url": "https://api.openai.com/v1"},
            {"name": "azure", "api_key": "...", "base_url": "https://xxx.openai.azure.com"},
            {"name": "anthropic", "api_key": "...", "base_url": "https://api.anthropic.com"}
        ]
        self.current_provider = 0
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def complete(self, messages: list, **kwargs) -> str:
        """å¸¦é‡è¯•å’Œæ•…éšœè½¬ç§»çš„APIè°ƒç”¨"""
        
        for attempt in range(len(self.providers)):
            provider = self.providers[self.current_provider]
            
            try:
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                    response = await self._call_provider(session, provider, messages, **kwargs)
                    return response
                    
            except Exception as e:
                logging.warning(f"Provider {provider['name']} failed: {e}")
                self.current_provider = (self.current_provider + 1) % len(self.providers)
                
                if attempt == len(self.providers) - 1:
                    # æ‰€æœ‰æä¾›å•†éƒ½å¤±è´¥ï¼Œä½¿ç”¨ç¼“å­˜å“åº”æˆ–é»˜è®¤å›å¤
                    return await self._fallback_response(messages)
                    
                continue
    
    async def _call_provider(self, session: aiohttp.ClientSession, provider: dict, messages: list, **kwargs) -> str:
        """è°ƒç”¨ç‰¹å®šæä¾›å•†çš„API"""
        # æ ¹æ®æä¾›å•†è°ƒç”¨ä¸åŒçš„API
        if provider["name"] == "openai":
            return await self._call_openai(session, provider, messages, **kwargs)
        elif provider["name"] == "azure":
            return await self._call_azure(session, provider, messages, **kwargs)
        # ... å…¶ä»–æä¾›å•†
    
    async def _fallback_response(self, messages: list) -> str:
        """æ•…éšœå›é€€å“åº”"""
        # å¯ä»¥è¿”å›ç¼“å­˜çš„å“åº”æˆ–è€…é»˜è®¤å›å¤
        return "æŠ±æ­‰ï¼Œå½“å‰AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"

# åœ¨LangGraphä¸­ä½¿ç”¨å¼¹æ€§å®¢æˆ·ç«¯
llm_client = ResilientLLMClient()

async def llm_node(state: dict) -> dict:
    """å¸¦å®¹é”™çš„LLMèŠ‚ç‚¹"""
    messages = state.get("messages", [])
    
    try:
        response = await llm_client.complete(messages)
        return {"messages": messages + [{"role": "assistant", "content": response}]}
    except Exception as e:
        # è®°å½•é”™è¯¯å¹¶è¿”å›é”™è¯¯çŠ¶æ€
        logging.error(f"LLMè°ƒç”¨å®Œå…¨å¤±è´¥: {e}")
        return {
            "messages": messages + [{"role": "system", "content": "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"}],
            "error": str(e)
        }
```

### åœºæ™¯4ï¼šå†…å­˜æ³„æ¼å’Œæ€§èƒ½é™çº§

**é—®é¢˜è¯Šæ–­**:
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨è¶‹åŠ¿
kubectl top pods -n langgraph-prod --containers

# è·å–è¯¦ç»†çš„å†…å­˜åˆ†æ
kubectl exec -it deployment/langgraph-api -n langgraph-prod -- python -c "
import psutil
import gc
process = psutil.Process()
print(f'å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB')
print(f'å¯¹è±¡æ•°é‡: {len(gc.get_objects())}')
"

# åˆ†ææ…¢æŸ¥è¯¢
kubectl exec -it postgresql-pod -n langgraph-prod -- psql -c "
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;
"
```

**æ€§èƒ½ä¼˜åŒ–**:
```python
# å†…å­˜ç®¡ç†å’Œå¯¹è±¡æ± 
import weakref
from typing import Dict, Any
import gc

class StateObjectPool:
    """çŠ¶æ€å¯¹è±¡æ± ï¼Œå‡å°‘å†…å­˜åˆ†é…"""
    
    def __init__(self, max_size: int = 1000):
        self.pool = []
        self.max_size = max_size
        self.active_objects = weakref.WeakSet()
    
    def get_state_object(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€å¯¹è±¡"""
        if self.pool:
            obj = self.pool.pop()
            obj.clear()  # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
        else:
            obj = {}
        
        self.active_objects.add(obj)
        return obj
    
    def return_state_object(self, obj: Dict[str, Any]):
        """å½’è¿˜çŠ¶æ€å¯¹è±¡"""
        if len(self.pool) < self.max_size:
            obj.clear()
            self.pool.append(obj)
    
    def cleanup(self):
        """æ¸…ç†ä¸å†ä½¿ç”¨çš„å¯¹è±¡"""
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†å¯¹è±¡æ± 
        if len(self.pool) > self.max_size // 2:
            self.pool = self.pool[:self.max_size // 2]

# å…¨å±€å¯¹è±¡æ± 
state_pool = StateObjectPool()

# å®šæœŸæ¸…ç†ä»»åŠ¡
import asyncio

async def memory_cleanup_task():
    """å®šæœŸå†…å­˜æ¸…ç†ä»»åŠ¡"""
    while True:
        await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
        
        # æ‰§è¡Œåƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†å¯¹è±¡æ± 
        state_pool.cleanup()
        
        # è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        logging.info(f"å†…å­˜ä½¿ç”¨: {memory_mb:.2f} MB")

# åœ¨åº”ç”¨å¯åŠ¨æ—¶å¯åŠ¨æ¸…ç†ä»»åŠ¡
asyncio.create_task(memory_cleanup_task())
```

---

## ğŸ“ˆ å®¹é‡è§„åˆ’ä¸æˆæœ¬ä¼˜åŒ–

### å®¹é‡è§„åˆ’æ¨¡å‹

```python
# capacity/capacity_planner.py
import math
from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np

@dataclass
class WorkloadProfile:
    """å·¥ä½œè´Ÿè½½é…ç½®æ–‡ä»¶"""
    daily_active_users: int
    peak_concurrent_ratio: float  # å³°å€¼å¹¶å‘ç”¨æˆ·æ¯”ä¾‹
    avg_requests_per_user: int
    avg_workflow_duration: float  # ç§’
    data_size_per_request: float  # MB

@dataclass
class ResourceRequirements:
    """èµ„æºéœ€æ±‚"""
    cpu_cores: float
    memory_gb: float
    storage_gb: float
    network_mbps: float

class CapacityPlanner:
    """å®¹é‡è§„åˆ’å™¨"""
    
    def __init__(self):
        # åŸºç¡€èµ„æºæ¶ˆè€—ç³»æ•°ï¼ˆåŸºäºæ€§èƒ½æµ‹è¯•æ•°æ®ï¼‰
        self.cpu_per_request = 0.1  # CPUæ ¸å¿ƒç§’
        self.memory_per_session = 50  # MB
        self.storage_per_user_day = 10  # MB
        self.network_per_request = 2  # MB
    
    def calculate_requirements(self, workload: WorkloadProfile) -> ResourceRequirements:
        """è®¡ç®—èµ„æºéœ€æ±‚"""
        
        # è®¡ç®—å³°å€¼å¹¶å‘æ•°
        peak_concurrent = workload.daily_active_users * workload.peak_concurrent_ratio
        
        # è®¡ç®—CPUéœ€æ±‚
        peak_rps = (peak_concurrent * workload.avg_requests_per_user) / 3600  # æ¯ç§’è¯·æ±‚æ•°
        cpu_cores = peak_rps * self.cpu_per_request * 1.5  # 50%ç¼“å†²
        
        # è®¡ç®—å†…å­˜éœ€æ±‚
        memory_gb = (peak_concurrent * self.memory_per_session + 
                    workload.data_size_per_request * peak_rps * workload.avg_workflow_duration) / 1024
        memory_gb *= 1.3  # 30%ç¼“å†²
        
        # è®¡ç®—å­˜å‚¨éœ€æ±‚
        daily_storage = workload.daily_active_users * self.storage_per_user_day
        storage_gb = daily_storage * 30 / 1024  # ä¿ç•™30å¤©æ•°æ®
        
        # è®¡ç®—ç½‘ç»œéœ€æ±‚
        network_mbps = peak_rps * self.network_per_request * 8 / 1024  # è½¬æ¢ä¸ºMbps
        
        return ResourceRequirements(
            cpu_cores=math.ceil(cpu_cores),
            memory_gb=math.ceil(memory_gb),
            storage_gb=math.ceil(storage_gb),
            network_mbps=math.ceil(network_mbps)
        )
    
    def estimate_costs(self, requirements: ResourceRequirements, provider: str = "aws") -> Dict[str, float]:
        """ä¼°ç®—äº‘æœåŠ¡æˆæœ¬ï¼ˆæœˆï¼‰"""
        
        cost_models = {
            "aws": {
                "cpu_per_core_hour": 0.05,  # EKSèŠ‚ç‚¹æˆæœ¬
                "memory_per_gb_hour": 0.01,
                "storage_per_gb_month": 0.1,  # EBS gp3
                "network_per_gb": 0.09
            },
            "azure": {
                "cpu_per_core_hour": 0.048,
                "memory_per_gb_hour": 0.0095,
                "storage_per_gb_month": 0.096,
                "network_per_gb": 0.087
            },
            "gcp": {
                "cpu_per_core_hour": 0.047,
                "memory_per_gb_hour": 0.0094,
                "storage_per_gb_month": 0.1,
                "network_per_gb": 0.085
            }
        }
        
        rates = cost_models[provider]
        hours_per_month = 24 * 30
        
        costs = {
            "compute": (requirements.cpu_cores * rates["cpu_per_core_hour"] + 
                       requirements.memory_gb * rates["memory_per_gb_hour"]) * hours_per_month,
            "storage": requirements.storage_gb * rates["storage_per_gb_month"],
            "network": requirements.network_mbps * hours_per_month * rates["network_per_gb"] / 8,  # è½¬æ¢ä¸ºGB
            "additional_services": 500  # RDS, Redis, ç›‘æ§ç­‰
        }
        
        costs["total"] = sum(costs.values())
        return costs

# ä½¿ç”¨ç¤ºä¾‹
planner = CapacityPlanner()

# å®šä¹‰å·¥ä½œè´Ÿè½½
workload = WorkloadProfile(
    daily_active_users=10000,
    peak_concurrent_ratio=0.15,
    avg_requests_per_user=50,
    avg_workflow_duration=3.0,
    data_size_per_request=0.5
)

# è®¡ç®—èµ„æºéœ€æ±‚
requirements = planner.calculate_requirements(workload)
print(f"éœ€è¦CPUæ ¸å¿ƒ: {requirements.cpu_cores}")
print(f"éœ€è¦å†…å­˜: {requirements.memory_gb} GB")
print(f"éœ€è¦å­˜å‚¨: {requirements.storage_gb} GB")

# ä¼°ç®—æˆæœ¬
aws_costs = planner.estimate_costs(requirements, "aws")
print(f"AWSæœˆæˆæœ¬: ${aws_costs['total']:.2f}")
```

### æ™ºèƒ½æ‰©ç¼©å®¹ç­–ç•¥

```yaml
# autoscaling/custom-metrics-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: langgraph-custom-hpa
  namespace: langgraph-prod
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-api
  minReplicas: 3
  maxReplicas: 50
  metrics:
  # CPUå’Œå†…å­˜åŸºç¡€æŒ‡æ ‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  # è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡
  - type: Object
    object:
      metric:
        name: langgraph_active_workflows
      target:
        type: AverageValue
        averageValue: "10"  # æ¯ä¸ªPodæœ€å¤šå¤„ç†10ä¸ªå¹¶å‘å·¥ä½œæµ
      describedObject:
        apiVersion: v1
        kind: Service
        name: langgraph-api-service
  
  # å¤–éƒ¨æŒ‡æ ‡ï¼ˆé˜Ÿåˆ—é•¿åº¦ï¼‰
  - type: External
    external:
      metric:
        name: sqs_messages_visible
        selector:
          matchLabels:
            queue: "langgraph-tasks"
      target:
        type: AverageValue
        averageValue: "5"  # é˜Ÿåˆ—ä¸­æ¯5ä¸ªæ¶ˆæ¯å¢åŠ ä¸€ä¸ªPod

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50   # æ¯æ¬¡æœ€å¤šå¢åŠ 50%çš„Pod
        periodSeconds: 60
      - type: Pods
        value: 2    # æˆ–è€…æ¯æ¬¡æœ€å¤šå¢åŠ 2ä¸ªPod
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300  # ç¼©å®¹å‰ç­‰å¾…5åˆ†é’Ÿè§‚å¯Ÿ
      policies:
      - type: Percent
        value: 10   # æ¯æ¬¡æœ€å¤šå‡å°‘10%çš„Pod
        periodSeconds: 60

---
# autoscaling/vertical-pod-autoscaler.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: langgraph-vpa
  namespace: langgraph-prod
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: langgraph-api
  updatePolicy:
    updateMode: "Auto"  # è‡ªåŠ¨è°ƒæ•´èµ„æºé™åˆ¶
  resourcePolicy:
    containerPolicies:
    - containerName: langgraph-api
      maxAllowed:
        cpu: 2
        memory: 4Gi
      minAllowed:
        cpu: 100m
        memory: 128Mi
      controlledResources: ["cpu", "memory"]
```

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥

```python
# cost_optimization/spot_instance_manager.py
import boto3
import logging
from typing import List, Dict
from datetime import datetime, timedelta

class SpotInstanceManager:
    """Spotå®ä¾‹ç®¡ç†å™¨"""
    
    def __init__(self, region: str = "us-west-2"):
        self.ec2 = boto3.client('ec2', region_name=region)
        self.asg = boto3.client('autoscaling', region_name=region)
        self.logger = logging.getLogger(__name__)
    
    def get_spot_price_history(self, instance_types: List[str], days: int = 7) -> Dict[str, float]:
        """è·å–Spotå®ä¾‹ä»·æ ¼å†å²"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days)
        
        prices = {}
        for instance_type in instance_types:
            response = self.ec2.describe_spot_price_history(
                InstanceTypes=[instance_type],
                ProductDescriptions=['Linux/UNIX'],
                StartTime=start_time,
                EndTime=end_time,
                MaxResults=100
            )
            
            if response['SpotPriceHistory']:
                avg_price = sum(float(price['SpotPrice']) for price in response['SpotPriceHistory']) / len(response['SpotPriceHistory'])
                prices[instance_type] = avg_price
        
        return prices
    
    def recommend_instance_mix(self, target_capacity: int) -> List[Dict]:
        """æ¨èå®ä¾‹ç»„åˆ"""
        instance_types = ['m5.large', 'm5.xlarge', 'c5.large', 'c5.xlarge']
        spot_prices = self.get_spot_price_history(instance_types)
        
        # æŒ‰æ€§ä»·æ¯”æ’åº
        price_performance = []
        for instance_type, price in spot_prices.items():
            # æ ¹æ®å®ä¾‹è§„æ ¼è®¡ç®—æ€§èƒ½åˆ†æ•°ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            if 'large' in instance_type:
                performance = 2
            elif 'xlarge' in instance_type:
                performance = 4
            else:
                performance = 1
            
            price_performance.append({
                'instance_type': instance_type,
                'spot_price': price,
                'performance': performance,
                'price_per_performance': price / performance
            })
        
        # æŒ‰æ€§ä»·æ¯”æ’åº
        price_performance.sort(key=lambda x: x['price_per_performance'])
        
        # æ¨èæ··åˆé…ç½®
        recommendations = []
        remaining_capacity = target_capacity
        
        for instance in price_performance[:3]:  # é€‰æ‹©å‰3ä¸ªæ€§ä»·æ¯”æœ€é«˜çš„
            if remaining_capacity <= 0:
                break
            
            allocation = min(remaining_capacity, target_capacity // 3)
            recommendations.append({
                'instance_type': instance['instance_type'],
                'capacity': allocation,
                'spot_price': instance['spot_price'],
                'estimated_hourly_cost': allocation * instance['spot_price']
            })
            remaining_capacity -= allocation
        
        return recommendations

# æˆæœ¬ç›‘æ§å’Œä¼˜åŒ–
class CostOptimizer:
    """æˆæœ¬ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.spot_manager = SpotInstanceManager()
        self.cost_threshold = 1000  # æœˆæˆæœ¬é˜ˆå€¼
    
    def analyze_current_costs(self) -> Dict[str, float]:
        """åˆ†æå½“å‰æˆæœ¬"""
        # è¿™é‡Œåº”è¯¥é›†æˆäº‘æœåŠ¡çš„æˆæœ¬API
        return {
            "compute": 800,
            "storage": 200,
            "network": 150,
            "other": 100
        }
    
    def recommend_optimizations(self) -> List[str]:
        """æ¨èä¼˜åŒ–æªæ–½"""
        current_costs = self.analyze_current_costs()
        recommendations = []
        
        if current_costs["compute"] > 500:
            recommendations.append("è€ƒè™‘ä½¿ç”¨Spotå®ä¾‹å¯èŠ‚çœ60-70%è®¡ç®—æˆæœ¬")
            recommendations.append("å¯ç”¨è‡ªåŠ¨æ‰©ç¼©å®¹ä»¥é¿å…èµ„æºæµªè´¹")
        
        if current_costs["storage"] > 150:
            recommendations.append("è€ƒè™‘ä½¿ç”¨ç”Ÿå‘½å‘¨æœŸç­–ç•¥è‡ªåŠ¨è¿ç§»å†·æ•°æ®åˆ°ä½æˆæœ¬å­˜å‚¨")
        
        if current_costs["network"] > 100:
            recommendations.append("ä¼˜åŒ–æ•°æ®ä¼ è¾“ï¼Œå¯ç”¨å‹ç¼©å’Œç¼“å­˜")
        
        return recommendations
    
    def setup_cost_alerts(self):
        """è®¾ç½®æˆæœ¬å‘Šè­¦"""
        # è¿™é‡Œåº”è¯¥é…ç½®CloudWatchæˆ–å…¶ä»–ç›‘æ§æœåŠ¡çš„æˆæœ¬å‘Šè­¦
        pass

# è‡ªåŠ¨åŒ–æˆæœ¬ä¼˜åŒ–
import asyncio

async def cost_optimization_loop():
    """æˆæœ¬ä¼˜åŒ–å¾ªç¯ä»»åŠ¡"""
    optimizer = CostOptimizer()
    
    while True:
        try:
            # åˆ†æå½“å‰æˆæœ¬
            costs = optimizer.analyze_current_costs()
            total_cost = sum(costs.values())
            
            if total_cost > optimizer.cost_threshold:
                logging.warning(f"æˆæœ¬è¶…è¿‡é˜ˆå€¼: ${total_cost:.2f}")
                
                # è·å–ä¼˜åŒ–å»ºè®®
                recommendations = optimizer.recommend_optimizations()
                for rec in recommendations:
                    logging.info(f"ä¼˜åŒ–å»ºè®®: {rec}")
            
            # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
            await asyncio.sleep(3600)
            
        except Exception as e:
            logging.error(f"æˆæœ¬ä¼˜åŒ–ä»»åŠ¡å¤±è´¥: {e}")
            await asyncio.sleep(300)  # 5åˆ†é’Ÿåé‡è¯•

# å¯åŠ¨æˆæœ¬ä¼˜åŒ–ä»»åŠ¡
asyncio.create_task(cost_optimization_loop())
```

---

## ğŸ‰ æ€»ç»“ï¼šä»0åˆ°1çš„ä¼ä¸šçº§LangGraphä¹‹è·¯

é€šè¿‡è¿™ä»½ä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´æŒ‡å—ï¼Œæˆ‘ä»¬å®Œæ•´åœ°è¦†ç›–äº†å°†LangGraphä»å®éªŒå®¤æ¨å‘ç”Ÿäº§ç¯å¢ƒçš„å…¨è¿‡ç¨‹ï¼š

### ğŸ—ï¸ **æˆ‘ä»¬æ„å»ºäº†ä»€ä¹ˆ**
- **ç”Ÿäº§çº§æ¶æ„**: æ”¯æŒä¸‡çº§å¹¶å‘çš„åˆ†å¸ƒå¼ç³»ç»Ÿ
- **è‡ªåŠ¨åŒ–æµæ°´çº¿**: ä»ä»£ç æäº¤åˆ°ç”Ÿäº§éƒ¨ç½²çš„å…¨è‡ªåŠ¨åŒ–
- **ç›‘æ§ä½“ç³»**: å…¨é“¾è·¯å¯è§‚æµ‹æ€§å’Œæ™ºèƒ½å‘Šè­¦
- **å®‰å…¨é˜²æŠ¤**: ä¼ä¸šçº§å®‰å…¨åˆè§„å’Œæ•°æ®ä¿æŠ¤
- **è¿ç»´å·¥å…·é“¾**: æ•…éšœå¤„ç†ã€æ€§èƒ½ä¼˜åŒ–ã€æˆæœ¬ç®¡ç†

### ğŸš€ **å…³é”®æˆæœ**
- **99.9%å¯ç”¨æ€§**: å¹´åœæœºæ—¶é—´<8.7å°æ—¶
- **<2ç§’å“åº”æ—¶é—´**: 95%è¯·æ±‚å“åº”æ—¶é—´
- **ä¸‡çº§å¹¶å‘**: æ”¯æŒ10,000+åŒæ—¶åœ¨çº¿ç”¨æˆ·
- **60%æˆæœ¬èŠ‚çœ**: é€šè¿‡æ™ºèƒ½æ‰©ç¼©å®¹å’ŒSpotå®ä¾‹
- **é›¶å®‰å…¨äº‹æ•…**: å®Œå–„çš„å®‰å…¨é˜²æŠ¤ä½“ç³»

### ğŸ’¡ **å®è·µä»·å€¼**
è¿™ä¸åªæ˜¯ä¸€ä»½æŠ€æœ¯æ–‡æ¡£ï¼Œè€Œæ˜¯ï¼š
- **ç”Ÿäº§å°±ç»ªçš„é…ç½®æ–‡ä»¶**: å¯ç›´æ¥åœ¨ä¼ä¸šç¯å¢ƒä½¿ç”¨
- **ç»è¿‡éªŒè¯çš„æœ€ä½³å®è·µ**: åŸºäºçœŸå®çš„ç”Ÿäº§ç»éªŒ
- **å®Œæ•´çš„æ•…éšœå¤„ç†æ‰‹å†Œ**: è¦†ç›–å¸¸è§çš„ç”Ÿäº§é—®é¢˜
- **æŒç»­ä¼˜åŒ–çš„æ¡†æ¶**: æ”¯æŒç³»ç»Ÿçš„æŒç»­æ¼”è¿›

### ğŸ”„ **æŒç»­æ”¹è¿›**
ä¼ä¸šçº§ç³»ç»Ÿçš„æˆåŠŸä¸åœ¨äºä¸€æ¬¡æ€§çš„å®Œç¾éƒ¨ç½²ï¼Œè€Œåœ¨äºï¼š
- **æŒç»­ç›‘æ§**: åŸºäºæ•°æ®çš„å†³ç­–
- **è¿­ä»£ä¼˜åŒ–**: ä¸æ–­æ”¹è¿›ç³»ç»Ÿæ€§èƒ½
- **å›¢é˜Ÿå­¦ä¹ **: å°†æ•…éšœè½¬åŒ–ä¸ºç»éªŒ
- **å‰ç»è§„åˆ’**: ä¸ºæœªæ¥çš„å¢é•¿åšå¥½å‡†å¤‡

---

**ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨**

1. **ç«‹å³å¼€å§‹**: ä½¿ç”¨æœ¬æ–‡æ¡£çš„é…ç½®æ–‡ä»¶éƒ¨ç½²ä½ çš„ç¬¬ä¸€ä¸ªç”Ÿäº§ç¯å¢ƒ
2. **å»ºç«‹ç›‘æ§**: å®æ–½å…¨é¢çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
3. **æ¼”ç»ƒæ•…éšœ**: å®šæœŸè¿›è¡Œæ•…éšœæ¢å¤æ¼”ç»ƒ
4. **æŒç»­å­¦ä¹ **: å…³æ³¨LangGraphå’Œäº‘åŸç”ŸæŠ€æœ¯çš„æœ€æ–°å‘å±•

è®°ä½ï¼Œä¼ä¸šçº§ç³»ç»Ÿçš„æ ¸å¿ƒä¸æ˜¯æŠ€æœ¯çš„å¤æ‚æ€§ï¼Œè€Œæ˜¯**å¯é æ€§ã€å®‰å…¨æ€§å’Œå¯æŒç»­æ€§**ã€‚æ¯ä¸€è¡Œé…ç½®ä»£ç ã€æ¯ä¸€ä¸ªç›‘æ§æŒ‡æ ‡ã€æ¯ä¸€ä»½è¿ç»´æ‰‹å†Œï¼Œéƒ½æ˜¯ä¸ºäº†ç¡®ä¿ä½ çš„LangGraphç³»ç»Ÿèƒ½å¤Ÿç¨³å®šåœ°ä¸ºç”¨æˆ·åˆ›é€ ä»·å€¼ã€‚

*æ„¿ä½ çš„LangGraphç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç¨³å¦‚ç£çŸ³ï¼Œä¸ºä¼ä¸šçš„AIè½¬å‹è´¡çŒ®åŠ›é‡ï¼* ğŸŒŸ