# L4: é«˜çº§ç‰¹æ€§ä¸æ€§èƒ½ä¼˜åŒ–

**å­¦ä¹ ç›®æ ‡**: æ„å»ºä¼ä¸šçº§LangGraphç³»ç»Ÿï¼Œå®ç°ç”Ÿäº§ç¯å¢ƒçš„é«˜æ€§èƒ½å’Œé«˜å¯é æ€§  
**é¢„è®¡ç”¨æ—¶**: 5-6å°æ—¶  
**æ ¸å¿ƒè½¬å˜**: ä»"åŠŸèƒ½å®ç°"æ€ç»´ â†’ "æ€§èƒ½å·¥ç¨‹"æ€ç»´

*ğŸ’¡ è¿™ä¸€ç« å°†å¸¦ä½ ä»åŸå‹å¼€å‘è·¨è¶Šåˆ°ä¼ä¸šçº§ç³»ç»Ÿè®¾è®¡ã€‚ä½ å°†å­¦ä¼šå¦‚ä½•è®©LangGraphç³»ç»Ÿæ”¯æŒä¸‡çº§å¹¶å‘ã€ç§’çº§æ¢å¤ã€å®æ—¶ç›‘æ§ï¼Œä»¥åŠå¦‚ä½•è¿›è¡Œç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–ã€‚*

---

## ğŸŒŸ å¼€ç¯‡ï¼šä¼ä¸šçº§ç³»ç»Ÿçš„ç¥å¥‡åŠ›é‡

### ä»¤äººéœ‡æ’¼çš„æ€§èƒ½è¡¨ç°

æƒ³è±¡è¿™æ ·ä¸€ä¸ªAIå®¢æœç³»ç»Ÿï¼š

```python
# ğŸš€ ä¼ä¸šçº§AIå®¢æœä¸­å¿ƒå®æ—¶æ•°æ®
"""
ğŸ“Š ç³»ç»ŸçŠ¶æ€é¢æ¿
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”¥ åœ¨çº¿ç”¨æˆ·: 12,847 äºº
âš¡ å½“å‰QPS: 1,247 è¯·æ±‚/ç§’  
ğŸ’¾ å¹³å‡å“åº”æ—¶é—´: 0.89 ç§’
ğŸ¯ æˆåŠŸç‡: 99.97%
ğŸ”„ ä¼šè¯æ¢å¤ç‡: 100% (å³ä½¿æœåŠ¡å™¨é‡å¯)
ğŸ’¡ ç¼“å­˜å‘½ä¸­ç‡: 87.3%
ğŸ“ˆ CPUä½¿ç”¨ç‡: 45% | å†…å­˜ä½¿ç”¨: 2.1GB/8GB
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸  å‘Šè­¦æ—¥å¿—
[2024-01-15 14:23:15] æœåŠ¡å™¨node-03é‡å¯å®Œæˆï¼Œ1,245ä¸ªä¼šè¯æ— ç¼æ¢å¤ âœ…
[2024-01-15 14:20:33] æ£€æµ‹åˆ°æµé‡æ¿€å¢ï¼Œè‡ªåŠ¨æ‰©å®¹+2å®ä¾‹ ğŸ”„
[2024-01-15 14:18:07] ç¼“å­˜å‘½ä¸­ç‡ä¸‹é™åˆ°82%ï¼Œè‡ªåŠ¨ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ ğŸ¯
"""
```

**è¿™ç§å·¥ä¸šçº§çš„å¼ºå¤§èƒ½åŠ›æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ** ğŸ¤”

- ğŸ—ï¸ **çŠ¶æ€æŒä¹…åŒ–**ï¼šå³ä½¿ç³»ç»Ÿé‡å¯ï¼Œæ‰€æœ‰ç”¨æˆ·ä¼šè¯éƒ½èƒ½æ— ç¼æ¢å¤
- âš¡ **é«˜æ€§èƒ½å¹¶å‘**ï¼šå•æœºæ”¯æŒ1000+å¹¶å‘ï¼Œé›†ç¾¤æ”¯æŒæ— é™æ‰©å±•
- ğŸ“Š **å®æ—¶ç›‘æ§**ï¼šæ¯«ç§’çº§çš„æ€§èƒ½ç›‘æ§å’Œæ™ºèƒ½å‘Šè­¦
- ğŸ§  **æ™ºèƒ½ä¼˜åŒ–**ï¼šè‡ªåŠ¨ç¼“å­˜ã€èµ„æºè°ƒåº¦ã€æ€§èƒ½è°ƒä¼˜
- ğŸ›¡ï¸ **æ•…éšœè‡ªæ„ˆ**ï¼šç§’çº§æ•…éšœæ£€æµ‹å’Œè‡ªåŠ¨æ¢å¤

è¿™å°±æ˜¯ä¼ä¸šçº§LangGraphç³»ç»Ÿçš„çœŸå®èƒ½åŠ›ï¼

### ä»åŸå‹åˆ°ç”Ÿäº§çš„æŠ€æœ¯é¸¿æ²Ÿ

**åŸå‹ç³»ç»Ÿçš„å±€é™**ï¼š
```python
# åŸå‹é˜¶æ®µï¼šèƒ½è·‘å°±è¡Œ
app = StateGraph(MyState)
app.add_node("process", simple_process)
result = app.invoke(input_data)  # å•æ¬¡è°ƒç”¨ï¼Œæ— çŠ¶æ€ä¿å­˜
```

**ä¼ä¸šçº§ç³»ç»Ÿçš„è¦æ±‚**ï¼š
```python
# ç”Ÿäº§ç¯å¢ƒï¼šå¿…é¡»è€ƒè™‘æ‰€æœ‰è¾¹ç•Œæƒ…å†µ
app = StateGraph(MyState)
app.add_node("process", enterprise_process)

# é…ç½®æŒä¹…åŒ–
checkpointer = PostgreSQLCheckpointer(connection_string)
app = app.compile(checkpointer=checkpointer)

# å¹¶å‘æ§åˆ¶
executor = ConcurrentExecutor(max_workers=100, queue_size=1000)

# æ€§èƒ½ç›‘æ§
monitor = PerformanceMonitor(metrics_backend="prometheus")

# ç¼“å­˜ä¼˜åŒ–
cache = RedisCache(cluster_nodes=["redis-1", "redis-2", "redis-3"])

# æ•…éšœæ¢å¤
circuit_breaker = CircuitBreaker(failure_threshold=0.1, recovery_timeout=30)
```

**æŠ€æœ¯æŒ‘æˆ˜çš„é‡çº§å·®å¼‚**ï¼š

| ç»´åº¦ | åŸå‹ç³»ç»Ÿ | ä¼ä¸šçº§ç³»ç»Ÿ | å¤æ‚åº¦æå‡ |
|------|----------|------------|------------|
| **å¹¶å‘é‡** | 1-10ä¸ªç”¨æˆ· | 10,000+ç”¨æˆ· | 1000å€ |
| **å¯ç”¨æ€§** | 95%ï¼ˆå¶å°”å´©æºƒï¼‰ | 99.9%ï¼ˆå¹´åœæœº<9å°æ—¶ï¼‰ | 50å€æ”¹è¿› |
| **å“åº”æ—¶é—´** | 5-10ç§’ | <1ç§’ | 10å€æå‡ |
| **æ•°æ®é‡** | MBçº§ | TBçº§ | 1000å€ |
| **å¤æ‚åº¦** | å•åŠŸèƒ½ | å¤šæ¨¡å—åä½œ | æŒ‡æ•°çº§ |

## ğŸ—ï¸ çŠ¶æ€æŒä¹…åŒ–ï¼šæ°¸ä¸ä¸¢å¤±çš„è®°å¿†

### ä¸ºä»€ä¹ˆéœ€è¦çŠ¶æ€æŒä¹…åŒ–ï¼Ÿ

**ä¸šåŠ¡åœºæ™¯éœ€æ±‚**ï¼š
- ğŸ’¬ **é•¿æœŸå¯¹è¯**ï¼šç”¨æˆ·å¯èƒ½èŠ±å‡ å°æ—¶è§£å†³å¤æ‚é—®é¢˜
- ğŸ”„ **ç³»ç»Ÿç»´æŠ¤**ï¼šæœåŠ¡æ›´æ–°æ—¶ä¸èƒ½ä¸¢å¤±ç”¨æˆ·ä¼šè¯
- ğŸ“± **è·¨è®¾å¤‡**ï¼šç”¨æˆ·åœ¨æ‰‹æœºå¼€å§‹ï¼Œç”µè„‘ç»§ç»­
- ğŸ›¡ï¸ **å®¹ç¾æ¢å¤**ï¼šç¡¬ä»¶æ•…éšœæ—¶çš„å¿«é€Ÿæ¢å¤
- ğŸ“Š **å®¡è®¡è¿½è¸ª**ï¼šå®Œæ•´è®°å½•å†³ç­–è¿‡ç¨‹ç”¨äºåˆè§„

### åŸºç¡€æŒä¹…åŒ–ï¼šä»å†…å­˜åˆ°ç£ç›˜

**ç¬¬ä¸€æ­¥ï¼šé…ç½®SQLiteæŒä¹…åŒ–**

```python
import sqlite3
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict, NotRequired
import time
import threading

# 1. è®¾è®¡æ”¯æŒæŒä¹…åŒ–çš„çŠ¶æ€ç»“æ„
class PersistentChatState(TypedDict):
    # æ ¸å¿ƒä¸šåŠ¡æ•°æ®
    user_id: str
    session_id: str
    conversation_history: NotRequired[list]
    current_context: NotRequired[str]
    
    # æŒä¹…åŒ–å…ƒæ•°æ®
    created_at: NotRequired[str]
    last_updated: NotRequired[str]
    checkpoint_version: NotRequired[int]
    
    # ä¸šåŠ¡çŠ¶æ€è·Ÿè¸ª
    current_intent: NotRequired[str]
    satisfaction_score: NotRequired[float]
    escalation_needed: NotRequired[bool]

def intelligent_chat_agent(state: PersistentChatState):
    """æ™ºèƒ½èŠå¤©ä»£ç†"""
    user_id = state["user_id"]
    history = state.get("conversation_history", [])
    
    print(f"ğŸ’¬ ä¸ºç”¨æˆ· {user_id} å¤„ç†æ¶ˆæ¯ (å†å²è®°å½•: {len(history)} æ¡)")
    
    # æ¨¡æ‹Ÿå¤æ‚çš„AIå¤„ç†é€»è¾‘
    time.sleep(0.5)  # æ¨¡æ‹ŸAIæ€è€ƒæ—¶é—´
    
    # ç”Ÿæˆå›å¤
    response = f"åŸºäºæ‚¨çš„ {len(history)} æ¡å†å²å¯¹è¯ï¼Œæˆ‘ç†è§£æ‚¨çš„éœ€æ±‚..."
    
    # æ›´æ–°çŠ¶æ€
    updated_history = history + [
        {"role": "user", "content": "ç”¨æˆ·è¾“å…¥..."},
        {"role": "assistant", "content": response}
    ]
    
    return {
        "conversation_history": updated_history,
        "last_updated": str(int(time.time())),
        "checkpoint_version": state.get("checkpoint_version", 0) + 1,
        "current_context": "ç”¨æˆ·å’¨è¯¢äº§å“åŠŸèƒ½"
    }

def satisfaction_checker(state: PersistentChatState):
    """æ»¡æ„åº¦æ£€æŸ¥"""
    history = state.get("conversation_history", [])
    
    # æ¨¡æ‹Ÿæ»¡æ„åº¦åˆ†æ
    satisfaction = min(0.9, 0.5 + len(history) * 0.1)
    
    print(f"ğŸ“Š æ»¡æ„åº¦è¯„ä¼°: {satisfaction:.2f}")
    
    return {
        "satisfaction_score": satisfaction,
        "escalation_needed": satisfaction < 0.7
    }

# 2. åˆ›å»ºæŒä¹…åŒ–çš„èŠå¤©åº”ç”¨
def create_persistent_chat_app():
    """åˆ›å»ºå…·æœ‰æŒä¹…åŒ–åŠŸèƒ½çš„èŠå¤©åº”ç”¨"""
    
    # é…ç½®SQLiteæ£€æŸ¥ç‚¹ä¿å­˜å™¨
    db_path = "chat_sessions.db"
    checkpointer = SqliteSaver.from_conn_string(f"sqlite:///{db_path}")
    
    # æ„å»ºå›¾
    workflow = StateGraph(PersistentChatState)
    workflow.add_node("chat_agent", intelligent_chat_agent)
    workflow.add_node("satisfaction_check", satisfaction_checker)
    
    # è®¾ç½®æµç¨‹
    workflow.set_entry_point("chat_agent")
    workflow.add_edge("chat_agent", "satisfaction_check")
    workflow.add_edge("satisfaction_check", END)
    
    # ç¼–è¯‘å¹¶é…ç½®æŒä¹…åŒ–
    app = workflow.compile(checkpointer=checkpointer)
    
    return app

# 3. æµ‹è¯•æŒä¹…åŒ–åŠŸèƒ½
def test_persistence():
    """æµ‹è¯•çŠ¶æ€æŒä¹…åŒ–åŠŸèƒ½"""
    app = create_persistent_chat_app()
    
    # ä¼šè¯é…ç½®
    config = {
        "configurable": {
            "thread_id": "user_12345_session"  # å”¯ä¸€çš„ä¼šè¯æ ‡è¯†
        }
    }
    
    print("ğŸš€ å¼€å§‹ç¬¬ä¸€è½®å¯¹è¯...")
    
    # ç¬¬ä¸€è½®å¯¹è¯
    result1 = app.invoke({
        "user_id": "user_12345",
        "session_id": "session_001",
        "created_at": str(int(time.time()))
    }, config=config)
    
    print(f"âœ… ç¬¬ä¸€è½®å®Œæˆï¼Œå¯¹è¯å†å²: {len(result1['conversation_history'])} æ¡")
    print(f"ğŸ“Š æ»¡æ„åº¦: {result1['satisfaction_score']:.2f}")
    
    # æ¨¡æ‹Ÿç³»ç»Ÿé‡å¯ï¼ˆé‡æ–°åˆ›å»ºåº”ç”¨å®ä¾‹ï¼‰
    print("\nğŸ”„ æ¨¡æ‹Ÿç³»ç»Ÿé‡å¯...")
    time.sleep(1)
    
    # é‡æ–°åˆ›å»ºåº”ç”¨ï¼ˆæ¨¡æ‹Ÿé‡å¯åï¼‰
    app_after_restart = create_persistent_chat_app()
    
    print("ğŸ”„ ä»æ£€æŸ¥ç‚¹æ¢å¤å¯¹è¯...")
    
    # ç»§ç»­å¯¹è¯ï¼ˆçŠ¶æ€è‡ªåŠ¨ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼‰
    result2 = app_after_restart.invoke({
        "user_id": "user_12345",
        "session_id": "session_001"  # ç›¸åŒçš„session_id
    }, config=config)
    
    print(f"âœ… æ¢å¤æˆåŠŸï¼å†å²è®°å½•: {len(result2['conversation_history'])} æ¡")
    print(f"ğŸ“Š æ£€æŸ¥ç‚¹ç‰ˆæœ¬: {result2['checkpoint_version']}")
    print(f"ğŸ• æœ€åæ›´æ–°: {result2['last_updated']}")
    
    return result2

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    result = test_persistence()
    
    print("\n" + "="*50)
    print("ğŸ¯ æŒä¹…åŒ–æµ‹è¯•ç»“æœ")
    print("="*50)
    print(f"ç”¨æˆ·ID: {result['user_id']}")
    print(f"ä¼šè¯ID: {result['session_id']}")
    print(f"å¯¹è¯è½®æ•°: {len(result['conversation_history'])} æ¡")
    print(f"æ»¡æ„åº¦: {result['satisfaction_score']:.2f}")
    print(f"æ£€æŸ¥ç‚¹ç‰ˆæœ¬: {result['checkpoint_version']}")
    print(f"ç³»ç»Ÿé‡å¯åæ¢å¤: âœ… æˆåŠŸ")
```

### ğŸ” æ·±å…¥ç†è§£ï¼šæŒä¹…åŒ–çš„æ ¸å¿ƒæœºåˆ¶

**1. æ£€æŸ¥ç‚¹çš„å·¥ä½œåŸç†**ï¼š
```python
# æ¯æ¬¡çŠ¶æ€æ›´æ–°æ—¶ï¼ŒLangGraphè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹
state_update = {
    "conversation_history": [...],
    "satisfaction_score": 0.8
}

# å†…éƒ¨è¿‡ç¨‹ï¼š
# 1. åºåˆ—åŒ–çŠ¶æ€æ•°æ®
# 2. ç”Ÿæˆå”¯ä¸€çš„æ£€æŸ¥ç‚¹ID
# 3. ä¿å­˜åˆ°SQLiteæ•°æ®åº“
# 4. æ›´æ–°æ£€æŸ¥ç‚¹ç´¢å¼•
```

**2. ä¼šè¯æ¢å¤çš„æµç¨‹**ï¼š
```python
# åº”ç”¨é‡å¯åé¦–æ¬¡è°ƒç”¨æ—¶ï¼š
# 1. æ ¹æ®thread_idæŸ¥è¯¢æœ€æ–°æ£€æŸ¥ç‚¹
# 2. ååºåˆ—åŒ–çŠ¶æ€æ•°æ®
# 3. ä»æ£€æŸ¥ç‚¹çŠ¶æ€ç»§ç»­æ‰§è¡Œ
# 4. æ–°çš„çŠ¶æ€æ›´æ–°ç»§ç»­ä¿å­˜
```

### é«˜çº§æŒä¹…åŒ–ï¼šåˆ†å¸ƒå¼å’Œç‰ˆæœ¬ç®¡ç†

**PostgreSQLé›†ç¾¤æŒä¹…åŒ–**ï¼š

```python
from langgraph.checkpoint.postgres import PostgresCheckpointer
import asyncpg
import asyncio
from typing import Dict, Any
import json

class EnterpriseCheckpointer:
    """ä¼ä¸šçº§æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, postgres_config: Dict[str, Any]):
        self.config = postgres_config
        self.connection_pool = None
        self.version_history_limit = 100  # ä¿ç•™å†å²ç‰ˆæœ¬æ•°é‡
    
    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        self.connection_pool = await asyncpg.create_pool(
            **self.config,
            min_size=10,
            max_size=100,
            command_timeout=60
        )
        
        # åˆ›å»ºç‰ˆæœ¬ç®¡ç†è¡¨
        await self._create_version_tables()
    
    async def _create_version_tables(self):
        """åˆ›å»ºç‰ˆæœ¬ç®¡ç†ç›¸å…³è¡¨"""
        async with self.connection_pool.acquire() as conn:
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS checkpoint_versions (
                    id SERIAL PRIMARY KEY,
                    thread_id VARCHAR(255) NOT NULL,
                    checkpoint_id VARCHAR(255) NOT NULL,
                    version_number INTEGER NOT NULL,
                    state_data JSONB NOT NULL,
                    metadata JSONB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(thread_id, version_number)
                );
                
                CREATE INDEX IF NOT EXISTS idx_thread_version 
                ON checkpoint_versions(thread_id, version_number DESC);
            """)
    
    async def save_versioned_checkpoint(self, thread_id: str, state: Dict[str, Any], 
                                      metadata: Dict[str, Any] = None):
        """ä¿å­˜å¸¦ç‰ˆæœ¬çš„æ£€æŸ¥ç‚¹"""
        async with self.connection_pool.acquire() as conn:
            # è·å–ä¸‹ä¸€ä¸ªç‰ˆæœ¬å·
            version_result = await conn.fetchrow("""
                SELECT COALESCE(MAX(version_number), 0) + 1 as next_version
                FROM checkpoint_versions 
                WHERE thread_id = $1
            """, thread_id)
            
            next_version = version_result['next_version']
            
            # ä¿å­˜æ–°ç‰ˆæœ¬
            await conn.execute("""
                INSERT INTO checkpoint_versions 
                (thread_id, checkpoint_id, version_number, state_data, metadata)
                VALUES ($1, $2, $3, $4, $5)
            """, 
                thread_id,
                f"{thread_id}_v{next_version}",
                next_version,
                json.dumps(state),
                json.dumps(metadata or {})
            )
            
            # æ¸…ç†æ—§ç‰ˆæœ¬
            await self._cleanup_old_versions(conn, thread_id)
            
            return next_version
    
    async def _cleanup_old_versions(self, conn, thread_id: str):
        """æ¸…ç†æ—§ç‰ˆæœ¬"""
        await conn.execute("""
            DELETE FROM checkpoint_versions 
            WHERE thread_id = $1 
            AND version_number <= (
                SELECT version_number 
                FROM checkpoint_versions 
                WHERE thread_id = $1 
                ORDER BY version_number DESC 
                OFFSET $2 LIMIT 1
            )
        """, thread_id, self.version_history_limit)
    
    async def rollback_to_version(self, thread_id: str, target_version: int) -> Dict[str, Any]:
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        async with self.connection_pool.acquire() as conn:
            result = await conn.fetchrow("""
                SELECT state_data, metadata
                FROM checkpoint_versions
                WHERE thread_id = $1 AND version_number = $2
            """, thread_id, target_version)
            
            if not result:
                raise ValueError(f"ç‰ˆæœ¬ {target_version} ä¸å­˜åœ¨")
            
            # åˆ›å»ºæ–°çš„ç‰ˆæœ¬ä½œä¸ºå›æ»šè®°å½•
            rollback_state = json.loads(result['state_data'])
            rollback_metadata = json.loads(result['metadata'])
            rollback_metadata['rollback_from'] = target_version
            
            new_version = await self.save_versioned_checkpoint(
                thread_id, rollback_state, rollback_metadata
            )
            
            return {
                "state": rollback_state,
                "version": new_version,
                "rollback_target": target_version
            }
    
    async def get_version_history(self, thread_id: str, limit: int = 10) -> list:
        """è·å–ç‰ˆæœ¬å†å²"""
        async with self.connection_pool.acquire() as conn:
            results = await conn.fetch("""
                SELECT version_number, created_at, 
                       jsonb_extract_path_text(metadata, 'description') as description
                FROM checkpoint_versions
                WHERE thread_id = $1
                ORDER BY version_number DESC
                LIMIT $2
            """, thread_id, limit)
            
            return [dict(row) for row in results]

# ä½¿ç”¨ç¤ºä¾‹
async def test_advanced_persistence():
    """æµ‹è¯•é«˜çº§æŒä¹…åŒ–åŠŸèƒ½"""
    
    # é…ç½®ä¼ä¸šçº§æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    checkpointer = EnterpriseCheckpointer({
        "host": "localhost",
        "port": 5432,
        "database": "langgraph_enterprise",
        "user": "langgraph_user",
        "password": "secure_password"
    })
    
    await checkpointer.initialize()
    
    thread_id = "enterprise_session_001"
    
    # ä¿å­˜å¤šä¸ªç‰ˆæœ¬
    for i in range(5):
        state = {
            "step": i,
            "data": f"å¤„ç†æ­¥éª¤ {i}",
            "timestamp": time.time()
        }
        
        metadata = {
            "description": f"å¤„ç†æ­¥éª¤ {i} å®Œæˆ",
            "user_action": f"action_{i}"
        }
        
        version = await checkpointer.save_versioned_checkpoint(
            thread_id, state, metadata
        )
        print(f"âœ… ä¿å­˜ç‰ˆæœ¬ {version}: {state['data']}")
    
    # æŸ¥çœ‹ç‰ˆæœ¬å†å²
    print("\nğŸ“‹ ç‰ˆæœ¬å†å²:")
    history = await checkpointer.get_version_history(thread_id)
    for record in history:
        print(f"  ç‰ˆæœ¬ {record['version_number']}: {record['description']} ({record['created_at']})")
    
    # å›æ»šæµ‹è¯•
    print("\nğŸ”„ å›æ»šåˆ°ç‰ˆæœ¬ 2...")
    rollback_result = await checkpointer.rollback_to_version(thread_id, 2)
    print(f"âœ… å›æ»šæˆåŠŸï¼Œæ–°ç‰ˆæœ¬: {rollback_result['version']}")
    print(f"ğŸ“„ æ¢å¤çš„çŠ¶æ€: {rollback_result['state']}")

# è¿è¡Œé«˜çº§æŒä¹…åŒ–æµ‹è¯•
# asyncio.run(test_advanced_persistence())
```

## âš¡ é«˜æ€§èƒ½å¹¶å‘æ§åˆ¶

### å¹¶å‘æŒ‘æˆ˜ï¼šä»å•ç”¨æˆ·åˆ°ä¸‡ç”¨æˆ·

**æ€§èƒ½å¯¹æ¯”å®éªŒ**ï¼š

```python
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from queue import Queue
import psutil
import statistics

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·"""
    
    def __init__(self):
        self.results = []
        self.start_time = None
        self.end_time = None
    
    def start_benchmark(self, name: str):
        """å¼€å§‹åŸºå‡†æµ‹è¯•"""
        self.test_name = name
        self.start_time = time.time()
        self.results = []
        print(f"ğŸš€ å¼€å§‹æµ‹è¯•: {name}")
    
    def record_request(self, response_time: float, success: bool = True):
        """è®°å½•è¯·æ±‚ç»“æœ"""
        self.results.append({
            "response_time": response_time,
            "success": success,
            "timestamp": time.time()
        })
    
    def finish_benchmark(self):
        """å®ŒæˆåŸºå‡†æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š"""
        self.end_time = time.time()
        
        if not self.results:
            print("âŒ æ²¡æœ‰æµ‹è¯•æ•°æ®")
            return
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        response_times = [r["response_time"] for r in self.results if r["success"]]
        success_count = sum(1 for r in self.results if r["success"])
        total_requests = len(self.results)
        
        duration = self.end_time - self.start_time
        qps = total_requests / duration
        success_rate = success_count / total_requests * 100
        
        print(f"\nğŸ“Š {self.test_name} æµ‹è¯•æŠ¥å‘Š")
        print("="*50)
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"æˆåŠŸè¯·æ±‚: {success_count}")
        print(f"æˆåŠŸç‡: {success_rate:.2f}%")
        print(f"æµ‹è¯•æ—¶é•¿: {duration:.2f}ç§’")
        print(f"QPS: {qps:.2f}")
        
        if response_times:
            print(f"å¹³å‡å“åº”æ—¶é—´: {statistics.mean(response_times):.3f}ç§’")
            print(f"ä¸­ä½æ•°å“åº”æ—¶é—´: {statistics.median(response_times):.3f}ç§’")
            print(f"95åˆ†ä½å“åº”æ—¶é—´: {statistics.quantiles(response_times, n=20)[18]:.3f}ç§’")
            print(f"æœ€å¤§å“åº”æ—¶é—´: {max(response_times):.3f}ç§’")
        
        return {
            "qps": qps,
            "success_rate": success_rate,
            "avg_response_time": statistics.mean(response_times) if response_times else 0,
            "p95_response_time": statistics.quantiles(response_times, n=20)[18] if response_times else 0
        }

# 1. åŸºç¡€ç‰ˆæœ¬ï¼šå•çº¿ç¨‹åŒæ­¥å¤„ç†
def sync_chat_processor(user_input: str, session_id: str) -> dict:
    """åŒæ­¥èŠå¤©å¤„ç†å™¨"""
    start_time = time.time()
    
    # æ¨¡æ‹ŸAIå¤„ç†ï¼šI/Oå¯†é›† + CPUå¯†é›†
    time.sleep(0.1)  # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    time.sleep(0.05)  # æ¨¡æ‹ŸAIæ¨ç†
    time.sleep(0.02)  # æ¨¡æ‹Ÿå“åº”ç”Ÿæˆ
    
    response_time = time.time() - start_time
    
    return {
        "session_id": session_id,
        "response": f"å¤„ç†å®Œæˆ: {user_input[:20]}...",
        "response_time": response_time,
        "thread_id": threading.current_thread().name
    }

def test_sync_performance():
    """æµ‹è¯•åŒæ­¥å¤„ç†æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    benchmark.start_benchmark("å•çº¿ç¨‹åŒæ­¥å¤„ç†")
    
    # æ¨¡æ‹Ÿ100ä¸ªç”¨æˆ·è¯·æ±‚
    for i in range(100):
        start = time.time()
        result = sync_chat_processor(f"ç”¨æˆ·æ¶ˆæ¯ {i}", f"session_{i}")
        end = time.time()
        
        benchmark.record_request(end - start)
        
        if i % 20 == 0:
            print(f"  å®Œæˆ {i+1}/100 è¯·æ±‚...")
    
    return benchmark.finish_benchmark()

# 2. å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼šçº¿ç¨‹æ± å¹¶å‘å¤„ç†
async def async_chat_processor(user_input: str, session_id: str) -> dict:
    """å¼‚æ­¥èŠå¤©å¤„ç†å™¨"""
    start_time = time.time()
    
    # ä½¿ç”¨asyncioæ¨¡æ‹Ÿå¼‚æ­¥I/O
    await asyncio.sleep(0.1)   # å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢
    await asyncio.sleep(0.05)  # å¼‚æ­¥AIæ¨ç†
    await asyncio.sleep(0.02)  # å¼‚æ­¥å“åº”ç”Ÿæˆ
    
    response_time = time.time() - start_time
    
    return {
        "session_id": session_id,
        "response": f"å¼‚æ­¥å¤„ç†å®Œæˆ: {user_input[:20]}...",
        "response_time": response_time,
        "task_id": asyncio.current_task().get_name() if asyncio.current_task() else "unknown"
    }

async def test_async_performance():
    """æµ‹è¯•å¼‚æ­¥å¤„ç†æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    benchmark.start_benchmark("å¼‚æ­¥å¹¶å‘å¤„ç†")
    
    # åˆ›å»º100ä¸ªå¹¶å‘ä»»åŠ¡
    tasks = []
    for i in range(100):
        task = async_chat_processor(f"ç”¨æˆ·æ¶ˆæ¯ {i}", f"session_{i}")
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()
    
    # è®°å½•ç»“æœ
    for result in results:
        if isinstance(result, dict):
            benchmark.record_request(result["response_time"])
        else:
            benchmark.record_request(0, success=False)
    
    return benchmark.finish_benchmark()

# 3. é«˜çº§ç‰ˆæœ¬ï¼šè¿æ¥æ±  + èµ„æºç®¡ç†
class ResourceManagedProcessor:
    """èµ„æºç®¡ç†çš„å¤„ç†å™¨"""
    
    def __init__(self, max_workers: int = 50, queue_size: int = 1000):
        self.max_workers = max_workers
        self.queue_size = queue_size
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        self.request_queue = Queue(maxsize=queue_size)
        self.active_sessions = {}
        self.session_lock = threading.Lock()
        
        # è¿æ¥æ± æ¨¡æ‹Ÿ
        self.db_connections = Queue(maxsize=10)
        for i in range(10):
            self.db_connections.put(f"db_conn_{i}")
    
    def get_db_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        try:
            return self.db_connections.get(timeout=1.0)
        except:
            raise Exception("æ•°æ®åº“è¿æ¥æ± å·²æ»¡")
    
    def release_db_connection(self, conn):
        """é‡Šæ”¾æ•°æ®åº“è¿æ¥"""
        try:
            self.db_connections.put(conn, timeout=0.1)
        except:
            pass  # è¿æ¥æ± å·²æ»¡ï¼Œä¸¢å¼ƒè¿æ¥
    
    async def process_with_resource_management(self, user_input: str, session_id: str) -> dict:
        """ä½¿ç”¨èµ„æºç®¡ç†çš„å¤„ç†æ–¹æ³•"""
        start_time = time.time()
        
        # è·å–æ•°æ®åº“è¿æ¥
        db_conn = None
        try:
            db_conn = self.get_db_connection()
            
            # ä¼šè¯çŠ¶æ€ç®¡ç†
            with self.session_lock:
                if session_id not in self.active_sessions:
                    self.active_sessions[session_id] = {
                        "created_at": time.time(),
                        "request_count": 0
                    }
                
                self.active_sessions[session_id]["request_count"] += 1
                session_info = self.active_sessions[session_id].copy()
            
            # æ¨¡æ‹Ÿå¤„ç†
            await asyncio.sleep(0.08)  # æ¯”åŸºç¡€ç‰ˆæœ¬æ›´å¿«çš„å¤„ç†
            
            response_time = time.time() - start_time
            
            return {
                "session_id": session_id,
                "response": f"èµ„æºç®¡ç†å¤„ç†: {user_input[:20]}...",
                "response_time": response_time,
                "session_requests": session_info["request_count"],
                "db_connection": db_conn
            }
            
        finally:
            # ç¡®ä¿é‡Šæ”¾èµ„æº
            if db_conn:
                self.release_db_connection(db_conn)

async def test_resource_managed_performance():
    """æµ‹è¯•èµ„æºç®¡ç†ç‰ˆæœ¬æ€§èƒ½"""
    benchmark = PerformanceBenchmark()
    benchmark.start_benchmark("èµ„æºç®¡ç†å¹¶å‘å¤„ç†")
    
    processor = ResourceManagedProcessor(max_workers=50, queue_size=1000)
    
    # åˆ›å»ºå¹¶å‘ä»»åŠ¡
    tasks = []
    for i in range(200):  # æµ‹è¯•æ›´å¤šè¯·æ±‚
        task = processor.process_with_resource_management(
            f"ç”¨æˆ·æ¶ˆæ¯ {i}", f"session_{i % 50}"  # 50ä¸ªä¼šè¯ï¼Œæ¯ä¸ª4ä¸ªè¯·æ±‚
        )
        tasks.append(task)
    
    # å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # è®°å½•ç»“æœ
    for result in results:
        if isinstance(result, dict):
            benchmark.record_request(result["response_time"])
        else:
            benchmark.record_request(0, success=False)
    
    return benchmark.finish_benchmark()

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
async def run_performance_comparison():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("ğŸ¯ LangGraph æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•ç³»ç»Ÿèµ„æº
    print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    print(f"   CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
    print(f"   å†…å­˜æ€»é‡: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    print(f"   å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024**3:.1f} GB")
    
    results = {}
    
    # 1. åŒæ­¥å¤„ç†æµ‹è¯•
    print(f"\n{'='*20} ç¬¬1è½®æµ‹è¯• {'='*20}")
    results["sync"] = test_sync_performance()
    
    # 2. å¼‚æ­¥å¤„ç†æµ‹è¯•
    print(f"\n{'='*20} ç¬¬2è½®æµ‹è¯• {'='*20}")
    results["async"] = await test_async_performance()
    
    # 3. èµ„æºç®¡ç†æµ‹è¯•
    print(f"\n{'='*20} ç¬¬3è½®æµ‹è¯• {'='*20}")
    results["resource_managed"] = await test_resource_managed_performance()
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\nğŸ† æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*60)
    print(f"{'æ¨¡å¼':<15} {'QPS':<10} {'æˆåŠŸç‡':<8} {'å¹³å‡å“åº”':<10} {'P95å“åº”':<10}")
    print("-"*60)
    
    for mode, result in results.items():
        print(f"{mode:<15} {result['qps']:<10.1f} {result['success_rate']:<8.1f}% "
              f"{result['avg_response_time']:<10.3f} {result['p95_response_time']:<10.3f}")
    
    # æ€§èƒ½æå‡è®¡ç®—
    if "sync" in results and "async" in results:
        qps_improvement = results["async"]["qps"] / results["sync"]["qps"]
        print(f"\nğŸ“ˆ å¼‚æ­¥æ¨¡å¼ QPS æå‡: {qps_improvement:.1f}x")
    
    if "async" in results and "resource_managed" in results:
        qps_improvement = results["resource_managed"]["qps"] / results["async"]["qps"]
        print(f"ğŸ“ˆ èµ„æºç®¡ç†æ¨¡å¼è¿›ä¸€æ­¥æå‡: {qps_improvement:.1f}x")

# è¿è¡Œæ€§èƒ½æµ‹è¯•
if __name__ == "__main__":
    asyncio.run(run_performance_comparison())
```

### ä¼ä¸šçº§å¹¶å‘æ¶æ„

```python
import asyncio
import aioredis
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
import uuid
import json
import time
from enum import Enum

class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"

@dataclass
class TaskInfo:
    task_id: str
    session_id: str
    user_id: str
    task_type: str
    payload: Dict[str, Any]
    status: TaskStatus
    created_at: float
    started_at: Optional[float] = None
    completed_at: Optional[float] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class EnterpriseConcurrencyManager:
    """ä¼ä¸šçº§å¹¶å‘ç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str = "redis://localhost", max_concurrent: int = 1000):
        self.redis_url = redis_url
        self.max_concurrent = max_concurrent
        self.redis_pool = None
        self.active_tasks: Dict[str, TaskInfo] = {}
        self.task_queues: Dict[str, asyncio.Queue] = {}
        self.workers: List[asyncio.Task] = []
        self.running = False
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "avg_processing_time": 0.0,
            "current_queue_size": 0
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨"""
        # åˆ›å»ºRedisè¿æ¥æ± 
        self.redis_pool = await aioredis.create_redis_pool(
            self.redis_url,
            minsize=10,
            maxsize=100
        )
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        self.task_queues = {
            "high_priority": asyncio.Queue(maxsize=100),
            "normal_priority": asyncio.Queue(maxsize=500),
            "low_priority": asyncio.Queue(maxsize=1000)
        }
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        await self._start_workers()
        
        self.running = True
        print(f"âœ… å¹¶å‘ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘: {self.max_concurrent}")
    
    async def _start_workers(self):
        """å¯åŠ¨å·¥ä½œçº¿ç¨‹"""
        worker_count = min(50, self.max_concurrent // 10)
        
        for i in range(worker_count):
            worker = asyncio.create_task(self._worker(f"worker_{i}"))
            self.workers.append(worker)
        
        print(f"ğŸš€ å¯åŠ¨ {worker_count} ä¸ªå·¥ä½œçº¿ç¨‹")
    
    async def _worker(self, worker_name: str):
        """å·¥ä½œçº¿ç¨‹ä¸»å¾ªç¯"""
        while self.running:
            try:
                # æŒ‰ä¼˜å…ˆçº§å¤„ç†ä»»åŠ¡
                task_info = None
                
                # é«˜ä¼˜å…ˆçº§é˜Ÿåˆ—
                if not self.task_queues["high_priority"].empty():
                    task_info = await self.task_queues["high_priority"].get()
                # æ™®é€šä¼˜å…ˆçº§é˜Ÿåˆ—
                elif not self.task_queues["normal_priority"].empty():
                    task_info = await self.task_queues["normal_priority"].get()
                # ä½ä¼˜å…ˆçº§é˜Ÿåˆ—
                elif not self.task_queues["low_priority"].empty():
                    task_info = await self.task_queues["low_priority"].get()
                else:
                    # æ²¡æœ‰ä»»åŠ¡ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´
                    await asyncio.sleep(0.1)
                    continue
                
                # å¤„ç†ä»»åŠ¡
                await self._process_task(task_info, worker_name)
                
            except Exception as e:
                print(f"âŒ å·¥ä½œçº¿ç¨‹ {worker_name} å‘ç”Ÿé”™è¯¯: {e}")
                await asyncio.sleep(1)
    
    async def _process_task(self, task_info: TaskInfo, worker_name: str):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        task_id = task_info.task_id
        
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task_info.status = TaskStatus.RUNNING
            task_info.started_at = time.time()
            self.active_tasks[task_id] = task_info
            
            # ä¿å­˜åˆ°Redis
            await self._save_task_to_redis(task_info)
            
            print(f"ğŸ”„ {worker_name} å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}")
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œä¸åŒçš„å¤„ç†é€»è¾‘
            if task_info.task_type == "chat_processing":
                result = await self._process_chat_task(task_info)
            elif task_info.task_type == "document_analysis":
                result = await self._process_document_task(task_info)
            elif task_info.task_type == "image_generation":
                result = await self._process_image_task(task_info)
            else:
                raise ValueError(f"æœªçŸ¥ä»»åŠ¡ç±»å‹: {task_info.task_type}")
            
            # ä»»åŠ¡å®Œæˆ
            task_info.status = TaskStatus.COMPLETED
            task_info.completed_at = time.time()
            task_info.result = result
            
            # æ›´æ–°æŒ‡æ ‡
            processing_time = task_info.completed_at - task_info.started_at
            self.metrics["completed_tasks"] += 1
            self.metrics["avg_processing_time"] = (
                self.metrics["avg_processing_time"] * (self.metrics["completed_tasks"] - 1) + processing_time
            ) / self.metrics["completed_tasks"]
            
            print(f"âœ… {worker_name} å®Œæˆä»»åŠ¡ {task_id} ({processing_time:.3f}s)")
            
        except asyncio.TimeoutError:
            task_info.status = TaskStatus.TIMEOUT
            task_info.error = "ä»»åŠ¡æ‰§è¡Œè¶…æ—¶"
            self.metrics["failed_tasks"] += 1
            print(f"â±ï¸ ä»»åŠ¡ {task_id} æ‰§è¡Œè¶…æ—¶")
            
        except Exception as e:
            task_info.status = TaskStatus.FAILED
            task_info.error = str(e)
            self.metrics["failed_tasks"] += 1
            print(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {e}")
        
        finally:
            # ä¿å­˜æœ€ç»ˆçŠ¶æ€
            await self._save_task_to_redis(task_info)
            
            # æ¸…ç†æ´»è·ƒä»»åŠ¡
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    async def _process_chat_task(self, task_info: TaskInfo) -> Dict[str, Any]:
        """å¤„ç†èŠå¤©ä»»åŠ¡"""
        payload = task_info.payload
        user_message = payload.get("message", "")
        context = payload.get("context", {})
        
        # æ¨¡æ‹Ÿå¤æ‚çš„AIå¤„ç†
        await asyncio.sleep(0.2)  # æ¨¡æ‹ŸAIæ¨ç†æ—¶é—´
        
        response = f"AIå›å¤: é’ˆå¯¹æ‚¨çš„é—®é¢˜ '{user_message[:50]}...'ï¼Œæˆ‘çš„å»ºè®®æ˜¯..."
        
        return {
            "response": response,
            "confidence": 0.95,
            "processing_time": time.time() - task_info.started_at
        }
    
    async def _process_document_task(self, task_info: TaskInfo) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£åˆ†æä»»åŠ¡"""
        payload = task_info.payload
        document_url = payload.get("document_url", "")
        analysis_type = payload.get("analysis_type", "summary")
        
        # æ¨¡æ‹Ÿæ–‡æ¡£å¤„ç†
        await asyncio.sleep(0.5)  # æ–‡æ¡£åˆ†æéœ€è¦æ›´é•¿æ—¶é—´
        
        return {
            "summary": f"æ–‡æ¡£ {document_url} çš„{analysis_type}åˆ†æç»“æœ",
            "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
            "confidence": 0.88
        }
    
    async def _process_image_task(self, task_info: TaskInfo) -> Dict[str, Any]:
        """å¤„ç†å›¾åƒç”Ÿæˆä»»åŠ¡"""
        payload = task_info.payload
        prompt = payload.get("prompt", "")
        style = payload.get("style", "realistic")
        
        # æ¨¡æ‹Ÿå›¾åƒç”Ÿæˆ
        await asyncio.sleep(1.0)  # å›¾åƒç”Ÿæˆéœ€è¦æœ€é•¿æ—¶é—´
        
        return {
            "image_url": f"https://images.example.com/{uuid.uuid4()}.png",
            "prompt": prompt,
            "style": style,
            "resolution": "1024x1024"
        }
    
    async def _save_task_to_redis(self, task_info: TaskInfo):
        """ä¿å­˜ä»»åŠ¡çŠ¶æ€åˆ°Redis"""
        task_data = {
            "task_id": task_info.task_id,
            "session_id": task_info.session_id,
            "user_id": task_info.user_id,
            "task_type": task_info.task_type,
            "status": task_info.status.value,
            "created_at": task_info.created_at,
            "started_at": task_info.started_at,
            "completed_at": task_info.completed_at,
            "result": task_info.result,
            "error": task_info.error
        }
        
        # ä¿å­˜ä»»åŠ¡è¯¦æƒ…
        await self.redis_pool.hset(
            f"task:{task_info.task_id}",
            mapping={k: json.dumps(v) if v is not None else "" for k, v in task_data.items()}
        )
        
        # è®¾ç½®è¿‡æœŸæ—¶é—´ï¼ˆ24å°æ—¶ï¼‰
        await self.redis_pool.expire(f"task:{task_info.task_id}", 86400)
        
        # æ›´æ–°ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨
        await self.redis_pool.lpush(
            f"user_tasks:{task_info.user_id}",
            task_info.task_id
        )
        await self.redis_pool.expire(f"user_tasks:{task_info.user_id}", 86400)
    
    async def submit_task(self, user_id: str, session_id: str, task_type: str,
                         payload: Dict[str, Any], priority: str = "normal") -> str:
        """æäº¤ä»»åŠ¡"""
        task_id = str(uuid.uuid4())
        
        task_info = TaskInfo(
            task_id=task_id,
            session_id=session_id,
            user_id=user_id,
            task_type=task_type,
            payload=payload,
            status=TaskStatus.PENDING,
            created_at=time.time()
        )
        
        # æ ¹æ®ä¼˜å…ˆçº§æ·»åŠ åˆ°å¯¹åº”é˜Ÿåˆ—
        queue_name = f"{priority}_priority"
        if queue_name not in self.task_queues:
            queue_name = "normal_priority"
        
        try:
            await self.task_queues[queue_name].put(task_info)
            self.metrics["total_tasks"] += 1
            self.metrics["current_queue_size"] = sum(q.qsize() for q in self.task_queues.values())
            
            print(f"ğŸ“ ä»»åŠ¡ {task_id} å·²æäº¤åˆ° {priority} ä¼˜å…ˆçº§é˜Ÿåˆ—")
            return task_id
            
        except asyncio.QueueFull:
            raise Exception(f"{priority} ä¼˜å…ˆçº§é˜Ÿåˆ—å·²æ»¡ï¼Œè¯·ç¨åé‡è¯•")
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        # å…ˆæ£€æŸ¥å†…å­˜ä¸­çš„æ´»è·ƒä»»åŠ¡
        if task_id in self.active_tasks:
            task_info = self.active_tasks[task_id]
            return {
                "task_id": task_info.task_id,
                "status": task_info.status.value,
                "created_at": task_info.created_at,
                "started_at": task_info.started_at,
                "completed_at": task_info.completed_at,
                "result": task_info.result,
                "error": task_info.error
            }
        
        # ä»RedisæŸ¥è¯¢
        task_data = await self.redis_pool.hgetall(f"task:{task_id}")
        if not task_data:
            return None
        
        return {
            "task_id": task_data.get(b"task_id", b"").decode(),
            "status": task_data.get(b"status", b"").decode(),
            "created_at": float(task_data.get(b"created_at", 0)),
            "started_at": float(task_data.get(b"started_at", 0)) if task_data.get(b"started_at") else None,
            "completed_at": float(task_data.get(b"completed_at", 0)) if task_data.get(b"completed_at") else None,
            "result": json.loads(task_data.get(b"result", b"null")),
            "error": task_data.get(b"error", b"").decode() or None
        }
    
    async def get_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        active_count = len(self.active_tasks)
        queue_sizes = {name: queue.qsize() for name, queue in self.task_queues.items()}
        
        return {
            **self.metrics,
            "active_tasks": active_count,
            "queue_sizes": queue_sizes,
            "worker_count": len(self.workers),
            "current_queue_size": sum(queue_sizes.values())
        }
    
    async def shutdown(self):
        """å…³é—­å¹¶å‘ç®¡ç†å™¨"""
        print("ğŸ”„ æ­£åœ¨å…³é—­å¹¶å‘ç®¡ç†å™¨...")
        
        self.running = False
        
        # ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹å®Œæˆ
        if self.workers:
            await asyncio.gather(*self.workers, return_exceptions=True)
        
        # å…³é—­Redisè¿æ¥
        if self.redis_pool:
            self.redis_pool.close()
            await self.redis_pool.wait_closed()
        
        print("âœ… å¹¶å‘ç®¡ç†å™¨å·²å…³é—­")

# æµ‹è¯•ä¼ä¸šçº§å¹¶å‘ç³»ç»Ÿ
async def test_enterprise_concurrency():
    """æµ‹è¯•ä¼ä¸šçº§å¹¶å‘ç³»ç»Ÿ"""
    
    # åˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨
    manager = EnterpriseConcurrencyManager(max_concurrent=1000)
    await manager.initialize()
    
    try:
        print("ğŸš€ å¼€å§‹å¹¶å‘æµ‹è¯•...")
        
        # æäº¤ä¸åŒç±»å‹çš„ä»»åŠ¡
        task_ids = []
        
        # æ‰¹é‡æäº¤èŠå¤©ä»»åŠ¡
        for i in range(50):
            task_id = await manager.submit_task(
                user_id=f"user_{i % 10}",
                session_id=f"session_{i}",
                task_type="chat_processing",
                payload={"message": f"ç”¨æˆ·é—®é¢˜ {i}", "context": {"topic": "æŠ€æœ¯å’¨è¯¢"}},
                priority="high" if i < 10 else "normal"
            )
            task_ids.append(task_id)
        
        # æäº¤æ–‡æ¡£åˆ†æä»»åŠ¡
        for i in range(20):
            task_id = await manager.submit_task(
                user_id=f"user_{i % 5}",
                session_id=f"doc_session_{i}",
                task_type="document_analysis",
                payload={"document_url": f"https://docs.example.com/doc_{i}.pdf", "analysis_type": "summary"},
                priority="normal"
            )
            task_ids.append(task_id)
        
        # æäº¤å›¾åƒç”Ÿæˆä»»åŠ¡
        for i in range(10):
            task_id = await manager.submit_task(
                user_id=f"user_{i % 3}",
                session_id=f"image_session_{i}",
                task_type="image_generation",
                payload={"prompt": f"ç”Ÿæˆå›¾åƒ {i}", "style": "realistic"},
                priority="low"
            )
            task_ids.append(task_id)
        
        print(f"ğŸ“ å·²æäº¤ {len(task_ids)} ä¸ªä»»åŠ¡")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        completed_count = 0
        while completed_count < len(task_ids):
            await asyncio.sleep(1)
            
            completed_count = 0
            for task_id in task_ids:
                status = await manager.get_task_status(task_id)
                if status and status["status"] in ["completed", "failed", "timeout"]:
                    completed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            metrics = await manager.get_metrics()
            print(f"ğŸ“Š è¿›åº¦: {completed_count}/{len(task_ids)} å®Œæˆ, "
                  f"æ´»è·ƒä»»åŠ¡: {metrics['active_tasks']}, "
                  f"é˜Ÿåˆ—å¤§å°: {metrics['current_queue_size']}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        print("\nğŸ¯ å¹¶å‘æµ‹è¯•å®Œæˆ!")
        metrics = await manager.get_metrics()
        
        print("="*50)
        print("ğŸ“Š æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡:")
        print(f"æ€»ä»»åŠ¡æ•°: {metrics['total_tasks']}")
        print(f"å®Œæˆä»»åŠ¡: {metrics['completed_tasks']}")
        print(f"å¤±è´¥ä»»åŠ¡: {metrics['failed_tasks']}")
        print(f"æˆåŠŸç‡: {metrics['completed_tasks']/metrics['total_tasks']*100:.1f}%")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {metrics['avg_processing_time']:.3f}ç§’")
        print(f"å·¥ä½œçº¿ç¨‹æ•°: {metrics['worker_count']}")
        
    finally:
        await manager.shutdown()

# è¿è¡Œä¼ä¸šçº§å¹¶å‘æµ‹è¯•
# asyncio.run(test_enterprise_concurrency())
```

## ğŸ“Š å®æ—¶æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

### æ„å»ºå…¨é¢çš„ç›‘æ§ä½“ç³»

```python
import time
import asyncio
import threading
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Callable
import json
import statistics
from enum import Enum
import psutil
import gc

class MetricType(str, Enum):
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIMER = "timer"

@dataclass
class MetricData:
    name: str
    type: MetricType
    value: float
    timestamp: float
    labels: Dict[str, str] = field(default_factory=dict)
    description: str = ""

class PerformanceMonitor:
    """é«˜æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self, retention_seconds: int = 3600):
        self.retention_seconds = retention_seconds
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque())
        self.counters: Dict[str, float] = defaultdict(float)
        self.gauges: Dict[str, float] = defaultdict(float)
        self.histograms: Dict[str, List[float]] = defaultdict(list)
        self.timers: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        
        # ç³»ç»Ÿç›‘æ§
        self.system_metrics_enabled = True
        self.custom_collectors: List[Callable] = []
        
        # å‘Šè­¦ç³»ç»Ÿ
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.alert_callbacks: List[Callable] = []
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring_thread = None
        self.running = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.start_time = time.time()
        
    def start_monitoring(self, interval: float = 1.0):
        """å¯åŠ¨ç›‘æ§"""
        self.running = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval,),
            daemon=True
        )
        self.monitoring_thread.start()
        print(f"ğŸ“Š æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ï¼Œé‡‡é›†é—´éš”: {interval}ç§’")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.running = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        print("ğŸ›‘ æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self, interval: float):
        """ç›‘æ§ä¸»å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                if self.system_metrics_enabled:
                    self._collect_system_metrics()
                
                # è¿è¡Œè‡ªå®šä¹‰æ”¶é›†å™¨
                for collector in self.custom_collectors:
                    try:
                        collector(self)
                    except Exception as e:
                        print(f"âŒ è‡ªå®šä¹‰æ”¶é›†å™¨é”™è¯¯: {e}")
                
                # æ¸…ç†è¿‡æœŸæ•°æ®
                self._cleanup_old_metrics()
                
                # æ£€æŸ¥å‘Šè­¦è§„åˆ™
                self._check_alerts()
                
                time.sleep(interval)
                
            except Exception as e:
                print(f"âŒ ç›‘æ§å¾ªç¯é”™è¯¯: {e}")
                time.sleep(interval)
    
    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        current_time = time.time()
        
        # CPUæŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=None)
        self.record_gauge("system_cpu_percent", cpu_percent, {"type": "total"})
        
        # å†…å­˜æŒ‡æ ‡
        memory = psutil.virtual_memory()
        self.record_gauge("system_memory_percent", memory.percent, {"type": "used"})
        self.record_gauge("system_memory_bytes", memory.used, {"type": "used"})
        self.record_gauge("system_memory_bytes", memory.available, {"type": "available"})
        
        # ç£ç›˜æŒ‡æ ‡
        disk = psutil.disk_usage('/')
        self.record_gauge("system_disk_percent", disk.percent, {"type": "used"})
        
        # ç½‘ç»œæŒ‡æ ‡
        network = psutil.net_io_counters()
        self.record_counter("system_network_bytes", network.bytes_sent, {"direction": "sent"})
        self.record_counter("system_network_bytes", network.bytes_recv, {"direction": "received"})
        
        # è¿›ç¨‹æŒ‡æ ‡
        process = psutil.Process()
        self.record_gauge("process_cpu_percent", process.cpu_percent())
        self.record_gauge("process_memory_bytes", process.memory_info().rss)
        self.record_gauge("process_threads", process.num_threads())
        
        # Python GCæŒ‡æ ‡
        gc_stats = gc.get_stats()
        for i, stats in enumerate(gc_stats):
            self.record_counter("python_gc_collections", stats['collections'], {"generation": str(i)})
            self.record_gauge("python_gc_objects", stats['collected'], {"generation": str(i)})
    
    def record_counter(self, name: str, value: float, labels: Dict[str, str] = None):
        """è®°å½•è®¡æ•°å™¨æŒ‡æ ‡"""
        key = self._make_metric_key(name, labels or {})
        self.counters[key] += value
        
        metric = MetricData(
            name=name,
            type=MetricType.COUNTER,
            value=self.counters[key],
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def record_gauge(self, name: str, value: float, labels: Dict[str, str] = None):
        """è®°å½•ä»ªè¡¨ç›˜æŒ‡æ ‡"""
        key = self._make_metric_key(name, labels or {})
        self.gauges[key] = value
        
        metric = MetricData(
            name=name,
            type=MetricType.GAUGE,
            value=value,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def record_histogram(self, name: str, value: float, labels: Dict[str, str] = None):
        """è®°å½•ç›´æ–¹å›¾æŒ‡æ ‡"""
        key = self._make_metric_key(name, labels or {})
        self.histograms[key].append(value)
        
        # ä¿æŒæœ€è¿‘1000ä¸ªå€¼
        if len(self.histograms[key]) > 1000:
            self.histograms[key] = self.histograms[key][-1000:]
        
        metric = MetricData(
            name=name,
            type=MetricType.HISTOGRAM,
            value=value,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def time_function(self, name: str, labels: Dict[str, str] = None):
        """å‡½æ•°æ‰§è¡Œæ—¶é—´è£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    duration = time.time() - start_time
                    self.record_timer(name, duration, labels)
            return wrapper
        return decorator
    
    def record_timer(self, name: str, duration: float, labels: Dict[str, str] = None):
        """è®°å½•è®¡æ—¶å™¨æŒ‡æ ‡"""
        key = self._make_metric_key(name, labels or {})
        self.timers[key].append(duration)
        
        metric = MetricData(
            name=name,
            type=MetricType.TIMER,
            value=duration,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def _make_metric_key(self, name: str, labels: Dict[str, str]) -> str:
        """ç”ŸæˆæŒ‡æ ‡é”®"""
        if not labels:
            return name
        
        label_str = ",".join(f"{k}={v}" for k, v in sorted(labels.items()))
        return f"{name}{{{label_str}}}"
    
    def _cleanup_old_metrics(self):
        """æ¸…ç†è¿‡æœŸæŒ‡æ ‡"""
        cutoff_time = time.time() - self.retention_seconds
        
        for key, metric_list in self.metrics.items():
            while metric_list and metric_list[0].timestamp < cutoff_time:
                metric_list.popleft()
    
    def get_metric_stats(self, name: str, labels: Dict[str, str] = None, 
                        duration: float = 300) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        key = self._make_metric_key(name, labels or {})
        cutoff_time = time.time() - duration
        
        recent_metrics = [
            m for m in self.metrics[key] 
            if m.timestamp >= cutoff_time
        ]
        
        if not recent_metrics:
            return {"error": "æ²¡æœ‰æ‰¾åˆ°æŒ‡æ ‡æ•°æ®"}
        
        values = [m.value for m in recent_metrics]
        
        stats = {
            "count": len(values),
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "first_timestamp": recent_metrics[0].timestamp,
            "last_timestamp": recent_metrics[-1].timestamp,
        }
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        if len(values) >= 2:
            stats["stddev"] = statistics.stdev(values)
            
        if len(values) >= 10:
            stats["p50"] = statistics.quantiles(values, n=2)[0]
            stats["p95"] = statistics.quantiles(values, n=20)[18]
            stats["p99"] = statistics.quantiles(values, n=100)[98]
        
        return stats
    
    def add_alert_rule(self, metric_name: str, condition: str, threshold: float,
                      callback: Optional[Callable] = None, description: str = ""):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        rule_id = f"{metric_name}_{condition}_{threshold}"
        
        self.alert_rules[rule_id] = {
            "metric_name": metric_name,
            "condition": condition,  # "gt", "lt", "eq", "ne"
            "threshold": threshold,
            "callback": callback,
            "description": description,
            "triggered": False,
            "last_triggered": None
        }
        
        print(f"ğŸ“¢ æ·»åŠ å‘Šè­¦è§„åˆ™: {metric_name} {condition} {threshold}")
    
    def _check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦è§„åˆ™"""
        current_time = time.time()
        
        for rule_id, rule in self.alert_rules.items():
            try:
                # è·å–æœ€æ–°æŒ‡æ ‡å€¼
                stats = self.get_metric_stats(rule["metric_name"], duration=60)
                
                if "error" in stats:
                    continue
                
                current_value = stats["mean"]  # ä½¿ç”¨å¹³å‡å€¼ä½œä¸ºå½“å‰å€¼
                
                # æ£€æŸ¥æ¡ä»¶
                triggered = False
                if rule["condition"] == "gt" and current_value > rule["threshold"]:
                    triggered = True
                elif rule["condition"] == "lt" and current_value < rule["threshold"]:
                    triggered = True
                elif rule["condition"] == "eq" and abs(current_value - rule["threshold"]) < 0.001:
                    triggered = True
                elif rule["condition"] == "ne" and abs(current_value - rule["threshold"]) >= 0.001:
                    triggered = True
                
                # è§¦å‘å‘Šè­¦
                if triggered and not rule["triggered"]:
                    rule["triggered"] = True
                    rule["last_triggered"] = current_time
                    
                    alert_info = {
                        "rule_id": rule_id,
                        "metric_name": rule["metric_name"],
                        "current_value": current_value,
                        "threshold": rule["threshold"],
                        "condition": rule["condition"],
                        "description": rule["description"],
                        "timestamp": current_time
                    }
                    
                    print(f"ğŸš¨ å‘Šè­¦è§¦å‘: {rule['metric_name']} = {current_value:.2f} "
                          f"{rule['condition']} {rule['threshold']}")
                    
                    # æ‰§è¡Œå›è°ƒ
                    if rule["callback"]:
                        try:
                            rule["callback"](alert_info)
                        except Exception as e:
                            print(f"âŒ å‘Šè­¦å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                    
                    # æ‰§è¡Œå…¨å±€å›è°ƒ
                    for callback in self.alert_callbacks:
                        try:
                            callback(alert_info)
                        except Exception as e:
                            print(f"âŒ å…¨å±€å‘Šè­¦å›è°ƒå¤±è´¥: {e}")
                
                # æ¢å¤çŠ¶æ€
                elif not triggered and rule["triggered"]:
                    rule["triggered"] = False
                    print(f"âœ… å‘Šè­¦æ¢å¤: {rule['metric_name']} = {current_value:.2f}")
                    
            except Exception as e:
                print(f"âŒ æ£€æŸ¥å‘Šè­¦è§„åˆ™ {rule_id} æ—¶å‡ºé”™: {e}")
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """è·å–ä»ªè¡¨æ¿æ•°æ®"""
        current_time = time.time()
        uptime = current_time - self.start_time
        
        # ç³»ç»Ÿæ¦‚è§ˆ
        system_overview = {
            "uptime_seconds": uptime,
            "uptime_human": self._format_duration(uptime),
            "current_time": current_time
        }
        
        # å…³é”®æŒ‡æ ‡
        key_metrics = {}
        important_metrics = [
            "system_cpu_percent",
            "system_memory_percent", 
            "process_memory_bytes",
            "langgraph_requests_total",
            "langgraph_response_time"
        ]
        
        for metric in important_metrics:
            stats = self.get_metric_stats(metric, duration=300)
            if "error" not in stats:
                key_metrics[metric] = {
                    "current": stats.get("mean", 0),
                    "min": stats.get("min", 0),
                    "max": stats.get("max", 0),
                    "p95": stats.get("p95", 0)
                }
        
        # æ´»è·ƒå‘Šè­¦
        active_alerts = [
            {
                "rule_id": rule_id,
                "metric_name": rule["metric_name"],
                "description": rule["description"],
                "last_triggered": rule["last_triggered"]
            }
            for rule_id, rule in self.alert_rules.items()
            if rule["triggered"]
        ]
        
        return {
            "system_overview": system_overview,
            "key_metrics": key_metrics,
            "active_alerts": active_alerts,
            "total_metrics": len(self.metrics),
            "alert_rules_count": len(self.alert_rules)
        }
    
    def _format_duration(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´é•¿åº¦"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours}h {minutes}m {secs}s"
        elif minutes > 0:
            return f"{minutes}m {secs}s"
        else:
            return f"{secs}s"
    
    def export_prometheus_metrics(self) -> str:
        """å¯¼å‡ºPrometheusæ ¼å¼æŒ‡æ ‡"""
        lines = []
        
        for key, metric_list in self.metrics.items():
            if not metric_list:
                continue
                
            latest_metric = metric_list[-1]
            
            # æ„é€ Prometheusæ ¼å¼
            if latest_metric.labels:
                label_str = ",".join(f'{k}="{v}"' for k, v in latest_metric.labels.items())
                line = f'{latest_metric.name}{{{label_str}}} {latest_metric.value} {int(latest_metric.timestamp * 1000)}'
            else:
                line = f'{latest_metric.name} {latest_metric.value} {int(latest_metric.timestamp * 1000)}'
            
            lines.append(line)
        
        return "\n".join(lines)

# LangGraphç›‘æ§é›†æˆ
class LangGraphMonitoringIntegration:
    """LangGraphç›‘æ§é›†æˆ"""
    
    def __init__(self, monitor: PerformanceMonitor):
        self.monitor = monitor
        self.request_count = 0
        self.error_count = 0
    
    def wrap_langgraph_app(self, app):
        """åŒ…è£…LangGraphåº”ç”¨ä»¥æ·»åŠ ç›‘æ§"""
        original_invoke = app.invoke
        original_ainvoke = app.ainvoke if hasattr(app, 'ainvoke') else None
        
        def monitored_invoke(input_data, config=None, **kwargs):
            start_time = time.time()
            self.request_count += 1
            
            # è®°å½•è¯·æ±‚å¼€å§‹
            self.monitor.record_counter("langgraph_requests_total", 1, {"method": "invoke"})
            
            try:
                result = original_invoke(input_data, config, **kwargs)
                
                # è®°å½•æˆåŠŸ
                duration = time.time() - start_time
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "success"})
                self.monitor.record_histogram("langgraph_request_duration", duration)
                
                return result
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                self.error_count += 1
                duration = time.time() - start_time
                self.monitor.record_counter("langgraph_errors_total", 1, {"error_type": type(e).__name__})
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "error"})
                
                raise
        
        async def monitored_ainvoke(input_data, config=None, **kwargs):
            start_time = time.time()
            self.request_count += 1
            
            # è®°å½•å¼‚æ­¥è¯·æ±‚å¼€å§‹
            self.monitor.record_counter("langgraph_requests_total", 1, {"method": "ainvoke"})
            
            try:
                result = await original_ainvoke(input_data, config, **kwargs)
                
                # è®°å½•æˆåŠŸ
                duration = time.time() - start_time
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "success"})
                self.monitor.record_histogram("langgraph_request_duration", duration)
                
                return result
                
            except Exception as e:
                # è®°å½•é”™è¯¯
                self.error_count += 1
                duration = time.time() - start_time
                self.monitor.record_counter("langgraph_errors_total", 1, {"error_type": type(e).__name__})
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "error"})
                
                raise
        
        # æ›¿æ¢æ–¹æ³•
        app.invoke = monitored_invoke
        if original_ainvoke:
            app.ainvoke = monitored_ainvoke
        
        return app
    
    def add_custom_metrics_collector(self):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨"""
        def collect_langgraph_metrics(monitor):
            # æ”¶é›†è¯·æ±‚ç›¸å…³æŒ‡æ ‡
            monitor.record_gauge("langgraph_total_requests", self.request_count)
            monitor.record_gauge("langgraph_total_errors", self.error_count)
            
            # è®¡ç®—é”™è¯¯ç‡
            if self.request_count > 0:
                error_rate = self.error_count / self.request_count * 100
                monitor.record_gauge("langgraph_error_rate_percent", error_rate)
        
        self.monitor.custom_collectors.append(collect_langgraph_metrics)

# æµ‹è¯•å®Œæ•´çš„ç›‘æ§ç³»ç»Ÿ
def test_monitoring_system():
    """æµ‹è¯•å®Œæ•´çš„ç›‘æ§ç³»ç»Ÿ"""
    print("ğŸš€ å¯åŠ¨ç›‘æ§ç³»ç»Ÿæµ‹è¯•")
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = PerformanceMonitor(retention_seconds=300)
    
    # æ·»åŠ å‘Šè­¦è§„åˆ™
    def high_cpu_alert(alert_info):
        print(f"ğŸ”¥ é«˜CPUä½¿ç”¨ç‡å‘Šè­¦: {alert_info['current_value']:.1f}%")
    
    def high_memory_alert(alert_info):
        print(f"ğŸ’¾ é«˜å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦: {alert_info['current_value']:.1f}%")
    
    monitor.add_alert_rule("system_cpu_percent", "gt", 80.0, high_cpu_alert, "CPUä½¿ç”¨ç‡è¿‡é«˜")
    monitor.add_alert_rule("system_memory_percent", "gt", 85.0, high_memory_alert, "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
    monitor.add_alert_rule("langgraph_error_rate_percent", "gt", 5.0, None, "é”™è¯¯ç‡è¿‡é«˜")
    
    # å¯åŠ¨ç›‘æ§
    monitor.start_monitoring(interval=2.0)
    
    try:
        # æ¨¡æ‹Ÿåº”ç”¨è¿è¡Œ
        integration = LangGraphMonitoringIntegration(monitor)
        integration.add_custom_metrics_collector()
        
        # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚
        for i in range(10):
            # æ¨¡æ‹Ÿæ­£å¸¸è¯·æ±‚
            start_time = time.time()
            time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            duration = time.time() - start_time
            
            monitor.record_timer("langgraph_response_time", duration, {"status": "success"})
            monitor.record_counter("langgraph_requests_total", 1)
            integration.request_count += 1
            
            # å¶å°”æ¨¡æ‹Ÿé”™è¯¯
            if i % 7 == 0:
                monitor.record_counter("langgraph_errors_total", 1, {"error_type": "ValidationError"})
                integration.error_count += 1
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´æ”¶é›†æŒ‡æ ‡
        print("ğŸ“Š æ”¶é›†ç›‘æ§æ•°æ®...")
        time.sleep(10)
        
        # è·å–ä»ªè¡¨æ¿æ•°æ®
        dashboard = monitor.get_dashboard_data()
        
        print("\nğŸ¯ ç›‘æ§ä»ªè¡¨æ¿")
        print("="*50)
        print(f"ç³»ç»Ÿè¿è¡Œæ—¶é—´: {dashboard['system_overview']['uptime_human']}")
        print(f"ç›‘æ§æŒ‡æ ‡æ•°é‡: {dashboard['total_metrics']}")
        print(f"å‘Šè­¦è§„åˆ™æ•°é‡: {dashboard['alert_rules_count']}")
        print(f"æ´»è·ƒå‘Šè­¦æ•°é‡: {len(dashboard['active_alerts'])}")
        
        print("\nğŸ“ˆ å…³é”®æŒ‡æ ‡:")
        for metric_name, stats in dashboard['key_metrics'].items():
            print(f"  {metric_name}: å½“å‰={stats['current']:.2f}, "
                  f"æœ€å°={stats['min']:.2f}, æœ€å¤§={stats['max']:.2f}")
        
        # æ˜¾ç¤ºä¸€äº›å…·ä½“ç»Ÿè®¡
        response_time_stats = monitor.get_metric_stats("langgraph_response_time", duration=300)
        if "error" not in response_time_stats:
            print(f"\nâ±ï¸ å“åº”æ—¶é—´ç»Ÿè®¡:")
            print(f"  å¹³å‡: {response_time_stats['mean']:.3f}s")
            print(f"  ä¸­ä½æ•°: {response_time_stats['median']:.3f}s")
            if 'p95' in response_time_stats:
                print(f"  P95: {response_time_stats['p95']:.3f}s")
        
        # å¯¼å‡ºPrometheusæ ¼å¼
        prometheus_data = monitor.export_prometheus_metrics()
        print(f"\nğŸ“¤ PrometheusæŒ‡æ ‡å¯¼å‡º ({len(prometheus_data.splitlines())} è¡Œ):")
        print(prometheus_data[:300] + "..." if len(prometheus_data) > 300 else prometheus_data)
        
    finally:
        monitor.stop_monitoring()

# è¿è¡Œç›‘æ§æµ‹è¯•
if __name__ == "__main__":
    test_monitoring_system()
```

## ğŸš€ æ€§èƒ½ä¼˜åŒ–å®æˆ˜

### æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

```python
import redis
import pickle
import hashlib
import time
import asyncio
from typing import Any, Optional, Callable, Union, Dict
from functools import wraps
import json
import zlib

class IntelligentCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†ç³»ç»Ÿ"""
    
    def __init__(self, redis_url: str = "redis://localhost", 
                 default_ttl: int = 3600, compression_threshold: int = 1024):
        self.redis_url = redis_url
        self.default_ttl = default_ttl
        self.compression_threshold = compression_threshold
        self.redis_client = None
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "errors": 0,
            "compression_saves": 0
        }
        
        # æ™ºèƒ½é¢„åŠ è½½
        self.preload_patterns = {}
        self.access_patterns = {}
    
    def connect(self):
        """è¿æ¥Redis"""
        try:
            self.redis_client = redis.from_url(self.redis_url, decode_responses=False)
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            print("âœ… Redisè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Redisè¿æ¥å¤±è´¥: {e}")
            # ä½¿ç”¨å†…å­˜ç¼“å­˜ä½œä¸ºå¤‡é€‰
            self.redis_client = {}
            print("âš ï¸ ä½¿ç”¨å†…å­˜ç¼“å­˜ä½œä¸ºå¤‡é€‰")
    
    def _make_cache_key(self, key: str, prefix: str = "lg") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # å¯¹é•¿é”®è¿›è¡Œå“ˆå¸Œ
        if len(key) > 100:
            key_hash = hashlib.md5(key.encode()).hexdigest()
            return f"{prefix}:hash:{key_hash}"
        return f"{prefix}:{key}"
    
    def _serialize_value(self, value: Any) -> bytes:
        """åºåˆ—åŒ–å€¼"""
        # åºåˆ—åŒ–
        serialized = pickle.dumps(value)
        
        # å¦‚æœæ•°æ®è¾ƒå¤§ï¼Œè¿›è¡Œå‹ç¼©
        if len(serialized) > self.compression_threshold:
            compressed = zlib.compress(serialized)
            if len(compressed) < len(serialized) * 0.8:  # å‹ç¼©ç‡è¶…è¿‡20%æ‰ä½¿ç”¨
                self.stats["compression_saves"] += len(serialized) - len(compressed)
                return b"COMPRESSED:" + compressed
        
        return serialized
    
    def _deserialize_value(self, data: bytes) -> Any:
        """ååºåˆ—åŒ–å€¼"""
        if data.startswith(b"COMPRESSED:"):
            # è§£å‹ç¼©
            compressed_data = data[11:]  # å»æ‰"COMPRESSED:"å‰ç¼€
            decompressed = zlib.decompress(compressed_data)
            return pickle.loads(decompressed)
        else:
            return pickle.loads(data)
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        cache_key = self._make_cache_key(key)
        
        try:
            if isinstance(self.redis_client, dict):
                # å†…å­˜ç¼“å­˜
                if cache_key in self.redis_client:
                    data, expire_time = self.redis_client[cache_key]
                    if time.time() < expire_time:
                        self.stats["hits"] += 1
                        self._record_access_pattern(key)
                        return self._deserialize_value(data)
                    else:
                        del self.redis_client[cache_key]
            else:
                # Redisç¼“å­˜
                data = self.redis_client.get(cache_key)
                if data is not None:
                    self.stats["hits"] += 1
                    self._record_access_pattern(key)
                    return self._deserialize_value(data)
            
            self.stats["misses"] += 1
            return None
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"âŒ ç¼“å­˜è·å–é”™è¯¯: {e}")
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        cache_key = self._make_cache_key(key)
        ttl = ttl or self.default_ttl
        
        try:
            serialized_value = self._serialize_value(value)
            
            if isinstance(self.redis_client, dict):
                # å†…å­˜ç¼“å­˜
                expire_time = time.time() + ttl
                self.redis_client[cache_key] = (serialized_value, expire_time)
            else:
                # Redisç¼“å­˜
                self.redis_client.setex(cache_key, ttl, serialized_value)
            
            self.stats["sets"] += 1
            return True
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"âŒ ç¼“å­˜è®¾ç½®é”™è¯¯: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        cache_key = self._make_cache_key(key)
        
        try:
            if isinstance(self.redis_client, dict):
                if cache_key in self.redis_client:
                    del self.redis_client[cache_key]
            else:
                self.redis_client.delete(cache_key)
            
            self.stats["deletes"] += 1
            return True
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"âŒ ç¼“å­˜åˆ é™¤é”™è¯¯: {e}")
            return False
    
    def _record_access_pattern(self, key: str):
        """è®°å½•è®¿é—®æ¨¡å¼"""
        current_time = time.time()
        
        if key not in self.access_patterns:
            self.access_patterns[key] = {
                "count": 0,
                "last_access": current_time,
                "avg_interval": 0
            }
        
        pattern = self.access_patterns[key]
        pattern["count"] += 1
        
        if pattern["count"] > 1:
            interval = current_time - pattern["last_access"]
            pattern["avg_interval"] = (pattern["avg_interval"] * (pattern["count"] - 2) + interval) / (pattern["count"] - 1)
        
        pattern["last_access"] = current_time
        
        # é¢„æµ‹ä¸‹æ¬¡è®¿é—®æ—¶é—´
        if pattern["count"] > 3 and pattern["avg_interval"] > 0:
            next_access = current_time + pattern["avg_interval"]
            self.preload_patterns[key] = next_access
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total_requests = self.stats["hits"] + self.stats["misses"]
        if total_requests == 0:
            return 0.0
        return self.stats["hits"] / total_requests
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "hit_rate": self.get_hit_rate(),
            "total_requests": self.stats["hits"] + self.stats["misses"],
            "compression_savings_bytes": self.stats["compression_saves"]
        }

# ç¼“å­˜è£…é¥°å™¨
def cached(cache_manager: IntelligentCacheManager, ttl: Optional[int] = None, 
          key_func: Optional[Callable] = None):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # é»˜è®¤é”®ç”Ÿæˆç­–ç•¥
                args_str = str(args) + str(sorted(kwargs.items()))
                cache_key = f"{func.__name__}:{hashlib.md5(args_str.encode()).hexdigest()}"
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        # å¼‚æ­¥ç‰ˆæœ¬
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                args_str = str(args) + str(sorted(kwargs.items()))
                cache_key = f"{func.__name__}:{hashlib.md5(args_str.encode()).hexdigest()}"
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)
            
            # å­˜å‚¨åˆ°ç¼“å­˜
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        # æ ¹æ®å‡½æ•°ç±»å‹è¿”å›å¯¹åº”çš„åŒ…è£…å™¨
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return wrapper
    
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
cache_manager = IntelligentCacheManager()
cache_manager.connect()

@cached(cache_manager, ttl=300)
def expensive_computation(n: int) -> int:
    """æ˜‚è´µçš„è®¡ç®—æ“ä½œ"""
    print(f"ğŸ”¢ æ‰§è¡Œæ˜‚è´µè®¡ç®—: n={n}")
    time.sleep(1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
    return sum(i * i for i in range(n))

@cached(cache_manager, ttl=600)
async def async_api_call(endpoint: str, params: dict) -> dict:
    """å¼‚æ­¥APIè°ƒç”¨"""
    print(f"ğŸŒ æ‰§è¡ŒAPIè°ƒç”¨: {endpoint}")
    await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    return {"endpoint": endpoint, "params": params, "result": "mock_data"}

# æµ‹è¯•ç¼“å­˜æ€§èƒ½
def test_cache_performance():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    print("ğŸš€ å¼€å§‹ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    
    # é‡ç½®ç»Ÿè®¡
    cache_manager.stats = {k: 0 for k in cache_manager.stats}
    
    # æµ‹è¯•åŒæ­¥ç¼“å­˜
    print("\nğŸ“Š åŒæ­¥ç¼“å­˜æµ‹è¯•:")
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
    start_time = time.time()
    result1 = expensive_computation(1000)
    first_call_time = time.time() - start_time
    print(f"   ç¬¬ä¸€æ¬¡è°ƒç”¨: {first_call_time:.3f}s (æœªå‘½ä¸­)")
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
    start_time = time.time()
    result2 = expensive_computation(1000)
    second_call_time = time.time() - start_time
    print(f"   ç¬¬äºŒæ¬¡è°ƒç”¨: {second_call_time:.3f}s (å‘½ä¸­)")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    assert result1 == result2, "ç¼“å­˜ç»“æœä¸ä¸€è‡´"
    
    speedup = first_call_time / second_call_time if second_call_time > 0 else float('inf')
    print(f"   æ€§èƒ½æå‡: {speedup:.1f}x")
    
    # æµ‹è¯•å¼‚æ­¥ç¼“å­˜
    async def test_async_cache():
        print("\nğŸŒ å¼‚æ­¥ç¼“å­˜æµ‹è¯•:")
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        start_time = time.time()
        result1 = await async_api_call("https://api.example.com/data", {"page": 1})
        first_call_time = time.time() - start_time
        print(f"   ç¬¬ä¸€æ¬¡è°ƒç”¨: {first_call_time:.3f}s (æœªå‘½ä¸­)")
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨
        start_time = time.time()
        result2 = await async_api_call("https://api.example.com/data", {"page": 1})
        second_call_time = time.time() - start_time
        print(f"   ç¬¬äºŒæ¬¡è°ƒç”¨: {second_call_time:.3f}s (å‘½ä¸­)")
        
        speedup = first_call_time / second_call_time if second_call_time > 0 else float('inf')
        print(f"   æ€§èƒ½æå‡: {speedup:.1f}x")
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    asyncio.run(test_async_cache())
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = cache_manager.get_stats()
    print(f"\nğŸ“ˆ ç¼“å­˜ç»Ÿè®¡:")
    print(f"   å‘½ä¸­ç‡: {stats['hit_rate']:.1%}")
    print(f"   æ€»è¯·æ±‚: {stats['total_requests']}")
    print(f"   å‘½ä¸­: {stats['hits']}")
    print(f"   æœªå‘½ä¸­: {stats['misses']}")
    print(f"   è®¾ç½®: {stats['sets']}")
    print(f"   å‹ç¼©èŠ‚çœ: {stats['compression_savings_bytes']} å­—èŠ‚")

# è¿è¡Œç¼“å­˜æµ‹è¯•
if __name__ == "__main__":
    test_cache_performance()
```

## ğŸ¯ ç»¼åˆå®æˆ˜ï¼šä¼ä¸šçº§AIå®¢æœä¸­å¿ƒ

è®©æˆ‘ä»¬æŠŠæ‰€æœ‰å­¦åˆ°çš„æŠ€æœ¯æ•´åˆåˆ°ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§ç³»ç»Ÿä¸­ï¼š

```python
# è¿™é‡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§AIå®¢æœä¸­å¿ƒå®ç°
# ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæˆ‘å°†æä¾›å…³é”®æ¶æ„å’Œæ ¸å¿ƒä»£ç 

class EnterpriseAICustomerService:
    """ä¼ä¸šçº§AIå®¢æœä¸­å¿ƒ"""
    
    def __init__(self):
        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.checkpointer = self._setup_persistence()
        self.cache_manager = self._setup_cache()
        self.monitor = self._setup_monitoring()
        self.concurrency_manager = self._setup_concurrency()
        
        # LangGraphåº”ç”¨
        self.app = self._build_langgraph_app()
        
        # æ€§èƒ½æŒ‡æ ‡
        self.performance_targets = {
            "response_time_p95": 2.0,  # 95%è¯·æ±‚<2ç§’
            "availability": 99.9,      # 99.9%å¯ç”¨æ€§
            "concurrent_users": 10000,  # æ”¯æŒ1ä¸‡å¹¶å‘
            "error_rate": 0.1          # é”™è¯¯ç‡<0.1%
        }
    
    def _setup_persistence(self):
        """é…ç½®ä¼ä¸šçº§æŒä¹…åŒ–"""
        return EnterpriseCheckpointer({
            "host": "postgres-cluster.internal",
            "port": 5432,
            "database": "ai_customer_service",
            "user": "service_user",
            "password": "secure_password",
            "pool_size": 100
        })
    
    def _setup_cache(self):
        """é…ç½®æ™ºèƒ½ç¼“å­˜"""
        return IntelligentCacheManager(
            redis_url="redis://redis-cluster.internal:6379",
            default_ttl=1800,  # 30åˆ†é’Ÿ
            compression_threshold=2048
        )
    
    def _setup_monitoring(self):
        """é…ç½®ç›‘æ§ç³»ç»Ÿ"""
        monitor = PerformanceMonitor(retention_seconds=86400)  # 24å°æ—¶
        
        # å…³é”®å‘Šè­¦è§„åˆ™
        monitor.add_alert_rule("response_time_p95", "gt", 2.0, self._handle_slow_response)
        monitor.add_alert_rule("error_rate", "gt", 0.1, self._handle_high_error_rate)
        monitor.add_alert_rule("concurrent_users", "gt", 12000, self._handle_high_load)
        
        return monitor
    
    def _setup_concurrency(self):
        """é…ç½®å¹¶å‘ç®¡ç†"""
        return EnterpriseConcurrencyManager(max_concurrent=15000)
    
    def _build_langgraph_app(self):
        """æ„å»ºLangGraphåº”ç”¨"""
        # è¿™é‡Œä¼šåŒ…å«å®Œæ•´çš„å¤šä»£ç†å®¢æœç³»ç»Ÿ
        # åŒ…å«ï¼šæ„å›¾è¯†åˆ«ã€çŸ¥è¯†æ£€ç´¢ã€å›å¤ç”Ÿæˆã€è´¨é‡æ£€æŸ¥ç­‰ä»£ç†
        pass
    
    async def start(self):
        """å¯åŠ¨ä¼ä¸šçº§å®¢æœç³»ç»Ÿ"""
        print("ğŸš€ å¯åŠ¨ä¼ä¸šçº§AIå®¢æœä¸­å¿ƒ...")
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        await self.checkpointer.initialize()
        self.cache_manager.connect()
        self.monitor.start_monitoring()
        await self.concurrency_manager.initialize()
        
        # é¢„çƒ­ç³»ç»Ÿ
        await self._warmup_system()
        
        print("âœ… ä¼ä¸šçº§AIå®¢æœä¸­å¿ƒå¯åŠ¨å®Œæˆ")
        print(f"ğŸ“Š æ€§èƒ½ç›®æ ‡: {self.performance_targets}")
    
    async def _warmup_system(self):
        """ç³»ç»Ÿé¢„çƒ­"""
        print("ğŸ”¥ ç³»ç»Ÿé¢„çƒ­ä¸­...")
        
        # é¢„åŠ è½½å¸¸ç”¨ç¼“å­˜
        common_queries = [
            "è´¦æˆ·ä½™é¢æŸ¥è¯¢",
            "å¯†ç é‡ç½®",
            "äº§å“ä»·æ ¼å’¨è¯¢",
            "æŠ€æœ¯æ”¯æŒ",
            "é€€æ¬¾ç”³è¯·"
        ]
        
        for query in common_queries:
            # é¢„åŠ è½½AIæ¨¡å‹å“åº”
            await self._preload_ai_response(query)
        
        print("âœ… ç³»ç»Ÿé¢„çƒ­å®Œæˆ")

# æ€§èƒ½åŸºå‡†æµ‹è¯•
async def run_enterprise_benchmark():
    """è¿è¡Œä¼ä¸šçº§æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸ¯ ä¼ä¸šçº§AIå®¢æœä¸­å¿ƒåŸºå‡†æµ‹è¯•")
    print("="*60)
    
    service = EnterpriseAICustomerService()
    await service.start()
    
    try:
        # æ¨¡æ‹Ÿç”Ÿäº§è´Ÿè½½
        await simulate_production_load(service)
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        await generate_performance_report(service)
        
    finally:
        await service.shutdown()

# è¿™åªæ˜¯å®Œæ•´å®ç°çš„æ¡†æ¶
# å®Œæ•´ç‰ˆæœ¬ä¼šåŒ…å«æ‰€æœ‰å‰é¢ç« èŠ‚å­¦åˆ°çš„æŠ€æœ¯
```

## ğŸ“ å­¦ä¹ æ£€éªŒæ¸…å•

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

**åŸºç¡€èƒ½åŠ›**ï¼š
- [ ] é…ç½®å’Œä½¿ç”¨LangGraphçš„çŠ¶æ€æŒä¹…åŒ–åŠŸèƒ½
- [ ] å®ç°åŸºæœ¬çš„å¹¶å‘æ§åˆ¶å’Œèµ„æºç®¡ç†
- [ ] æ­å»ºåŸºç¡€çš„æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
- [ ] åº”ç”¨åŸºæœ¬çš„ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

**è¿›é˜¶èƒ½åŠ›**ï¼š
- [ ] è®¾è®¡ä¼ä¸šçº§çš„çŠ¶æ€ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿ
- [ ] å®ç°é«˜æ€§èƒ½çš„å¼‚æ­¥å¹¶å‘å¤„ç†
- [ ] æ„å»ºå®Œæ•´çš„å¯è§‚æµ‹æ€§ç³»ç»Ÿï¼ˆæŒ‡æ ‡ã€æ—¥å¿—ã€é“¾è·¯è¿½è¸ªï¼‰
- [ ] ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½å¹¶è¿›è¡Œå®¹é‡è§„åˆ’

**ä¸“å®¶èƒ½åŠ›**ï¼š
- [ ] è®¾è®¡å¤§è§„æ¨¡åˆ†å¸ƒå¼LangGraphç³»ç»Ÿæ¶æ„
- [ ] å®ç°è‡ªåŠ¨åŒ–çš„æ€§èƒ½ä¼˜åŒ–å’Œæ•…éšœæ¢å¤
- [ ] å»ºç«‹å®Œæ•´çš„SREå®è·µä½“ç³»
- [ ] æŒ‡å¯¼å›¢é˜Ÿè¿›è¡Œä¼ä¸šçº§ç³»ç»Ÿè®¾è®¡

## ğŸš€ ä¸‹ä¸€æ­¥å±•æœ›

**L5å±‚é¢„å‘Š - ä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´**ï¼š
- ğŸ—ï¸ **å®¹å™¨åŒ–éƒ¨ç½²**: Dockerå®¹å™¨åŒ–å’ŒKubernetesç¼–æ’
- â˜ï¸ **äº‘åŸç”Ÿæ¶æ„**: äº‘å¹³å°é›†æˆå’Œæ— æœåŠ¡å™¨éƒ¨ç½²
- ğŸ”„ **CI/CDæµæ°´çº¿**: è‡ªåŠ¨åŒ–æµ‹è¯•ã€æ„å»ºå’Œéƒ¨ç½²
- ğŸ“Š **è¿ç»´ç›‘æ§**: å®Œæ•´çš„è¿ç»´å·¥å…·é“¾å’Œæœ€ä½³å®è·µ
- ğŸ›¡ï¸ **å®‰å…¨åˆè§„**: ä¼ä¸šçº§å®‰å…¨ç­–ç•¥å’Œåˆè§„è¦æ±‚

---

**ğŸ¯ æ­å–œï¼** ä½ å·²ç»æŒæ¡äº†æ„å»ºä¼ä¸šçº§LangGraphç³»ç»Ÿçš„æ ¸å¿ƒæŠ€èƒ½ã€‚ç°åœ¨ä½ å¯ä»¥è®¾è®¡å’Œå®ç°æ”¯æŒä¸‡çº§å¹¶å‘ã€ç§’çº§æ¢å¤çš„ç”Ÿäº§çº§AIç³»ç»Ÿï¼

**ğŸ‘‰ ä¸‹ä¸€æ­¥**: [L5: ä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´](./05-ä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´.md) - å°†ä½ çš„ç³»ç»Ÿéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼