# L4: 高级特性与性能优化

**学习目标**: 构建企业级LangGraph系统，实现生产环境的高性能和高可靠性  
**预计用时**: 5-6小时  
**核心转变**: 从"功能实现"思维 → "性能工程"思维

*💡 这一章将带你从原型开发跨越到企业级系统设计。你将学会如何让LangGraph系统支持万级并发、秒级恢复、实时监控，以及如何进行系统性的性能优化。*

---

## 🌟 开篇：企业级系统的神奇力量

### 令人震撼的性能表现

想象这样一个AI客服系统：

```python
# 🚀 企业级AI客服中心实时数据
"""
📊 系统状态面板
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
🔥 在线用户: 12,847 人
⚡ 当前QPS: 1,247 请求/秒  
💾 平均响应时间: 0.89 秒
🎯 成功率: 99.97%
🔄 会话恢复率: 100% (即使服务器重启)
💡 缓存命中率: 87.3%
📈 CPU使用率: 45% | 内存使用: 2.1GB/8GB
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

⚠️  告警日志
[2024-01-15 14:23:15] 服务器node-03重启完成，1,245个会话无缝恢复 ✅
[2024-01-15 14:20:33] 检测到流量激增，自动扩容+2实例 🔄
[2024-01-15 14:18:07] 缓存命中率下降到82%，自动优化缓存策略 🎯
"""
```

**这种工业级的强大能力是如何实现的？** 🤔

- 🏗️ **状态持久化**：即使系统重启，所有用户会话都能无缝恢复
- ⚡ **高性能并发**：单机支持1000+并发，集群支持无限扩展
- 📊 **实时监控**：毫秒级的性能监控和智能告警
- 🧠 **智能优化**：自动缓存、资源调度、性能调优
- 🛡️ **故障自愈**：秒级故障检测和自动恢复

这就是企业级LangGraph系统的真实能力！

### 从原型到生产的技术鸿沟

**原型系统的局限**：
```python
# 原型阶段：能跑就行
app = StateGraph(MyState)
app.add_node("process", simple_process)
result = app.invoke(input_data)  # 单次调用，无状态保存
```

**企业级系统的要求**：
```python
# 生产环境：必须考虑所有边界情况
app = StateGraph(MyState)
app.add_node("process", enterprise_process)

# 配置持久化
checkpointer = PostgreSQLCheckpointer(connection_string)
app = app.compile(checkpointer=checkpointer)

# 并发控制
executor = ConcurrentExecutor(max_workers=100, queue_size=1000)

# 性能监控
monitor = PerformanceMonitor(metrics_backend="prometheus")

# 缓存优化
cache = RedisCache(cluster_nodes=["redis-1", "redis-2", "redis-3"])

# 故障恢复
circuit_breaker = CircuitBreaker(failure_threshold=0.1, recovery_timeout=30)
```

**技术挑战的量级差异**：

| 维度 | 原型系统 | 企业级系统 | 复杂度提升 |
|------|----------|------------|------------|
| **并发量** | 1-10个用户 | 10,000+用户 | 1000倍 |
| **可用性** | 95%（偶尔崩溃） | 99.9%（年停机<9小时） | 50倍改进 |
| **响应时间** | 5-10秒 | <1秒 | 10倍提升 |
| **数据量** | MB级 | TB级 | 1000倍 |
| **复杂度** | 单功能 | 多模块协作 | 指数级 |

## 🏗️ 状态持久化：永不丢失的记忆

### 为什么需要状态持久化？

**业务场景需求**：
- 💬 **长期对话**：用户可能花几小时解决复杂问题
- 🔄 **系统维护**：服务更新时不能丢失用户会话
- 📱 **跨设备**：用户在手机开始，电脑继续
- 🛡️ **容灾恢复**：硬件故障时的快速恢复
- 📊 **审计追踪**：完整记录决策过程用于合规

### 基础持久化：从内存到磁盘

**第一步：配置SQLite持久化**

```python
import sqlite3
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import StateGraph, END
from typing_extensions import TypedDict, NotRequired
import time
import threading

# 1. 设计支持持久化的状态结构
class PersistentChatState(TypedDict):
    # 核心业务数据
    user_id: str
    session_id: str
    conversation_history: NotRequired[list]
    current_context: NotRequired[str]
    
    # 持久化元数据
    created_at: NotRequired[str]
    last_updated: NotRequired[str]
    checkpoint_version: NotRequired[int]
    
    # 业务状态跟踪
    current_intent: NotRequired[str]
    satisfaction_score: NotRequired[float]
    escalation_needed: NotRequired[bool]

def intelligent_chat_agent(state: PersistentChatState):
    """智能聊天代理"""
    user_id = state["user_id"]
    history = state.get("conversation_history", [])
    
    print(f"💬 为用户 {user_id} 处理消息 (历史记录: {len(history)} 条)")
    
    # 模拟复杂的AI处理逻辑
    time.sleep(0.5)  # 模拟AI思考时间
    
    # 生成回复
    response = f"基于您的 {len(history)} 条历史对话，我理解您的需求..."
    
    # 更新状态
    updated_history = history + [
        {"role": "user", "content": "用户输入..."},
        {"role": "assistant", "content": response}
    ]
    
    return {
        "conversation_history": updated_history,
        "last_updated": str(int(time.time())),
        "checkpoint_version": state.get("checkpoint_version", 0) + 1,
        "current_context": "用户咨询产品功能"
    }

def satisfaction_checker(state: PersistentChatState):
    """满意度检查"""
    history = state.get("conversation_history", [])
    
    # 模拟满意度分析
    satisfaction = min(0.9, 0.5 + len(history) * 0.1)
    
    print(f"📊 满意度评估: {satisfaction:.2f}")
    
    return {
        "satisfaction_score": satisfaction,
        "escalation_needed": satisfaction < 0.7
    }

# 2. 创建持久化的聊天应用
def create_persistent_chat_app():
    """创建具有持久化功能的聊天应用"""
    
    # 配置SQLite检查点保存器
    db_path = "chat_sessions.db"
    checkpointer = SqliteSaver.from_conn_string(f"sqlite:///{db_path}")
    
    # 构建图
    workflow = StateGraph(PersistentChatState)
    workflow.add_node("chat_agent", intelligent_chat_agent)
    workflow.add_node("satisfaction_check", satisfaction_checker)
    
    # 设置流程
    workflow.set_entry_point("chat_agent")
    workflow.add_edge("chat_agent", "satisfaction_check")
    workflow.add_edge("satisfaction_check", END)
    
    # 编译并配置持久化
    app = workflow.compile(checkpointer=checkpointer)
    
    return app

# 3. 测试持久化功能
def test_persistence():
    """测试状态持久化功能"""
    app = create_persistent_chat_app()
    
    # 会话配置
    config = {
        "configurable": {
            "thread_id": "user_12345_session"  # 唯一的会话标识
        }
    }
    
    print("🚀 开始第一轮对话...")
    
    # 第一轮对话
    result1 = app.invoke({
        "user_id": "user_12345",
        "session_id": "session_001",
        "created_at": str(int(time.time()))
    }, config=config)
    
    print(f"✅ 第一轮完成，对话历史: {len(result1['conversation_history'])} 条")
    print(f"📊 满意度: {result1['satisfaction_score']:.2f}")
    
    # 模拟系统重启（重新创建应用实例）
    print("\n🔄 模拟系统重启...")
    time.sleep(1)
    
    # 重新创建应用（模拟重启后）
    app_after_restart = create_persistent_chat_app()
    
    print("🔄 从检查点恢复对话...")
    
    # 继续对话（状态自动从检查点恢复）
    result2 = app_after_restart.invoke({
        "user_id": "user_12345",
        "session_id": "session_001"  # 相同的session_id
    }, config=config)
    
    print(f"✅ 恢复成功！历史记录: {len(result2['conversation_history'])} 条")
    print(f"📊 检查点版本: {result2['checkpoint_version']}")
    print(f"🕐 最后更新: {result2['last_updated']}")
    
    return result2

# 运行测试
if __name__ == "__main__":
    result = test_persistence()
    
    print("\n" + "="*50)
    print("🎯 持久化测试结果")
    print("="*50)
    print(f"用户ID: {result['user_id']}")
    print(f"会话ID: {result['session_id']}")
    print(f"对话轮数: {len(result['conversation_history'])} 条")
    print(f"满意度: {result['satisfaction_score']:.2f}")
    print(f"检查点版本: {result['checkpoint_version']}")
    print(f"系统重启后恢复: ✅ 成功")
```

### 🔍 深入理解：持久化的核心机制

**1. 检查点的工作原理**：
```python
# 每次状态更新时，LangGraph自动保存检查点
state_update = {
    "conversation_history": [...],
    "satisfaction_score": 0.8
}

# 内部过程：
# 1. 序列化状态数据
# 2. 生成唯一的检查点ID
# 3. 保存到SQLite数据库
# 4. 更新检查点索引
```

**2. 会话恢复的流程**：
```python
# 应用重启后首次调用时：
# 1. 根据thread_id查询最新检查点
# 2. 反序列化状态数据
# 3. 从检查点状态继续执行
# 4. 新的状态更新继续保存
```

### 高级持久化：分布式和版本管理

**PostgreSQL集群持久化**：

```python
from langgraph.checkpoint.postgres import PostgresCheckpointer
import asyncpg
import asyncio
from typing import Dict, Any
import json

class EnterpriseCheckpointer:
    """企业级检查点管理器"""
    
    def __init__(self, postgres_config: Dict[str, Any]):
        self.config = postgres_config
        self.connection_pool = None
        self.version_history_limit = 100  # 保留历史版本数量
    
    async def initialize(self):
        """初始化连接池"""
        self.connection_pool = await asyncpg.create_pool(
            **self.config,
            min_size=10,
            max_size=100,
            command_timeout=60
        )
        
        # 创建版本管理表
        await self._create_version_tables()
    
    async def _create_version_tables(self):
        """创建版本管理相关表"""
        async with self.connection_pool.acquire() as conn:
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS checkpoint_versions (
                    id SERIAL PRIMARY KEY,
                    thread_id VARCHAR(255) NOT NULL,
                    checkpoint_id VARCHAR(255) NOT NULL,
                    version_number INTEGER NOT NULL,
                    state_data JSONB NOT NULL,
                    metadata JSONB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(thread_id, version_number)
                );
                
                CREATE INDEX IF NOT EXISTS idx_thread_version 
                ON checkpoint_versions(thread_id, version_number DESC);
            """)
    
    async def save_versioned_checkpoint(self, thread_id: str, state: Dict[str, Any], 
                                      metadata: Dict[str, Any] = None):
        """保存带版本的检查点"""
        async with self.connection_pool.acquire() as conn:
            # 获取下一个版本号
            version_result = await conn.fetchrow("""
                SELECT COALESCE(MAX(version_number), 0) + 1 as next_version
                FROM checkpoint_versions 
                WHERE thread_id = $1
            """, thread_id)
            
            next_version = version_result['next_version']
            
            # 保存新版本
            await conn.execute("""
                INSERT INTO checkpoint_versions 
                (thread_id, checkpoint_id, version_number, state_data, metadata)
                VALUES ($1, $2, $3, $4, $5)
            """, 
                thread_id,
                f"{thread_id}_v{next_version}",
                next_version,
                json.dumps(state),
                json.dumps(metadata or {})
            )
            
            # 清理旧版本
            await self._cleanup_old_versions(conn, thread_id)
            
            return next_version
    
    async def _cleanup_old_versions(self, conn, thread_id: str):
        """清理旧版本"""
        await conn.execute("""
            DELETE FROM checkpoint_versions 
            WHERE thread_id = $1 
            AND version_number <= (
                SELECT version_number 
                FROM checkpoint_versions 
                WHERE thread_id = $1 
                ORDER BY version_number DESC 
                OFFSET $2 LIMIT 1
            )
        """, thread_id, self.version_history_limit)
    
    async def rollback_to_version(self, thread_id: str, target_version: int) -> Dict[str, Any]:
        """回滚到指定版本"""
        async with self.connection_pool.acquire() as conn:
            result = await conn.fetchrow("""
                SELECT state_data, metadata
                FROM checkpoint_versions
                WHERE thread_id = $1 AND version_number = $2
            """, thread_id, target_version)
            
            if not result:
                raise ValueError(f"版本 {target_version} 不存在")
            
            # 创建新的版本作为回滚记录
            rollback_state = json.loads(result['state_data'])
            rollback_metadata = json.loads(result['metadata'])
            rollback_metadata['rollback_from'] = target_version
            
            new_version = await self.save_versioned_checkpoint(
                thread_id, rollback_state, rollback_metadata
            )
            
            return {
                "state": rollback_state,
                "version": new_version,
                "rollback_target": target_version
            }
    
    async def get_version_history(self, thread_id: str, limit: int = 10) -> list:
        """获取版本历史"""
        async with self.connection_pool.acquire() as conn:
            results = await conn.fetch("""
                SELECT version_number, created_at, 
                       jsonb_extract_path_text(metadata, 'description') as description
                FROM checkpoint_versions
                WHERE thread_id = $1
                ORDER BY version_number DESC
                LIMIT $2
            """, thread_id, limit)
            
            return [dict(row) for row in results]

# 使用示例
async def test_advanced_persistence():
    """测试高级持久化功能"""
    
    # 配置企业级检查点管理器
    checkpointer = EnterpriseCheckpointer({
        "host": "localhost",
        "port": 5432,
        "database": "langgraph_enterprise",
        "user": "langgraph_user",
        "password": "secure_password"
    })
    
    await checkpointer.initialize()
    
    thread_id = "enterprise_session_001"
    
    # 保存多个版本
    for i in range(5):
        state = {
            "step": i,
            "data": f"处理步骤 {i}",
            "timestamp": time.time()
        }
        
        metadata = {
            "description": f"处理步骤 {i} 完成",
            "user_action": f"action_{i}"
        }
        
        version = await checkpointer.save_versioned_checkpoint(
            thread_id, state, metadata
        )
        print(f"✅ 保存版本 {version}: {state['data']}")
    
    # 查看版本历史
    print("\n📋 版本历史:")
    history = await checkpointer.get_version_history(thread_id)
    for record in history:
        print(f"  版本 {record['version_number']}: {record['description']} ({record['created_at']})")
    
    # 回滚测试
    print("\n🔄 回滚到版本 2...")
    rollback_result = await checkpointer.rollback_to_version(thread_id, 2)
    print(f"✅ 回滚成功，新版本: {rollback_result['version']}")
    print(f"📄 恢复的状态: {rollback_result['state']}")

# 运行高级持久化测试
# asyncio.run(test_advanced_persistence())
```

## ⚡ 高性能并发控制

### 并发挑战：从单用户到万用户

**性能对比实验**：

```python
import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
from queue import Queue
import psutil
import statistics

class PerformanceBenchmark:
    """性能基准测试工具"""
    
    def __init__(self):
        self.results = []
        self.start_time = None
        self.end_time = None
    
    def start_benchmark(self, name: str):
        """开始基准测试"""
        self.test_name = name
        self.start_time = time.time()
        self.results = []
        print(f"🚀 开始测试: {name}")
    
    def record_request(self, response_time: float, success: bool = True):
        """记录请求结果"""
        self.results.append({
            "response_time": response_time,
            "success": success,
            "timestamp": time.time()
        })
    
    def finish_benchmark(self):
        """完成基准测试并生成报告"""
        self.end_time = time.time()
        
        if not self.results:
            print("❌ 没有测试数据")
            return
        
        # 计算统计数据
        response_times = [r["response_time"] for r in self.results if r["success"]]
        success_count = sum(1 for r in self.results if r["success"])
        total_requests = len(self.results)
        
        duration = self.end_time - self.start_time
        qps = total_requests / duration
        success_rate = success_count / total_requests * 100
        
        print(f"\n📊 {self.test_name} 测试报告")
        print("="*50)
        print(f"总请求数: {total_requests}")
        print(f"成功请求: {success_count}")
        print(f"成功率: {success_rate:.2f}%")
        print(f"测试时长: {duration:.2f}秒")
        print(f"QPS: {qps:.2f}")
        
        if response_times:
            print(f"平均响应时间: {statistics.mean(response_times):.3f}秒")
            print(f"中位数响应时间: {statistics.median(response_times):.3f}秒")
            print(f"95分位响应时间: {statistics.quantiles(response_times, n=20)[18]:.3f}秒")
            print(f"最大响应时间: {max(response_times):.3f}秒")
        
        return {
            "qps": qps,
            "success_rate": success_rate,
            "avg_response_time": statistics.mean(response_times) if response_times else 0,
            "p95_response_time": statistics.quantiles(response_times, n=20)[18] if response_times else 0
        }

# 1. 基础版本：单线程同步处理
def sync_chat_processor(user_input: str, session_id: str) -> dict:
    """同步聊天处理器"""
    start_time = time.time()
    
    # 模拟AI处理：I/O密集 + CPU密集
    time.sleep(0.1)  # 模拟数据库查询
    time.sleep(0.05)  # 模拟AI推理
    time.sleep(0.02)  # 模拟响应生成
    
    response_time = time.time() - start_time
    
    return {
        "session_id": session_id,
        "response": f"处理完成: {user_input[:20]}...",
        "response_time": response_time,
        "thread_id": threading.current_thread().name
    }

def test_sync_performance():
    """测试同步处理性能"""
    benchmark = PerformanceBenchmark()
    benchmark.start_benchmark("单线程同步处理")
    
    # 模拟100个用户请求
    for i in range(100):
        start = time.time()
        result = sync_chat_processor(f"用户消息 {i}", f"session_{i}")
        end = time.time()
        
        benchmark.record_request(end - start)
        
        if i % 20 == 0:
            print(f"  完成 {i+1}/100 请求...")
    
    return benchmark.finish_benchmark()

# 2. 多线程版本：线程池并发处理
async def async_chat_processor(user_input: str, session_id: str) -> dict:
    """异步聊天处理器"""
    start_time = time.time()
    
    # 使用asyncio模拟异步I/O
    await asyncio.sleep(0.1)   # 异步数据库查询
    await asyncio.sleep(0.05)  # 异步AI推理
    await asyncio.sleep(0.02)  # 异步响应生成
    
    response_time = time.time() - start_time
    
    return {
        "session_id": session_id,
        "response": f"异步处理完成: {user_input[:20]}...",
        "response_time": response_time,
        "task_id": asyncio.current_task().get_name() if asyncio.current_task() else "unknown"
    }

async def test_async_performance():
    """测试异步处理性能"""
    benchmark = PerformanceBenchmark()
    benchmark.start_benchmark("异步并发处理")
    
    # 创建100个并发任务
    tasks = []
    for i in range(100):
        task = async_chat_processor(f"用户消息 {i}", f"session_{i}")
        tasks.append(task)
    
    # 并发执行所有任务
    start_time = time.time()
    results = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()
    
    # 记录结果
    for result in results:
        if isinstance(result, dict):
            benchmark.record_request(result["response_time"])
        else:
            benchmark.record_request(0, success=False)
    
    return benchmark.finish_benchmark()

# 3. 高级版本：连接池 + 资源管理
class ResourceManagedProcessor:
    """资源管理的处理器"""
    
    def __init__(self, max_workers: int = 50, queue_size: int = 1000):
        self.max_workers = max_workers
        self.queue_size = queue_size
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        self.request_queue = Queue(maxsize=queue_size)
        self.active_sessions = {}
        self.session_lock = threading.Lock()
        
        # 连接池模拟
        self.db_connections = Queue(maxsize=10)
        for i in range(10):
            self.db_connections.put(f"db_conn_{i}")
    
    def get_db_connection(self):
        """获取数据库连接"""
        try:
            return self.db_connections.get(timeout=1.0)
        except:
            raise Exception("数据库连接池已满")
    
    def release_db_connection(self, conn):
        """释放数据库连接"""
        try:
            self.db_connections.put(conn, timeout=0.1)
        except:
            pass  # 连接池已满，丢弃连接
    
    async def process_with_resource_management(self, user_input: str, session_id: str) -> dict:
        """使用资源管理的处理方法"""
        start_time = time.time()
        
        # 获取数据库连接
        db_conn = None
        try:
            db_conn = self.get_db_connection()
            
            # 会话状态管理
            with self.session_lock:
                if session_id not in self.active_sessions:
                    self.active_sessions[session_id] = {
                        "created_at": time.time(),
                        "request_count": 0
                    }
                
                self.active_sessions[session_id]["request_count"] += 1
                session_info = self.active_sessions[session_id].copy()
            
            # 模拟处理
            await asyncio.sleep(0.08)  # 比基础版本更快的处理
            
            response_time = time.time() - start_time
            
            return {
                "session_id": session_id,
                "response": f"资源管理处理: {user_input[:20]}...",
                "response_time": response_time,
                "session_requests": session_info["request_count"],
                "db_connection": db_conn
            }
            
        finally:
            # 确保释放资源
            if db_conn:
                self.release_db_connection(db_conn)

async def test_resource_managed_performance():
    """测试资源管理版本性能"""
    benchmark = PerformanceBenchmark()
    benchmark.start_benchmark("资源管理并发处理")
    
    processor = ResourceManagedProcessor(max_workers=50, queue_size=1000)
    
    # 创建并发任务
    tasks = []
    for i in range(200):  # 测试更多请求
        task = processor.process_with_resource_management(
            f"用户消息 {i}", f"session_{i % 50}"  # 50个会话，每个4个请求
        )
        tasks.append(task)
    
    # 并发执行
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # 记录结果
    for result in results:
        if isinstance(result, dict):
            benchmark.record_request(result["response_time"])
        else:
            benchmark.record_request(0, success=False)
    
    return benchmark.finish_benchmark()

# 性能对比测试
async def run_performance_comparison():
    """运行完整的性能对比测试"""
    print("🎯 LangGraph 性能基准测试")
    print("="*60)
    
    # 测试系统资源
    print(f"💻 系统信息:")
    print(f"   CPU核心数: {psutil.cpu_count()}")
    print(f"   内存总量: {psutil.virtual_memory().total / 1024**3:.1f} GB")
    print(f"   可用内存: {psutil.virtual_memory().available / 1024**3:.1f} GB")
    
    results = {}
    
    # 1. 同步处理测试
    print(f"\n{'='*20} 第1轮测试 {'='*20}")
    results["sync"] = test_sync_performance()
    
    # 2. 异步处理测试
    print(f"\n{'='*20} 第2轮测试 {'='*20}")
    results["async"] = await test_async_performance()
    
    # 3. 资源管理测试
    print(f"\n{'='*20} 第3轮测试 {'='*20}")
    results["resource_managed"] = await test_resource_managed_performance()
    
    # 生成对比报告
    print(f"\n🏆 性能对比总结")
    print("="*60)
    print(f"{'模式':<15} {'QPS':<10} {'成功率':<8} {'平均响应':<10} {'P95响应':<10}")
    print("-"*60)
    
    for mode, result in results.items():
        print(f"{mode:<15} {result['qps']:<10.1f} {result['success_rate']:<8.1f}% "
              f"{result['avg_response_time']:<10.3f} {result['p95_response_time']:<10.3f}")
    
    # 性能提升计算
    if "sync" in results and "async" in results:
        qps_improvement = results["async"]["qps"] / results["sync"]["qps"]
        print(f"\n📈 异步模式 QPS 提升: {qps_improvement:.1f}x")
    
    if "async" in results and "resource_managed" in results:
        qps_improvement = results["resource_managed"]["qps"] / results["async"]["qps"]
        print(f"📈 资源管理模式进一步提升: {qps_improvement:.1f}x")

# 运行性能测试
if __name__ == "__main__":
    asyncio.run(run_performance_comparison())
```

### 企业级并发架构

```python
import asyncio
import aioredis
from dataclasses import dataclass
from typing import Optional, Dict, Any, List
import uuid
import json
import time
from enum import Enum

class TaskStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"

@dataclass
class TaskInfo:
    task_id: str
    session_id: str
    user_id: str
    task_type: str
    payload: Dict[str, Any]
    status: TaskStatus
    created_at: float
    started_at: Optional[float] = None
    completed_at: Optional[float] = None
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class EnterpriseConcurrencyManager:
    """企业级并发管理器"""
    
    def __init__(self, redis_url: str = "redis://localhost", max_concurrent: int = 1000):
        self.redis_url = redis_url
        self.max_concurrent = max_concurrent
        self.redis_pool = None
        self.active_tasks: Dict[str, TaskInfo] = {}
        self.task_queues: Dict[str, asyncio.Queue] = {}
        self.workers: List[asyncio.Task] = []
        self.running = False
        
        # 性能指标
        self.metrics = {
            "total_tasks": 0,
            "completed_tasks": 0,
            "failed_tasks": 0,
            "avg_processing_time": 0.0,
            "current_queue_size": 0
        }
    
    async def initialize(self):
        """初始化并发管理器"""
        # 创建Redis连接池
        self.redis_pool = await aioredis.create_redis_pool(
            self.redis_url,
            minsize=10,
            maxsize=100
        )
        
        # 创建任务队列
        self.task_queues = {
            "high_priority": asyncio.Queue(maxsize=100),
            "normal_priority": asyncio.Queue(maxsize=500),
            "low_priority": asyncio.Queue(maxsize=1000)
        }
        
        # 启动工作线程
        await self._start_workers()
        
        self.running = True
        print(f"✅ 并发管理器初始化完成，最大并发: {self.max_concurrent}")
    
    async def _start_workers(self):
        """启动工作线程"""
        worker_count = min(50, self.max_concurrent // 10)
        
        for i in range(worker_count):
            worker = asyncio.create_task(self._worker(f"worker_{i}"))
            self.workers.append(worker)
        
        print(f"🚀 启动 {worker_count} 个工作线程")
    
    async def _worker(self, worker_name: str):
        """工作线程主循环"""
        while self.running:
            try:
                # 按优先级处理任务
                task_info = None
                
                # 高优先级队列
                if not self.task_queues["high_priority"].empty():
                    task_info = await self.task_queues["high_priority"].get()
                # 普通优先级队列
                elif not self.task_queues["normal_priority"].empty():
                    task_info = await self.task_queues["normal_priority"].get()
                # 低优先级队列
                elif not self.task_queues["low_priority"].empty():
                    task_info = await self.task_queues["low_priority"].get()
                else:
                    # 没有任务，等待一段时间
                    await asyncio.sleep(0.1)
                    continue
                
                # 处理任务
                await self._process_task(task_info, worker_name)
                
            except Exception as e:
                print(f"❌ 工作线程 {worker_name} 发生错误: {e}")
                await asyncio.sleep(1)
    
    async def _process_task(self, task_info: TaskInfo, worker_name: str):
        """处理单个任务"""
        task_id = task_info.task_id
        
        try:
            # 更新任务状态
            task_info.status = TaskStatus.RUNNING
            task_info.started_at = time.time()
            self.active_tasks[task_id] = task_info
            
            # 保存到Redis
            await self._save_task_to_redis(task_info)
            
            print(f"🔄 {worker_name} 开始处理任务 {task_id}")
            
            # 根据任务类型执行不同的处理逻辑
            if task_info.task_type == "chat_processing":
                result = await self._process_chat_task(task_info)
            elif task_info.task_type == "document_analysis":
                result = await self._process_document_task(task_info)
            elif task_info.task_type == "image_generation":
                result = await self._process_image_task(task_info)
            else:
                raise ValueError(f"未知任务类型: {task_info.task_type}")
            
            # 任务完成
            task_info.status = TaskStatus.COMPLETED
            task_info.completed_at = time.time()
            task_info.result = result
            
            # 更新指标
            processing_time = task_info.completed_at - task_info.started_at
            self.metrics["completed_tasks"] += 1
            self.metrics["avg_processing_time"] = (
                self.metrics["avg_processing_time"] * (self.metrics["completed_tasks"] - 1) + processing_time
            ) / self.metrics["completed_tasks"]
            
            print(f"✅ {worker_name} 完成任务 {task_id} ({processing_time:.3f}s)")
            
        except asyncio.TimeoutError:
            task_info.status = TaskStatus.TIMEOUT
            task_info.error = "任务执行超时"
            self.metrics["failed_tasks"] += 1
            print(f"⏱️ 任务 {task_id} 执行超时")
            
        except Exception as e:
            task_info.status = TaskStatus.FAILED
            task_info.error = str(e)
            self.metrics["failed_tasks"] += 1
            print(f"❌ 任务 {task_id} 执行失败: {e}")
        
        finally:
            # 保存最终状态
            await self._save_task_to_redis(task_info)
            
            # 清理活跃任务
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    async def _process_chat_task(self, task_info: TaskInfo) -> Dict[str, Any]:
        """处理聊天任务"""
        payload = task_info.payload
        user_message = payload.get("message", "")
        context = payload.get("context", {})
        
        # 模拟复杂的AI处理
        await asyncio.sleep(0.2)  # 模拟AI推理时间
        
        response = f"AI回复: 针对您的问题 '{user_message[:50]}...'，我的建议是..."
        
        return {
            "response": response,
            "confidence": 0.95,
            "processing_time": time.time() - task_info.started_at
        }
    
    async def _process_document_task(self, task_info: TaskInfo) -> Dict[str, Any]:
        """处理文档分析任务"""
        payload = task_info.payload
        document_url = payload.get("document_url", "")
        analysis_type = payload.get("analysis_type", "summary")
        
        # 模拟文档处理
        await asyncio.sleep(0.5)  # 文档分析需要更长时间
        
        return {
            "summary": f"文档 {document_url} 的{analysis_type}分析结果",
            "key_points": ["要点1", "要点2", "要点3"],
            "confidence": 0.88
        }
    
    async def _process_image_task(self, task_info: TaskInfo) -> Dict[str, Any]:
        """处理图像生成任务"""
        payload = task_info.payload
        prompt = payload.get("prompt", "")
        style = payload.get("style", "realistic")
        
        # 模拟图像生成
        await asyncio.sleep(1.0)  # 图像生成需要最长时间
        
        return {
            "image_url": f"https://images.example.com/{uuid.uuid4()}.png",
            "prompt": prompt,
            "style": style,
            "resolution": "1024x1024"
        }
    
    async def _save_task_to_redis(self, task_info: TaskInfo):
        """保存任务状态到Redis"""
        task_data = {
            "task_id": task_info.task_id,
            "session_id": task_info.session_id,
            "user_id": task_info.user_id,
            "task_type": task_info.task_type,
            "status": task_info.status.value,
            "created_at": task_info.created_at,
            "started_at": task_info.started_at,
            "completed_at": task_info.completed_at,
            "result": task_info.result,
            "error": task_info.error
        }
        
        # 保存任务详情
        await self.redis_pool.hset(
            f"task:{task_info.task_id}",
            mapping={k: json.dumps(v) if v is not None else "" for k, v in task_data.items()}
        )
        
        # 设置过期时间（24小时）
        await self.redis_pool.expire(f"task:{task_info.task_id}", 86400)
        
        # 更新用户任务列表
        await self.redis_pool.lpush(
            f"user_tasks:{task_info.user_id}",
            task_info.task_id
        )
        await self.redis_pool.expire(f"user_tasks:{task_info.user_id}", 86400)
    
    async def submit_task(self, user_id: str, session_id: str, task_type: str,
                         payload: Dict[str, Any], priority: str = "normal") -> str:
        """提交任务"""
        task_id = str(uuid.uuid4())
        
        task_info = TaskInfo(
            task_id=task_id,
            session_id=session_id,
            user_id=user_id,
            task_type=task_type,
            payload=payload,
            status=TaskStatus.PENDING,
            created_at=time.time()
        )
        
        # 根据优先级添加到对应队列
        queue_name = f"{priority}_priority"
        if queue_name not in self.task_queues:
            queue_name = "normal_priority"
        
        try:
            await self.task_queues[queue_name].put(task_info)
            self.metrics["total_tasks"] += 1
            self.metrics["current_queue_size"] = sum(q.qsize() for q in self.task_queues.values())
            
            print(f"📝 任务 {task_id} 已提交到 {priority} 优先级队列")
            return task_id
            
        except asyncio.QueueFull:
            raise Exception(f"{priority} 优先级队列已满，请稍后重试")
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """获取任务状态"""
        # 先检查内存中的活跃任务
        if task_id in self.active_tasks:
            task_info = self.active_tasks[task_id]
            return {
                "task_id": task_info.task_id,
                "status": task_info.status.value,
                "created_at": task_info.created_at,
                "started_at": task_info.started_at,
                "completed_at": task_info.completed_at,
                "result": task_info.result,
                "error": task_info.error
            }
        
        # 从Redis查询
        task_data = await self.redis_pool.hgetall(f"task:{task_id}")
        if not task_data:
            return None
        
        return {
            "task_id": task_data.get(b"task_id", b"").decode(),
            "status": task_data.get(b"status", b"").decode(),
            "created_at": float(task_data.get(b"created_at", 0)),
            "started_at": float(task_data.get(b"started_at", 0)) if task_data.get(b"started_at") else None,
            "completed_at": float(task_data.get(b"completed_at", 0)) if task_data.get(b"completed_at") else None,
            "result": json.loads(task_data.get(b"result", b"null")),
            "error": task_data.get(b"error", b"").decode() or None
        }
    
    async def get_metrics(self) -> Dict[str, Any]:
        """获取性能指标"""
        active_count = len(self.active_tasks)
        queue_sizes = {name: queue.qsize() for name, queue in self.task_queues.items()}
        
        return {
            **self.metrics,
            "active_tasks": active_count,
            "queue_sizes": queue_sizes,
            "worker_count": len(self.workers),
            "current_queue_size": sum(queue_sizes.values())
        }
    
    async def shutdown(self):
        """关闭并发管理器"""
        print("🔄 正在关闭并发管理器...")
        
        self.running = False
        
        # 等待所有工作线程完成
        if self.workers:
            await asyncio.gather(*self.workers, return_exceptions=True)
        
        # 关闭Redis连接
        if self.redis_pool:
            self.redis_pool.close()
            await self.redis_pool.wait_closed()
        
        print("✅ 并发管理器已关闭")

# 测试企业级并发系统
async def test_enterprise_concurrency():
    """测试企业级并发系统"""
    
    # 初始化并发管理器
    manager = EnterpriseConcurrencyManager(max_concurrent=1000)
    await manager.initialize()
    
    try:
        print("🚀 开始并发测试...")
        
        # 提交不同类型的任务
        task_ids = []
        
        # 批量提交聊天任务
        for i in range(50):
            task_id = await manager.submit_task(
                user_id=f"user_{i % 10}",
                session_id=f"session_{i}",
                task_type="chat_processing",
                payload={"message": f"用户问题 {i}", "context": {"topic": "技术咨询"}},
                priority="high" if i < 10 else "normal"
            )
            task_ids.append(task_id)
        
        # 提交文档分析任务
        for i in range(20):
            task_id = await manager.submit_task(
                user_id=f"user_{i % 5}",
                session_id=f"doc_session_{i}",
                task_type="document_analysis",
                payload={"document_url": f"https://docs.example.com/doc_{i}.pdf", "analysis_type": "summary"},
                priority="normal"
            )
            task_ids.append(task_id)
        
        # 提交图像生成任务
        for i in range(10):
            task_id = await manager.submit_task(
                user_id=f"user_{i % 3}",
                session_id=f"image_session_{i}",
                task_type="image_generation",
                payload={"prompt": f"生成图像 {i}", "style": "realistic"},
                priority="low"
            )
            task_ids.append(task_id)
        
        print(f"📝 已提交 {len(task_ids)} 个任务")
        
        # 等待所有任务完成
        completed_count = 0
        while completed_count < len(task_ids):
            await asyncio.sleep(1)
            
            completed_count = 0
            for task_id in task_ids:
                status = await manager.get_task_status(task_id)
                if status and status["status"] in ["completed", "failed", "timeout"]:
                    completed_count += 1
            
            # 显示进度
            metrics = await manager.get_metrics()
            print(f"📊 进度: {completed_count}/{len(task_ids)} 完成, "
                  f"活跃任务: {metrics['active_tasks']}, "
                  f"队列大小: {metrics['current_queue_size']}")
        
        # 生成最终报告
        print("\n🎯 并发测试完成!")
        metrics = await manager.get_metrics()
        
        print("="*50)
        print("📊 最终性能指标:")
        print(f"总任务数: {metrics['total_tasks']}")
        print(f"完成任务: {metrics['completed_tasks']}")
        print(f"失败任务: {metrics['failed_tasks']}")
        print(f"成功率: {metrics['completed_tasks']/metrics['total_tasks']*100:.1f}%")
        print(f"平均处理时间: {metrics['avg_processing_time']:.3f}秒")
        print(f"工作线程数: {metrics['worker_count']}")
        
    finally:
        await manager.shutdown()

# 运行企业级并发测试
# asyncio.run(test_enterprise_concurrency())
```

## 📊 实时性能监控系统

### 构建全面的监控体系

```python
import time
import asyncio
import threading
from collections import defaultdict, deque
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Callable
import json
import statistics
from enum import Enum
import psutil
import gc

class MetricType(str, Enum):
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    TIMER = "timer"

@dataclass
class MetricData:
    name: str
    type: MetricType
    value: float
    timestamp: float
    labels: Dict[str, str] = field(default_factory=dict)
    description: str = ""

class PerformanceMonitor:
    """高性能监控系统"""
    
    def __init__(self, retention_seconds: int = 3600):
        self.retention_seconds = retention_seconds
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque())
        self.counters: Dict[str, float] = defaultdict(float)
        self.gauges: Dict[str, float] = defaultdict(float)
        self.histograms: Dict[str, List[float]] = defaultdict(list)
        self.timers: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        
        # 系统监控
        self.system_metrics_enabled = True
        self.custom_collectors: List[Callable] = []
        
        # 告警系统
        self.alert_rules: Dict[str, Dict[str, Any]] = {}
        self.alert_callbacks: List[Callable] = []
        
        # 监控线程
        self.monitoring_thread = None
        self.running = False
        
        # 性能统计
        self.start_time = time.time()
        
    def start_monitoring(self, interval: float = 1.0):
        """启动监控"""
        self.running = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval,),
            daemon=True
        )
        self.monitoring_thread.start()
        print(f"📊 性能监控已启动，采集间隔: {interval}秒")
    
    def stop_monitoring(self):
        """停止监控"""
        self.running = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        print("🛑 性能监控已停止")
    
    def _monitoring_loop(self, interval: float):
        """监控主循环"""
        while self.running:
            try:
                # 收集系统指标
                if self.system_metrics_enabled:
                    self._collect_system_metrics()
                
                # 运行自定义收集器
                for collector in self.custom_collectors:
                    try:
                        collector(self)
                    except Exception as e:
                        print(f"❌ 自定义收集器错误: {e}")
                
                # 清理过期数据
                self._cleanup_old_metrics()
                
                # 检查告警规则
                self._check_alerts()
                
                time.sleep(interval)
                
            except Exception as e:
                print(f"❌ 监控循环错误: {e}")
                time.sleep(interval)
    
    def _collect_system_metrics(self):
        """收集系统指标"""
        current_time = time.time()
        
        # CPU指标
        cpu_percent = psutil.cpu_percent(interval=None)
        self.record_gauge("system_cpu_percent", cpu_percent, {"type": "total"})
        
        # 内存指标
        memory = psutil.virtual_memory()
        self.record_gauge("system_memory_percent", memory.percent, {"type": "used"})
        self.record_gauge("system_memory_bytes", memory.used, {"type": "used"})
        self.record_gauge("system_memory_bytes", memory.available, {"type": "available"})
        
        # 磁盘指标
        disk = psutil.disk_usage('/')
        self.record_gauge("system_disk_percent", disk.percent, {"type": "used"})
        
        # 网络指标
        network = psutil.net_io_counters()
        self.record_counter("system_network_bytes", network.bytes_sent, {"direction": "sent"})
        self.record_counter("system_network_bytes", network.bytes_recv, {"direction": "received"})
        
        # 进程指标
        process = psutil.Process()
        self.record_gauge("process_cpu_percent", process.cpu_percent())
        self.record_gauge("process_memory_bytes", process.memory_info().rss)
        self.record_gauge("process_threads", process.num_threads())
        
        # Python GC指标
        gc_stats = gc.get_stats()
        for i, stats in enumerate(gc_stats):
            self.record_counter("python_gc_collections", stats['collections'], {"generation": str(i)})
            self.record_gauge("python_gc_objects", stats['collected'], {"generation": str(i)})
    
    def record_counter(self, name: str, value: float, labels: Dict[str, str] = None):
        """记录计数器指标"""
        key = self._make_metric_key(name, labels or {})
        self.counters[key] += value
        
        metric = MetricData(
            name=name,
            type=MetricType.COUNTER,
            value=self.counters[key],
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def record_gauge(self, name: str, value: float, labels: Dict[str, str] = None):
        """记录仪表盘指标"""
        key = self._make_metric_key(name, labels or {})
        self.gauges[key] = value
        
        metric = MetricData(
            name=name,
            type=MetricType.GAUGE,
            value=value,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def record_histogram(self, name: str, value: float, labels: Dict[str, str] = None):
        """记录直方图指标"""
        key = self._make_metric_key(name, labels or {})
        self.histograms[key].append(value)
        
        # 保持最近1000个值
        if len(self.histograms[key]) > 1000:
            self.histograms[key] = self.histograms[key][-1000:]
        
        metric = MetricData(
            name=name,
            type=MetricType.HISTOGRAM,
            value=value,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def time_function(self, name: str, labels: Dict[str, str] = None):
        """函数执行时间装饰器"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = func(*args, **kwargs)
                    return result
                finally:
                    duration = time.time() - start_time
                    self.record_timer(name, duration, labels)
            return wrapper
        return decorator
    
    def record_timer(self, name: str, duration: float, labels: Dict[str, str] = None):
        """记录计时器指标"""
        key = self._make_metric_key(name, labels or {})
        self.timers[key].append(duration)
        
        metric = MetricData(
            name=name,
            type=MetricType.TIMER,
            value=duration,
            timestamp=time.time(),
            labels=labels or {}
        )
        
        self.metrics[key].append(metric)
    
    def _make_metric_key(self, name: str, labels: Dict[str, str]) -> str:
        """生成指标键"""
        if not labels:
            return name
        
        label_str = ",".join(f"{k}={v}" for k, v in sorted(labels.items()))
        return f"{name}{{{label_str}}}"
    
    def _cleanup_old_metrics(self):
        """清理过期指标"""
        cutoff_time = time.time() - self.retention_seconds
        
        for key, metric_list in self.metrics.items():
            while metric_list and metric_list[0].timestamp < cutoff_time:
                metric_list.popleft()
    
    def get_metric_stats(self, name: str, labels: Dict[str, str] = None, 
                        duration: float = 300) -> Dict[str, Any]:
        """获取指标统计信息"""
        key = self._make_metric_key(name, labels or {})
        cutoff_time = time.time() - duration
        
        recent_metrics = [
            m for m in self.metrics[key] 
            if m.timestamp >= cutoff_time
        ]
        
        if not recent_metrics:
            return {"error": "没有找到指标数据"}
        
        values = [m.value for m in recent_metrics]
        
        stats = {
            "count": len(values),
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "first_timestamp": recent_metrics[0].timestamp,
            "last_timestamp": recent_metrics[-1].timestamp,
        }
        
        # 计算百分位数
        if len(values) >= 2:
            stats["stddev"] = statistics.stdev(values)
            
        if len(values) >= 10:
            stats["p50"] = statistics.quantiles(values, n=2)[0]
            stats["p95"] = statistics.quantiles(values, n=20)[18]
            stats["p99"] = statistics.quantiles(values, n=100)[98]
        
        return stats
    
    def add_alert_rule(self, metric_name: str, condition: str, threshold: float,
                      callback: Optional[Callable] = None, description: str = ""):
        """添加告警规则"""
        rule_id = f"{metric_name}_{condition}_{threshold}"
        
        self.alert_rules[rule_id] = {
            "metric_name": metric_name,
            "condition": condition,  # "gt", "lt", "eq", "ne"
            "threshold": threshold,
            "callback": callback,
            "description": description,
            "triggered": False,
            "last_triggered": None
        }
        
        print(f"📢 添加告警规则: {metric_name} {condition} {threshold}")
    
    def _check_alerts(self):
        """检查告警规则"""
        current_time = time.time()
        
        for rule_id, rule in self.alert_rules.items():
            try:
                # 获取最新指标值
                stats = self.get_metric_stats(rule["metric_name"], duration=60)
                
                if "error" in stats:
                    continue
                
                current_value = stats["mean"]  # 使用平均值作为当前值
                
                # 检查条件
                triggered = False
                if rule["condition"] == "gt" and current_value > rule["threshold"]:
                    triggered = True
                elif rule["condition"] == "lt" and current_value < rule["threshold"]:
                    triggered = True
                elif rule["condition"] == "eq" and abs(current_value - rule["threshold"]) < 0.001:
                    triggered = True
                elif rule["condition"] == "ne" and abs(current_value - rule["threshold"]) >= 0.001:
                    triggered = True
                
                # 触发告警
                if triggered and not rule["triggered"]:
                    rule["triggered"] = True
                    rule["last_triggered"] = current_time
                    
                    alert_info = {
                        "rule_id": rule_id,
                        "metric_name": rule["metric_name"],
                        "current_value": current_value,
                        "threshold": rule["threshold"],
                        "condition": rule["condition"],
                        "description": rule["description"],
                        "timestamp": current_time
                    }
                    
                    print(f"🚨 告警触发: {rule['metric_name']} = {current_value:.2f} "
                          f"{rule['condition']} {rule['threshold']}")
                    
                    # 执行回调
                    if rule["callback"]:
                        try:
                            rule["callback"](alert_info)
                        except Exception as e:
                            print(f"❌ 告警回调执行失败: {e}")
                    
                    # 执行全局回调
                    for callback in self.alert_callbacks:
                        try:
                            callback(alert_info)
                        except Exception as e:
                            print(f"❌ 全局告警回调失败: {e}")
                
                # 恢复状态
                elif not triggered and rule["triggered"]:
                    rule["triggered"] = False
                    print(f"✅ 告警恢复: {rule['metric_name']} = {current_value:.2f}")
                    
            except Exception as e:
                print(f"❌ 检查告警规则 {rule_id} 时出错: {e}")
    
    def get_dashboard_data(self) -> Dict[str, Any]:
        """获取仪表板数据"""
        current_time = time.time()
        uptime = current_time - self.start_time
        
        # 系统概览
        system_overview = {
            "uptime_seconds": uptime,
            "uptime_human": self._format_duration(uptime),
            "current_time": current_time
        }
        
        # 关键指标
        key_metrics = {}
        important_metrics = [
            "system_cpu_percent",
            "system_memory_percent", 
            "process_memory_bytes",
            "langgraph_requests_total",
            "langgraph_response_time"
        ]
        
        for metric in important_metrics:
            stats = self.get_metric_stats(metric, duration=300)
            if "error" not in stats:
                key_metrics[metric] = {
                    "current": stats.get("mean", 0),
                    "min": stats.get("min", 0),
                    "max": stats.get("max", 0),
                    "p95": stats.get("p95", 0)
                }
        
        # 活跃告警
        active_alerts = [
            {
                "rule_id": rule_id,
                "metric_name": rule["metric_name"],
                "description": rule["description"],
                "last_triggered": rule["last_triggered"]
            }
            for rule_id, rule in self.alert_rules.items()
            if rule["triggered"]
        ]
        
        return {
            "system_overview": system_overview,
            "key_metrics": key_metrics,
            "active_alerts": active_alerts,
            "total_metrics": len(self.metrics),
            "alert_rules_count": len(self.alert_rules)
        }
    
    def _format_duration(self, seconds: float) -> str:
        """格式化时间长度"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        
        if hours > 0:
            return f"{hours}h {minutes}m {secs}s"
        elif minutes > 0:
            return f"{minutes}m {secs}s"
        else:
            return f"{secs}s"
    
    def export_prometheus_metrics(self) -> str:
        """导出Prometheus格式指标"""
        lines = []
        
        for key, metric_list in self.metrics.items():
            if not metric_list:
                continue
                
            latest_metric = metric_list[-1]
            
            # 构造Prometheus格式
            if latest_metric.labels:
                label_str = ",".join(f'{k}="{v}"' for k, v in latest_metric.labels.items())
                line = f'{latest_metric.name}{{{label_str}}} {latest_metric.value} {int(latest_metric.timestamp * 1000)}'
            else:
                line = f'{latest_metric.name} {latest_metric.value} {int(latest_metric.timestamp * 1000)}'
            
            lines.append(line)
        
        return "\n".join(lines)

# LangGraph监控集成
class LangGraphMonitoringIntegration:
    """LangGraph监控集成"""
    
    def __init__(self, monitor: PerformanceMonitor):
        self.monitor = monitor
        self.request_count = 0
        self.error_count = 0
    
    def wrap_langgraph_app(self, app):
        """包装LangGraph应用以添加监控"""
        original_invoke = app.invoke
        original_ainvoke = app.ainvoke if hasattr(app, 'ainvoke') else None
        
        def monitored_invoke(input_data, config=None, **kwargs):
            start_time = time.time()
            self.request_count += 1
            
            # 记录请求开始
            self.monitor.record_counter("langgraph_requests_total", 1, {"method": "invoke"})
            
            try:
                result = original_invoke(input_data, config, **kwargs)
                
                # 记录成功
                duration = time.time() - start_time
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "success"})
                self.monitor.record_histogram("langgraph_request_duration", duration)
                
                return result
                
            except Exception as e:
                # 记录错误
                self.error_count += 1
                duration = time.time() - start_time
                self.monitor.record_counter("langgraph_errors_total", 1, {"error_type": type(e).__name__})
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "error"})
                
                raise
        
        async def monitored_ainvoke(input_data, config=None, **kwargs):
            start_time = time.time()
            self.request_count += 1
            
            # 记录异步请求开始
            self.monitor.record_counter("langgraph_requests_total", 1, {"method": "ainvoke"})
            
            try:
                result = await original_ainvoke(input_data, config, **kwargs)
                
                # 记录成功
                duration = time.time() - start_time
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "success"})
                self.monitor.record_histogram("langgraph_request_duration", duration)
                
                return result
                
            except Exception as e:
                # 记录错误
                self.error_count += 1
                duration = time.time() - start_time
                self.monitor.record_counter("langgraph_errors_total", 1, {"error_type": type(e).__name__})
                self.monitor.record_timer("langgraph_response_time", duration, {"status": "error"})
                
                raise
        
        # 替换方法
        app.invoke = monitored_invoke
        if original_ainvoke:
            app.ainvoke = monitored_ainvoke
        
        return app
    
    def add_custom_metrics_collector(self):
        """添加自定义指标收集器"""
        def collect_langgraph_metrics(monitor):
            # 收集请求相关指标
            monitor.record_gauge("langgraph_total_requests", self.request_count)
            monitor.record_gauge("langgraph_total_errors", self.error_count)
            
            # 计算错误率
            if self.request_count > 0:
                error_rate = self.error_count / self.request_count * 100
                monitor.record_gauge("langgraph_error_rate_percent", error_rate)
        
        self.monitor.custom_collectors.append(collect_langgraph_metrics)

# 测试完整的监控系统
def test_monitoring_system():
    """测试完整的监控系统"""
    print("🚀 启动监控系统测试")
    
    # 创建监控器
    monitor = PerformanceMonitor(retention_seconds=300)
    
    # 添加告警规则
    def high_cpu_alert(alert_info):
        print(f"🔥 高CPU使用率告警: {alert_info['current_value']:.1f}%")
    
    def high_memory_alert(alert_info):
        print(f"💾 高内存使用率告警: {alert_info['current_value']:.1f}%")
    
    monitor.add_alert_rule("system_cpu_percent", "gt", 80.0, high_cpu_alert, "CPU使用率过高")
    monitor.add_alert_rule("system_memory_percent", "gt", 85.0, high_memory_alert, "内存使用率过高")
    monitor.add_alert_rule("langgraph_error_rate_percent", "gt", 5.0, None, "错误率过高")
    
    # 启动监控
    monitor.start_monitoring(interval=2.0)
    
    try:
        # 模拟应用运行
        integration = LangGraphMonitoringIntegration(monitor)
        integration.add_custom_metrics_collector()
        
        # 模拟一些请求
        for i in range(10):
            # 模拟正常请求
            start_time = time.time()
            time.sleep(0.1)  # 模拟处理时间
            duration = time.time() - start_time
            
            monitor.record_timer("langgraph_response_time", duration, {"status": "success"})
            monitor.record_counter("langgraph_requests_total", 1)
            integration.request_count += 1
            
            # 偶尔模拟错误
            if i % 7 == 0:
                monitor.record_counter("langgraph_errors_total", 1, {"error_type": "ValidationError"})
                integration.error_count += 1
        
        # 等待一段时间收集指标
        print("📊 收集监控数据...")
        time.sleep(10)
        
        # 获取仪表板数据
        dashboard = monitor.get_dashboard_data()
        
        print("\n🎯 监控仪表板")
        print("="*50)
        print(f"系统运行时间: {dashboard['system_overview']['uptime_human']}")
        print(f"监控指标数量: {dashboard['total_metrics']}")
        print(f"告警规则数量: {dashboard['alert_rules_count']}")
        print(f"活跃告警数量: {len(dashboard['active_alerts'])}")
        
        print("\n📈 关键指标:")
        for metric_name, stats in dashboard['key_metrics'].items():
            print(f"  {metric_name}: 当前={stats['current']:.2f}, "
                  f"最小={stats['min']:.2f}, 最大={stats['max']:.2f}")
        
        # 显示一些具体统计
        response_time_stats = monitor.get_metric_stats("langgraph_response_time", duration=300)
        if "error" not in response_time_stats:
            print(f"\n⏱️ 响应时间统计:")
            print(f"  平均: {response_time_stats['mean']:.3f}s")
            print(f"  中位数: {response_time_stats['median']:.3f}s")
            if 'p95' in response_time_stats:
                print(f"  P95: {response_time_stats['p95']:.3f}s")
        
        # 导出Prometheus格式
        prometheus_data = monitor.export_prometheus_metrics()
        print(f"\n📤 Prometheus指标导出 ({len(prometheus_data.splitlines())} 行):")
        print(prometheus_data[:300] + "..." if len(prometheus_data) > 300 else prometheus_data)
        
    finally:
        monitor.stop_monitoring()

# 运行监控测试
if __name__ == "__main__":
    test_monitoring_system()
```

## 🚀 性能优化实战

### 智能缓存系统

```python
import redis
import pickle
import hashlib
import time
import asyncio
from typing import Any, Optional, Callable, Union, Dict
from functools import wraps
import json
import zlib

class IntelligentCacheManager:
    """智能缓存管理系统"""
    
    def __init__(self, redis_url: str = "redis://localhost", 
                 default_ttl: int = 3600, compression_threshold: int = 1024):
        self.redis_url = redis_url
        self.default_ttl = default_ttl
        self.compression_threshold = compression_threshold
        self.redis_client = None
        
        # 缓存统计
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "errors": 0,
            "compression_saves": 0
        }
        
        # 智能预加载
        self.preload_patterns = {}
        self.access_patterns = {}
    
    def connect(self):
        """连接Redis"""
        try:
            self.redis_client = redis.from_url(self.redis_url, decode_responses=False)
            # 测试连接
            self.redis_client.ping()
            print("✅ Redis连接成功")
        except Exception as e:
            print(f"❌ Redis连接失败: {e}")
            # 使用内存缓存作为备选
            self.redis_client = {}
            print("⚠️ 使用内存缓存作为备选")
    
    def _make_cache_key(self, key: str, prefix: str = "lg") -> str:
        """生成缓存键"""
        # 对长键进行哈希
        if len(key) > 100:
            key_hash = hashlib.md5(key.encode()).hexdigest()
            return f"{prefix}:hash:{key_hash}"
        return f"{prefix}:{key}"
    
    def _serialize_value(self, value: Any) -> bytes:
        """序列化值"""
        # 序列化
        serialized = pickle.dumps(value)
        
        # 如果数据较大，进行压缩
        if len(serialized) > self.compression_threshold:
            compressed = zlib.compress(serialized)
            if len(compressed) < len(serialized) * 0.8:  # 压缩率超过20%才使用
                self.stats["compression_saves"] += len(serialized) - len(compressed)
                return b"COMPRESSED:" + compressed
        
        return serialized
    
    def _deserialize_value(self, data: bytes) -> Any:
        """反序列化值"""
        if data.startswith(b"COMPRESSED:"):
            # 解压缩
            compressed_data = data[11:]  # 去掉"COMPRESSED:"前缀
            decompressed = zlib.decompress(compressed_data)
            return pickle.loads(decompressed)
        else:
            return pickle.loads(data)
    
    def get(self, key: str) -> Optional[Any]:
        """获取缓存值"""
        cache_key = self._make_cache_key(key)
        
        try:
            if isinstance(self.redis_client, dict):
                # 内存缓存
                if cache_key in self.redis_client:
                    data, expire_time = self.redis_client[cache_key]
                    if time.time() < expire_time:
                        self.stats["hits"] += 1
                        self._record_access_pattern(key)
                        return self._deserialize_value(data)
                    else:
                        del self.redis_client[cache_key]
            else:
                # Redis缓存
                data = self.redis_client.get(cache_key)
                if data is not None:
                    self.stats["hits"] += 1
                    self._record_access_pattern(key)
                    return self._deserialize_value(data)
            
            self.stats["misses"] += 1
            return None
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"❌ 缓存获取错误: {e}")
            return None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """设置缓存值"""
        cache_key = self._make_cache_key(key)
        ttl = ttl or self.default_ttl
        
        try:
            serialized_value = self._serialize_value(value)
            
            if isinstance(self.redis_client, dict):
                # 内存缓存
                expire_time = time.time() + ttl
                self.redis_client[cache_key] = (serialized_value, expire_time)
            else:
                # Redis缓存
                self.redis_client.setex(cache_key, ttl, serialized_value)
            
            self.stats["sets"] += 1
            return True
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"❌ 缓存设置错误: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """删除缓存值"""
        cache_key = self._make_cache_key(key)
        
        try:
            if isinstance(self.redis_client, dict):
                if cache_key in self.redis_client:
                    del self.redis_client[cache_key]
            else:
                self.redis_client.delete(cache_key)
            
            self.stats["deletes"] += 1
            return True
            
        except Exception as e:
            self.stats["errors"] += 1
            print(f"❌ 缓存删除错误: {e}")
            return False
    
    def _record_access_pattern(self, key: str):
        """记录访问模式"""
        current_time = time.time()
        
        if key not in self.access_patterns:
            self.access_patterns[key] = {
                "count": 0,
                "last_access": current_time,
                "avg_interval": 0
            }
        
        pattern = self.access_patterns[key]
        pattern["count"] += 1
        
        if pattern["count"] > 1:
            interval = current_time - pattern["last_access"]
            pattern["avg_interval"] = (pattern["avg_interval"] * (pattern["count"] - 2) + interval) / (pattern["count"] - 1)
        
        pattern["last_access"] = current_time
        
        # 预测下次访问时间
        if pattern["count"] > 3 and pattern["avg_interval"] > 0:
            next_access = current_time + pattern["avg_interval"]
            self.preload_patterns[key] = next_access
    
    def get_hit_rate(self) -> float:
        """获取缓存命中率"""
        total_requests = self.stats["hits"] + self.stats["misses"]
        if total_requests == 0:
            return 0.0
        return self.stats["hits"] / total_requests
    
    def get_stats(self) -> Dict[str, Any]:
        """获取缓存统计信息"""
        return {
            **self.stats,
            "hit_rate": self.get_hit_rate(),
            "total_requests": self.stats["hits"] + self.stats["misses"],
            "compression_savings_bytes": self.stats["compression_saves"]
        }

# 缓存装饰器
def cached(cache_manager: IntelligentCacheManager, ttl: Optional[int] = None, 
          key_func: Optional[Callable] = None):
    """缓存装饰器"""
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # 生成缓存键
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # 默认键生成策略
                args_str = str(args) + str(sorted(kwargs.items()))
                cache_key = f"{func.__name__}:{hashlib.md5(args_str.encode()).hexdigest()}"
            
            # 尝试从缓存获取
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # 缓存未命中，执行函数
            result = func(*args, **kwargs)
            
            # 存储到缓存
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        # 异步版本
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            # 生成缓存键
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                args_str = str(args) + str(sorted(kwargs.items()))
                cache_key = f"{func.__name__}:{hashlib.md5(args_str.encode()).hexdigest()}"
            
            # 尝试从缓存获取
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # 缓存未命中，执行函数
            result = await func(*args, **kwargs)
            
            # 存储到缓存
            cache_manager.set(cache_key, result, ttl)
            
            return result
        
        # 根据函数类型返回对应的包装器
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return wrapper
    
    return decorator

# 使用示例
cache_manager = IntelligentCacheManager()
cache_manager.connect()

@cached(cache_manager, ttl=300)
def expensive_computation(n: int) -> int:
    """昂贵的计算操作"""
    print(f"🔢 执行昂贵计算: n={n}")
    time.sleep(1)  # 模拟耗时操作
    return sum(i * i for i in range(n))

@cached(cache_manager, ttl=600)
async def async_api_call(endpoint: str, params: dict) -> dict:
    """异步API调用"""
    print(f"🌐 执行API调用: {endpoint}")
    await asyncio.sleep(0.5)  # 模拟网络延迟
    return {"endpoint": endpoint, "params": params, "result": "mock_data"}

# 测试缓存性能
def test_cache_performance():
    """测试缓存性能"""
    print("🚀 开始缓存性能测试")
    
    # 重置统计
    cache_manager.stats = {k: 0 for k in cache_manager.stats}
    
    # 测试同步缓存
    print("\n📊 同步缓存测试:")
    
    # 第一次调用（缓存未命中）
    start_time = time.time()
    result1 = expensive_computation(1000)
    first_call_time = time.time() - start_time
    print(f"   第一次调用: {first_call_time:.3f}s (未命中)")
    
    # 第二次调用（缓存命中）
    start_time = time.time()
    result2 = expensive_computation(1000)
    second_call_time = time.time() - start_time
    print(f"   第二次调用: {second_call_time:.3f}s (命中)")
    
    # 验证结果一致性
    assert result1 == result2, "缓存结果不一致"
    
    speedup = first_call_time / second_call_time if second_call_time > 0 else float('inf')
    print(f"   性能提升: {speedup:.1f}x")
    
    # 测试异步缓存
    async def test_async_cache():
        print("\n🌐 异步缓存测试:")
        
        # 第一次调用
        start_time = time.time()
        result1 = await async_api_call("https://api.example.com/data", {"page": 1})
        first_call_time = time.time() - start_time
        print(f"   第一次调用: {first_call_time:.3f}s (未命中)")
        
        # 第二次调用
        start_time = time.time()
        result2 = await async_api_call("https://api.example.com/data", {"page": 1})
        second_call_time = time.time() - start_time
        print(f"   第二次调用: {second_call_time:.3f}s (命中)")
        
        speedup = first_call_time / second_call_time if second_call_time > 0 else float('inf')
        print(f"   性能提升: {speedup:.1f}x")
    
    # 运行异步测试
    asyncio.run(test_async_cache())
    
    # 显示缓存统计
    stats = cache_manager.get_stats()
    print(f"\n📈 缓存统计:")
    print(f"   命中率: {stats['hit_rate']:.1%}")
    print(f"   总请求: {stats['total_requests']}")
    print(f"   命中: {stats['hits']}")
    print(f"   未命中: {stats['misses']}")
    print(f"   设置: {stats['sets']}")
    print(f"   压缩节省: {stats['compression_savings_bytes']} 字节")

# 运行缓存测试
if __name__ == "__main__":
    test_cache_performance()
```

## 🎯 综合实战：企业级AI客服中心

让我们把所有学到的技术整合到一个完整的企业级系统中：

```python
# 这里是一个完整的企业级AI客服中心实现
# 由于篇幅限制，我将提供关键架构和核心代码

class EnterpriseAICustomerService:
    """企业级AI客服中心"""
    
    def __init__(self):
        # 核心组件初始化
        self.checkpointer = self._setup_persistence()
        self.cache_manager = self._setup_cache()
        self.monitor = self._setup_monitoring()
        self.concurrency_manager = self._setup_concurrency()
        
        # LangGraph应用
        self.app = self._build_langgraph_app()
        
        # 性能指标
        self.performance_targets = {
            "response_time_p95": 2.0,  # 95%请求<2秒
            "availability": 99.9,      # 99.9%可用性
            "concurrent_users": 10000,  # 支持1万并发
            "error_rate": 0.1          # 错误率<0.1%
        }
    
    def _setup_persistence(self):
        """配置企业级持久化"""
        return EnterpriseCheckpointer({
            "host": "postgres-cluster.internal",
            "port": 5432,
            "database": "ai_customer_service",
            "user": "service_user",
            "password": "secure_password",
            "pool_size": 100
        })
    
    def _setup_cache(self):
        """配置智能缓存"""
        return IntelligentCacheManager(
            redis_url="redis://redis-cluster.internal:6379",
            default_ttl=1800,  # 30分钟
            compression_threshold=2048
        )
    
    def _setup_monitoring(self):
        """配置监控系统"""
        monitor = PerformanceMonitor(retention_seconds=86400)  # 24小时
        
        # 关键告警规则
        monitor.add_alert_rule("response_time_p95", "gt", 2.0, self._handle_slow_response)
        monitor.add_alert_rule("error_rate", "gt", 0.1, self._handle_high_error_rate)
        monitor.add_alert_rule("concurrent_users", "gt", 12000, self._handle_high_load)
        
        return monitor
    
    def _setup_concurrency(self):
        """配置并发管理"""
        return EnterpriseConcurrencyManager(max_concurrent=15000)
    
    def _build_langgraph_app(self):
        """构建LangGraph应用"""
        # 这里会包含完整的多代理客服系统
        # 包含：意图识别、知识检索、回复生成、质量检查等代理
        pass
    
    async def start(self):
        """启动企业级客服系统"""
        print("🚀 启动企业级AI客服中心...")
        
        # 初始化各个组件
        await self.checkpointer.initialize()
        self.cache_manager.connect()
        self.monitor.start_monitoring()
        await self.concurrency_manager.initialize()
        
        # 预热系统
        await self._warmup_system()
        
        print("✅ 企业级AI客服中心启动完成")
        print(f"📊 性能目标: {self.performance_targets}")
    
    async def _warmup_system(self):
        """系统预热"""
        print("🔥 系统预热中...")
        
        # 预加载常用缓存
        common_queries = [
            "账户余额查询",
            "密码重置",
            "产品价格咨询",
            "技术支持",
            "退款申请"
        ]
        
        for query in common_queries:
            # 预加载AI模型响应
            await self._preload_ai_response(query)
        
        print("✅ 系统预热完成")

# 性能基准测试
async def run_enterprise_benchmark():
    """运行企业级性能基准测试"""
    print("🎯 企业级AI客服中心基准测试")
    print("="*60)
    
    service = EnterpriseAICustomerService()
    await service.start()
    
    try:
        # 模拟生产负载
        await simulate_production_load(service)
        
        # 生成性能报告
        await generate_performance_report(service)
        
    finally:
        await service.shutdown()

# 这只是完整实现的框架
# 完整版本会包含所有前面章节学到的技术
```

## 🎓 学习检验清单

完成本章学习后，你应该能够：

**基础能力**：
- [ ] 配置和使用LangGraph的状态持久化功能
- [ ] 实现基本的并发控制和资源管理
- [ ] 搭建基础的性能监控系统
- [ ] 应用基本的缓存优化策略

**进阶能力**：
- [ ] 设计企业级的状态版本管理系统
- [ ] 实现高性能的异步并发处理
- [ ] 构建完整的可观测性系统（指标、日志、链路追踪）
- [ ] 优化系统性能并进行容量规划

**专家能力**：
- [ ] 设计大规模分布式LangGraph系统架构
- [ ] 实现自动化的性能优化和故障恢复
- [ ] 建立完整的SRE实践体系
- [ ] 指导团队进行企业级系统设计

## 🚀 下一步展望

**L5层预告 - 企业级部署与运维**：
- 🏗️ **容器化部署**: Docker容器化和Kubernetes编排
- ☁️ **云原生架构**: 云平台集成和无服务器部署
- 🔄 **CI/CD流水线**: 自动化测试、构建和部署
- 📊 **运维监控**: 完整的运维工具链和最佳实践
- 🛡️ **安全合规**: 企业级安全策略和合规要求

---

**🎯 恭喜！** 你已经掌握了构建企业级LangGraph系统的核心技能。现在你可以设计和实现支持万级并发、秒级恢复的生产级AI系统！

**👉 下一步**: [L5: 企业级部署与运维](./05-企业级部署与运维.md) - 将你的系统部署到生产环境！