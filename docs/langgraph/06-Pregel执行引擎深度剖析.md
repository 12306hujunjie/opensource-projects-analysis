# L5: Pregelæ‰§è¡Œå¼•æ“æ·±åº¦å‰–æ

**å­¦ä¹ ç›®æ ‡**: æŒæ¡LangGraphçš„æ ¸å¿ƒæ‰§è¡Œæœºåˆ¶ï¼Œç†è§£åˆ†å¸ƒå¼å›¾è®¡ç®—çš„å·¥ç¨‹å®ç°  
**é¢„è®¡ç”¨æ—¶**: 4-5å°æ—¶  
**æ ¸å¿ƒè½¬å˜**: ä»"é¡ºåºæ‰§è¡Œ"æ€ç»´ â†’ "å›¾å¹¶è¡Œè®¡ç®—"æ€ç»´

*ğŸ’¡ è¿™ä¸€ç« å°†å¸¦ä½ æ·±å…¥LangGraphçš„å¿ƒè„â€”â€”Pregelæ‰§è¡Œå¼•æ“ã€‚è¿™ä¸ä»…æ˜¯æºç å­¦ä¹ ï¼Œæ›´æ˜¯ä¸€æ¬¡ä¸Googleåˆ†å¸ƒå¼è®¡ç®—æ€æƒ³çš„å¯¹è¯ã€‚ä½ å°†ç†è§£å¦‚ä½•å°†å­¦æœ¯ç•Œçš„å›¾è®¡ç®—ç†è®ºè½¬åŒ–ä¸ºå¯ç”Ÿäº§çš„AIå·¥ä½œæµå¼•æ“ã€‚*

---

## ğŸŒŸ å¼€ç¯‡ï¼šä»Googleè®ºæ–‡åˆ°ç”Ÿäº§å¼•æ“

### ä»¤äººç€è¿·çš„æŠ€æœ¯ä¼ æ‰¿

æƒ³è±¡ä¸€ä¸‹ï¼Œ2010å¹´Googleå‘è¡¨äº†ä¸€ç¯‡æ”¹å˜å›¾è®¡ç®—é¢†åŸŸçš„è®ºæ–‡ã€ŠPregel: A System for Large-Scale Graph Processingã€‹ã€‚åå¤šå¹´åï¼Œè¿™ä¸ªæ€æƒ³åœ¨LangGraphä¸­é‡è·æ–°ç”Ÿï¼š

```python
# Google Pregelçš„æ ¸å¿ƒæ€æƒ³ï¼šThink Like a Vertex
"""
æ¯ä¸ªé¡¶ç‚¹ï¼ˆèŠ‚ç‚¹ï¼‰ç‹¬ç«‹æ€è€ƒï¼š
1. æ¥æ”¶æ¥è‡ªå…¶ä»–é¡¶ç‚¹çš„æ¶ˆæ¯
2. æ›´æ–°è‡ªå·±çš„çŠ¶æ€  
3. å‘é‚»å±…å‘é€æ¶ˆæ¯
4. å†³å®šæ˜¯å¦åœ¨ä¸‹ä¸€è½®ç»§ç»­æ´»è·ƒ
"""

# LangGraphçš„Pregelå®ç°ï¼šThink Like an AI Agent
class PregelNode:
    def execute(self, state, messages):
        # 1. å¤„ç†æ¥æ”¶åˆ°çš„çŠ¶æ€å’Œæ¶ˆæ¯
        # 2. æ‰§è¡ŒAIé€»è¾‘ï¼ˆLLMè°ƒç”¨ã€å·¥å…·ä½¿ç”¨ç­‰ï¼‰
        # 3. æ›´æ–°çŠ¶æ€å¹¶å‘é€ç»™ä¸‹æ¸¸èŠ‚ç‚¹
        # 4. å†³å®šå·¥ä½œæµæ˜¯å¦ç»§ç»­
        pass
```

**è¿™ç§è®¾è®¡å“²å­¦çš„é©å‘½æ€§åœ¨äº**ï¼š
- ğŸ§  **å»ä¸­å¿ƒåŒ–æ‰§è¡Œ**ï¼šæ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹å†³ç­–ï¼Œæ— éœ€å…¨å±€åè°ƒ
- âš¡ **å¤©ç„¶å¹¶è¡Œæ€§**ï¼šèŠ‚ç‚¹å¯ä»¥åŒæ—¶æ‰§è¡Œï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸èµ„æº
- ğŸ”„ **è‡ªé€‚åº”ç»ˆæ­¢**ï¼šç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹ä½•æ—¶åœæ­¢ï¼Œæ— éœ€é¢„è®¾æ­¥æ•°
- ğŸ›¡ï¸ **æ•…éšœå®¹é”™**ï¼šå•ä¸ªèŠ‚ç‚¹å¤±è´¥ä¸ä¼šå½±å“æ•´ä¸ªè®¡ç®—

### ä»ä¼ ç»Ÿå·¥ä½œæµåˆ°å›¾è®¡ç®—çš„è·ƒè¿

**ä¼ ç»ŸAIå·¥ä½œæµçš„å±€é™**ï¼š
```python
# ä¼ ç»Ÿçº¿æ€§æ‰§è¡Œ
def traditional_workflow(input_data):
    step1_result = process_step1(input_data)      # å¿…é¡»ç­‰å¾…
    step2_result = process_step2(step1_result)    # ä¸²è¡Œæ‰§è¡Œ
    step3_result = process_step3(step2_result)    # æ— æ³•å¹¶è¡Œ
    return step3_result                           # å›ºå®šæµç¨‹
```

**LangGraph Pregelçš„çªç ´**ï¼š
```python
# å›¾å¹¶è¡Œè®¡ç®—
class AIWorkflow(StateGraph):
    def __init__(self):
        # å¤šä¸ªèŠ‚ç‚¹å¯ä»¥åŒæ—¶æ´»è·ƒ
        self.add_node("researcher", research_agent)
        self.add_node("analyzer", analysis_agent) 
        self.add_node("writer", writing_agent)
        
        # æ¡ä»¶æ¿€æ´»ï¼šåªæœ‰éœ€è¦æ—¶æ‰æ‰§è¡Œ
        self.add_conditional_edges("researcher", should_analyze)
        self.add_conditional_edges("analyzer", should_write)
        
        # è‡ªé€‚åº”ç»ˆæ­¢ï¼šæ»¡è¶³æ¡ä»¶æ—¶è‡ªåŠ¨åœæ­¢
        self.add_conditional_edges("writer", check_quality)
```

**æ€§èƒ½å’Œèƒ½åŠ›çš„é‡çº§æå‡**ï¼š

| ç»´åº¦ | ä¼ ç»Ÿå·¥ä½œæµ | LangGraph Pregel | æå‡å€æ•° |
|------|------------|------------------|----------|
| **å¹¶è¡Œåº¦** | 1ï¼ˆä¸²è¡Œï¼‰ | Nï¼ˆèŠ‚ç‚¹æ•°ï¼‰ | Nå€ |
| **å“åº”æ€§** | å›ºå®šå»¶è¿Ÿ | è‡ªé€‚åº”æœ€çŸ­è·¯å¾„ | 2-10å€ |
| **èµ„æºåˆ©ç”¨** | é¡ºåºä½¿ç”¨ | å¹¶è¡Œå……åˆ†åˆ©ç”¨ | 3-8å€ |
| **å¯æ‰©å±•æ€§** | çº¿æ€§å¢é•¿ | å›¾ç»“æ„æ‰©å±• | æŒ‡æ•°çº§ |
| **å®¹é”™èƒ½åŠ›** | å•ç‚¹æ•…éšœ | åˆ†å¸ƒå¼å®¹é”™ | è´¨çš„é£è·ƒ |

---

## ğŸ—ï¸ Pregelå¼•æ“æ ¸å¿ƒæ¶æ„æ·±åº¦è§£æ

### 2.1 Pregelç±»çš„ç³»ç»Ÿæ¶æ„

**Pregelä¸»ç±»ç»“æ„** (`pregel/main.py:307-3190`)ï¼š

```python
class Pregel(Runnable[Input, Output]):
    """LangGraphçš„æ ¸å¿ƒæ‰§è¡Œå¼•æ“ï¼ŒåŸºäºGoogle Pregelç®—æ³•å®ç°"""
    
    # å›¾ç»“æ„å®šä¹‰
    nodes: dict[str, PregelNode | NodeBuilder]           # èŠ‚ç‚¹æ˜ å°„
    channels: dict[str, BaseChannel | ManagedValueSpec]   # é€šé“å®šä¹‰
    
    # æ‰§è¡Œæ§åˆ¶
    stream_mode: StreamMode = "values"                    # æµå¼æ¨¡å¼
    interrupt_after_nodes: All | Sequence[str] = ()      # ä¸­æ–­æ§åˆ¶
    interrupt_before_nodes: All | Sequence[str] = ()     # å‰ç½®ä¸­æ–­
    
    # æ€§èƒ½ä¼˜åŒ–
    checkpointer: BaseCheckpointSaver | None = None      # æ£€æŸ¥ç‚¹é›†æˆ
    cache: BaseCache | None = None                       # ç¼“å­˜ç³»ç»Ÿ
    retry_policy: RetryPolicy | Sequence[RetryPolicy]    # é‡è¯•ç­–ç•¥
    
    # é«˜çº§ç‰¹æ€§
    trigger_to_nodes: Mapping[str, Sequence[str]]        # è§¦å‘æ˜ å°„
    step_timeout: float | None = None                    # è¶…æ—¶æ§åˆ¶
```

**æ¶æ„è®¾è®¡çš„å·¥ç¨‹æ™ºæ…§**ï¼š

1. **ç»„åˆä¼˜äºç»§æ‰¿**ï¼š
   ```python
   # Pregelé€šè¿‡ç»„åˆä¸åŒç»„ä»¶å®ç°å¤æ‚åŠŸèƒ½
   def __init__(self, *, nodes, channels, checkpointer=None, cache=None, ...):
       self.nodes = {k: v.build() if isinstance(v, NodeBuilder) else v 
                    for k, v in nodes.items()}
       self.channels = channels or {}
       self.checkpointer = checkpointer  # å¯æ’æ‹”çš„æŒä¹…åŒ–
       self.cache = cache               # å¯æ’æ‹”çš„ç¼“å­˜
   ```

2. **ç­–ç•¥æ¨¡å¼çš„ç²¾å¦™åº”ç”¨**ï¼š
   ```python
   # ä¸åŒçš„é‡è¯•ç­–ç•¥å¯ä»¥åŠ¨æ€é…ç½®
   self.retry_policy = (
       (retry_policy,) if isinstance(retry_policy, RetryPolicy) 
       else retry_policy
   )
   ```

3. **ä»»åŠ¡ä¿ç•™é€šé“è®¾è®¡**ï¼š
   ```python
   # TASKSé€šé“æ˜¯ç³»ç»Ÿä¿ç•™çš„ï¼Œç”¨äºå¤„ç†Sendæ¶ˆæ¯
   if TASKS in self.channels and not isinstance(self.channels[TASKS], Topic):
       raise ValueError(f"Channel '{TASKS}' is reserved")
   else:
       self.channels[TASKS] = Topic(Send, accumulate=False)
   ```

### 2.2 æ ¸å¿ƒæ‰§è¡Œç®—æ³•ï¼šprepare_next_tasks

**ä»»åŠ¡å‡†å¤‡çš„æ ¸å¿ƒé€»è¾‘** (`pregel/_algo.py:370-491`)ï¼š

```python
def prepare_next_tasks(
    checkpoint: Checkpoint,
    pending_writes: list[PendingWrite],
    processes: Mapping[str, PregelNode],
    channels: Mapping[str, BaseChannel],
    managed: ManagedValueMapping,
    config: RunnableConfig,
    step: int,
    stop: int,
    *,
    for_execution: bool,
    trigger_to_nodes: Mapping[str, Sequence[str]] | None = None,
    updated_channels: set[str] | None = None,
    **kwargs
) -> dict[str, PregelTask] | dict[str, PregelExecutableTask]:
    """å‡†å¤‡ä¸‹ä¸€ä¸ªPregelæ­¥éª¤çš„ä»»åŠ¡é›†åˆ"""
    
    tasks: list[PregelTask | PregelExecutableTask] = []
    
    # 1. å¤„ç†å¾…æ‰§è¡Œä»»åŠ¡ï¼ˆPUSHç±»å‹ï¼‰
    tasks_channel = cast(Optional[Topic[Send]], channels.get(TASKS))
    if tasks_channel and tasks_channel.is_available():
        for idx, _ in enumerate(tasks_channel.get()):
            if task := prepare_single_task(
                (PUSH, idx), None, checkpoint=checkpoint, ...
            ):
                tasks.append(task)
    
    # 2. æ™ºèƒ½èŠ‚ç‚¹é€‰æ‹©ä¼˜åŒ–
    if updated_channels and trigger_to_nodes:
        # åªæ¿€æ´»å—å½±å“çš„èŠ‚ç‚¹ï¼ˆé‡è¦ä¼˜åŒ–ï¼ï¼‰
        triggered_nodes: set[str] = set()
        for channel in updated_channels:
            if node_ids := trigger_to_nodes.get(channel):
                triggered_nodes.update(node_ids)
        candidate_nodes = sorted(triggered_nodes)  # ç¡®ä¿ç¡®å®šæ€§é¡ºåº
    elif not checkpoint["channel_versions"]:
        candidate_nodes = ()  # é¦–æ¬¡æ‰§è¡Œï¼Œæ— å€™é€‰èŠ‚ç‚¹
    else:
        candidate_nodes = processes.keys()  # æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹
    
    # 3. ä¸ºæ¯ä¸ªå€™é€‰èŠ‚ç‚¹å‡†å¤‡ä»»åŠ¡ï¼ˆPULLç±»å‹ï¼‰
    for name in candidate_nodes:
        if task := prepare_single_task(
            (PULL, name), None, checkpoint=checkpoint, ...
        ):
            tasks.append(task)
    
    return {t.id: t for t in tasks}
```

**ç®—æ³•è®¾è®¡çš„å…³é”®æ´å¯Ÿ**ï¼š

1. **PUSH vs PULLä»»åŠ¡æ¨¡å¼**ï¼š
   ```python
   # PUSHä»»åŠ¡ï¼šä¸»åŠ¨å‘é€çš„æ¶ˆæ¯ï¼ˆSendå¯¹è±¡ï¼‰
   # PULLä»»åŠ¡ï¼šè¢«åŠ¨æ¿€æ´»çš„èŠ‚ç‚¹ï¼ˆè¾¹è§¦å‘ï¼‰
   
   # è¿™ç§è®¾è®¡æ”¯æŒä¸¤ç§ä¸åŒçš„æ¿€æ´»æ¨¡å¼ï¼š
   # - äº‹ä»¶é©±åŠ¨ï¼šé€šè¿‡Sendä¸»åŠ¨è§¦å‘
   # - æ•°æ®é©±åŠ¨ï¼šé€šè¿‡çŠ¶æ€å˜åŒ–è¢«åŠ¨è§¦å‘
   ```

2. **æ™ºèƒ½èŠ‚ç‚¹é€‰æ‹©ä¼˜åŒ–**ï¼š
   ```python
   # å…³é”®ä¼˜åŒ–ï¼šåªæ£€æŸ¥å¯èƒ½è¢«æ¿€æ´»çš„èŠ‚ç‚¹
   # è€Œä¸æ˜¯éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œè¿™å¤§å¤§æé«˜äº†å¤§å›¾çš„æ€§èƒ½
   
   if updated_channels and trigger_to_nodes:
       # O(æ›´æ–°é€šé“æ•°) è€Œä¸æ˜¯ O(èŠ‚ç‚¹æ€»æ•°)
       triggered_nodes = get_affected_nodes(updated_channels)
   ```

3. **ç¡®å®šæ€§æ‰§è¡Œä¿è¯**ï¼š
   ```python
   # ç¡®ä¿ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º
   candidate_nodes = sorted(triggered_nodes)
   ```

### 2.3 PregelLoopï¼šæ‰§è¡Œå¾ªç¯çš„å¿ƒè„

**æ‰§è¡Œå¾ªç¯æ¶æ„** (`pregel/_loop.py:137-883`)ï¼š

```python
class PregelLoop:
    """Pregelæ‰§è¡Œå¾ªç¯çš„æ ¸å¿ƒå®ç°"""
    
    def __init__(self, pregel: Pregel, input: Any, config: RunnableConfig, ...):
        # çŠ¶æ€ç®¡ç†
        self.checkpoint: Checkpoint = ...           # å½“å‰æ£€æŸ¥ç‚¹
        self.tasks: dict[str, PregelTask] = {}      # å½“å‰ä»»åŠ¡é›†
        self.channels: dict[str, BaseChannel] = ... # é€šé“çŠ¶æ€
        
        # æ‰§è¡Œæ§åˆ¶
        self.step: int = 0                          # å½“å‰æ­¥æ•°
        self.stop: int = config.get("recursion_limit", 25)  # æœ€å¤§æ­¥æ•°
        self.status: PregelStatus = "pending"       # æ‰§è¡ŒçŠ¶æ€
        
        # æ€§èƒ½ä¼˜åŒ–
        self.updated_channels: set[str] = set()     # æ›´æ–°çš„é€šé“
        self.skip_done_tasks: bool = True           # è·³è¿‡å·²å®Œæˆä»»åŠ¡
        
        # é›†æˆç»„ä»¶
        self.checkpointer = pregel.checkpointer     # æ£€æŸ¥ç‚¹ä¿å­˜å™¨
        self.cache = pregel.cache                   # ç¼“å­˜ç³»ç»Ÿ
        self.retry_policy = pregel.retry_policy     # é‡è¯•ç­–ç•¥
    
    def tick(self) -> PregelLoop:
        """æ‰§è¡Œä¸€ä¸ªPregelæ­¥éª¤ï¼ˆè¶…å¾ªç¯ï¼‰"""
        
        # 1. å‡†å¤‡ä»»åŠ¡
        self.tasks = prepare_next_tasks(
            checkpoint=self.checkpoint,
            pending_writes=self.checkpoint_pending_writes,
            processes=self.nodes,
            channels=self.channels,
            managed=self.managed,
            config=self.config,
            step=self.step,
            stop=self.stop,
            for_execution=True,
            updated_channels=self.updated_channels,
            trigger_to_nodes=self.trigger_to_nodes,
            ...
        )
        
        # 2. æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸­æ–­
        if should_interrupt(
            self.checkpoint,
            self.interrupt_before,
            self.tasks,
        ):
            self.status = "interrupt_before"
            return self
        
        # 3. æäº¤ä»»åŠ¡æ‰§è¡Œ
        futures = {
            task.id: self.submit(task, self.retry_policy, self.step)
            for task in self.tasks.values()
        }
        
        # 4. æ”¶é›†æ‰§è¡Œç»“æœ
        # ... ä»»åŠ¡ç»“æœæ”¶é›†å’ŒçŠ¶æ€æ›´æ–°é€»è¾‘
        
        return self
```

**æ‰§è¡Œå¾ªç¯çš„å·¥ç¨‹ä¼˜åŠ¿**ï¼š

1. **å¢é‡çŠ¶æ€æ›´æ–°**ï¼š
   ```python
   # åªè·Ÿè¸ªå‘ç”Ÿå˜åŒ–çš„é€šé“ï¼Œé¿å…å…¨é‡æ‰«æ
   self.updated_channels: set[str] = set()
   
   # åœ¨ä»»åŠ¡æ‰§è¡Œåæ›´æ–°
   for task_id, result in task_results.items():
       if result.updated_channels:
           self.updated_channels.update(result.updated_channels)
   ```

2. **æ™ºèƒ½ä¸­æ–­æœºåˆ¶**ï¼š
   ```python
   # æ”¯æŒåœ¨èŠ‚ç‚¹æ‰§è¡Œå‰åè¿›è¡Œä¸­æ–­
   if should_interrupt(checkpoint, interrupt_before, tasks):
       return PregelLoop.with_status("interrupt_before")
   
   # ä¸­æ–­åå¯ä»¥ä»ä»»æ„ç‚¹æ¢å¤
   if should_interrupt(checkpoint, interrupt_after, completed_tasks):
       return PregelLoop.with_status("interrupt_after")
   ```

3. **ä»»åŠ¡çº§åˆ«çš„é‡è¯•ç­–ç•¥**ï¼š
   ```python
   # ä¸ºæ¯ä¸ªä»»åŠ¡åº”ç”¨é‡è¯•ç­–ç•¥
   futures = {
       task.id: self.submit(task, self.retry_policy, self.step)
       for task in self.tasks.values()
   }
   ```

---

## âš¡ å¹¶å‘æ§åˆ¶ä¸æ€§èƒ½ä¼˜åŒ–

### 3.1 ä»»åŠ¡è°ƒåº¦ä¸å¹¶å‘æ§åˆ¶

**æ™ºèƒ½ä»»åŠ¡æäº¤ç­–ç•¥**ï¼š
```python
class PregelExecutor:
    """Pregelä»»åŠ¡æ‰§è¡Œå™¨"""
    
    def submit_tasks(self, tasks: dict[str, PregelExecutableTask]) -> dict[str, Future]:
        """æ™ºèƒ½ä»»åŠ¡æäº¤ï¼Œè€ƒè™‘ä¾èµ–å…³ç³»å’Œèµ„æºé™åˆ¶"""
        
        # 1. ä¾èµ–åˆ†æ
        dependency_graph = self._analyze_dependencies(tasks)
        
        # 2. æ‹“æ‰‘æ’åº
        execution_order = self._topological_sort(dependency_graph)
        
        # 3. å¹¶å‘åº¦æ§åˆ¶
        max_concurrent = min(len(tasks), self.max_workers)
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # 4. æ‰¹é‡æäº¤
        futures = {}
        for batch in self._create_batches(execution_order):
            batch_futures = {
                task.id: self._submit_with_semaphore(task, semaphore)
                for task in batch
            }
            futures.update(batch_futures)
        
        return futures
    
    async def _submit_with_semaphore(self, task: PregelExecutableTask, semaphore: asyncio.Semaphore):
        """å¸¦ä¿¡å·é‡æ§åˆ¶çš„ä»»åŠ¡æäº¤"""
        async with semaphore:
            try:
                # åº”ç”¨é‡è¯•ç­–ç•¥
                result = await self._execute_with_retry(task)
                return result
            except Exception as e:
                # é”™è¯¯å¤„ç†å’Œä¸ŠæŠ¥
                self._handle_task_error(task, e)
                raise
```

**æ€§èƒ½ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯**ï¼š

1. **å†™æ—¶å¤åˆ¶ï¼ˆCopy-on-Writeï¼‰çŠ¶æ€ç®¡ç†**ï¼š
   ```python
   def apply_writes(
       checkpoint: Checkpoint,
       writes: Sequence[tuple[str, Any]],
       channels: Mapping[str, BaseChannel]
   ) -> tuple[Checkpoint, dict[str, Any]]:
       """é«˜æ•ˆçš„çŠ¶æ€æ›´æ–°ï¼Œåªå¤åˆ¶å‘ç”Ÿå˜åŒ–çš„éƒ¨åˆ†"""
       
       # 1. å»¶è¿Ÿå¤åˆ¶ï¼šåªåœ¨éœ€è¦æ—¶æ‰å¤åˆ¶çŠ¶æ€
       new_versions: dict[str, Any] = {}
       new_values: dict[str, Any] = {}
       
       for channel_name, value in writes:
           if channel_name in channels:
               channel = channels[channel_name]
               # åªæ›´æ–°çœŸæ­£å˜åŒ–çš„é€šé“
               if channel.update([value]):  # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
                   new_versions[channel_name] = generate_new_version()
                   new_values[channel_name] = channel.get()
       
       # 2. å¢é‡æ£€æŸ¥ç‚¹ï¼šåªåŒ…å«å˜æ›´
       if new_versions:
           new_checkpoint = {
               **checkpoint,
               "channel_versions": {**checkpoint["channel_versions"], **new_versions},
               "updated_channels": list(new_versions.keys())
           }
           return new_checkpoint, new_values
       
       return checkpoint, {}
   ```

2. **æ™ºèƒ½ç¼“å­˜ç­–ç•¥**ï¼š
   ```python
   class PregelCache:
       """Pregelæ‰§è¡Œçš„æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
       
       def __init__(self, cache_policy: CachePolicy):
           self.policy = cache_policy
           self.task_cache: dict[str, Any] = {}
           self.state_cache: dict[str, Any] = {}
       
       async def get_cached_result(self, task: PregelExecutableTask) -> Any | None:
           """è·å–ç¼“å­˜çš„ä»»åŠ¡ç»“æœ"""
           
           # 1. è®¡ç®—ä»»åŠ¡æŒ‡çº¹
           task_fingerprint = self._compute_task_fingerprint(task)
           
           # 2. æ£€æŸ¥ç¼“å­˜ç­–ç•¥
           if not self.policy.should_cache(task):
               return None
           
           # 3. æŸ¥æ‰¾ç¼“å­˜
           if result := self.task_cache.get(task_fingerprint):
               # éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
               if self._is_cache_valid(result, task):
                   return result
           
           return None
       
       def _compute_task_fingerprint(self, task: PregelExecutableTask) -> str:
           """è®¡ç®—ä»»åŠ¡çš„å”¯ä¸€æŒ‡çº¹"""
           # è€ƒè™‘ä»»åŠ¡è¾“å…¥ã€èŠ‚ç‚¹ä»£ç ç‰ˆæœ¬ã€ä¾èµ–çŠ¶æ€ç­‰
           components = [
               task.node.name,
               hash(task.input),
               task.node.code_version,  # èŠ‚ç‚¹ä»£ç çš„ç‰ˆæœ¬æ ‡è¯†
               hash(frozenset(task.dependencies.items()))
           ]
           return hashlib.sha256(str(components).encode()).hexdigest()
   ```

### 3.2 å†…å­˜ç®¡ç†ä¸èµ„æºä¼˜åŒ–

**å¤§çŠ¶æ€å¯¹è±¡çš„ä¼˜åŒ–å¤„ç†**ï¼š
```python
class ChannelManager:
    """é€šé“çŠ¶æ€çš„é«˜æ•ˆç®¡ç†"""
    
    def __init__(self):
        self.channels: dict[str, BaseChannel] = {}
        self.lazy_channels: dict[str, LazyChannel] = {}  # å»¶è¿ŸåŠ è½½
        self.compressed_channels: dict[str, CompressedChannel] = {}  # å‹ç¼©å­˜å‚¨
    
    def get_channel_value(self, name: str, checkpoint_id: str) -> Any:
        """æŒ‰éœ€åŠ è½½é€šé“å€¼"""
        
        # 1. æ£€æŸ¥çƒ­ç¼“å­˜
        if channel := self.channels.get(name):
            return channel.get()
        
        # 2. æ£€æŸ¥å»¶è¿Ÿé€šé“
        if lazy_channel := self.lazy_channels.get(name):
            value = lazy_channel.load(checkpoint_id)
            # æå‡åˆ°çƒ­ç¼“å­˜
            self.channels[name] = self._create_channel(name, value)
            return value
        
        # 3. ä»å‹ç¼©å­˜å‚¨åŠ è½½
        if compressed := self.compressed_channels.get(name):
            value = compressed.decompress(checkpoint_id)
            self.lazy_channels[name] = LazyChannel(value)
            return value
        
        raise KeyError(f"Channel {name} not found")
    
    def optimize_memory(self, memory_threshold: float = 0.8):
        """æ™ºèƒ½å†…å­˜ä¼˜åŒ–"""
        
        current_memory = psutil.virtual_memory().percent / 100.0
        if current_memory < memory_threshold:
            return
        
        # 1. å‹ç¼©ä¸å¸¸ç”¨é€šé“
        for name, channel in list(self.channels.items()):
            if not channel.recently_accessed():
                compressed = CompressedChannel.from_channel(channel)
                self.compressed_channels[name] = compressed
                del self.channels[name]
        
        # 2. æ¸…ç†è¿‡æœŸçš„å»¶è¿Ÿé€šé“
        for name, lazy_channel in list(self.lazy_channels.items()):
            if lazy_channel.is_expired():
                del self.lazy_channels[name]
        
        # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
```

**æµå¼å¤„ç†çš„å†…å­˜ä¼˜åŒ–**ï¼š
```python
class StreamProcessor:
    """æµå¼å¤„ç†çš„å†…å­˜ä¼˜åŒ–å®ç°"""
    
    def __init__(self, buffer_size: int = 1000):
        self.buffer_size = buffer_size
        self.buffer: collections.deque = collections.deque(maxlen=buffer_size)
    
    async def stream_process(self, pregel_loop: PregelLoop) -> AsyncIterator[Any]:
        """å†…å­˜é«˜æ•ˆçš„æµå¼å¤„ç†"""
        
        while pregel_loop.status == "pending":
            # 1. æ‰§è¡Œä¸€ä¸ªæ­¥éª¤
            pregel_loop = pregel_loop.tick()
            
            # 2. æå–è¾“å‡ºï¼ˆä¸ä¿ç•™å†å²ï¼‰
            if output := pregel_loop.output:
                yield output
                # ç«‹å³æ¸…ç†è¾“å‡ºï¼Œé‡Šæ”¾å†…å­˜
                pregel_loop.output = None
            
            # 3. æ£€æŸ¥å†…å­˜å‹åŠ›
            if self._should_checkpoint(pregel_loop):
                # åœ¨å†…å­˜å‹åŠ›å¤§æ—¶ä¸»åŠ¨åˆ›å»ºæ£€æŸ¥ç‚¹
                await self._force_checkpoint(pregel_loop)
                # æ¸…ç†å†…å­˜ä¸­çš„å†å²çŠ¶æ€
                pregel_loop.cleanup_history()
            
            # 4. è‡ªé€‚åº”æ‰¹å¤„ç†
            if self._should_batch(pregel_loop):
                # æ‰¹é‡å¤„ç†å¤šä¸ªæ­¥éª¤ä»¥æé«˜æ•ˆç‡
                yield from self._batch_process(pregel_loop)
    
    def _should_checkpoint(self, loop: PregelLoop) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸»åŠ¨åˆ›å»ºæ£€æŸ¥ç‚¹"""
        return (
            loop.step % 10 == 0 or  # å®šæœŸæ£€æŸ¥ç‚¹
            psutil.virtual_memory().percent > 80 or  # å†…å­˜å‹åŠ›
            len(loop.checkpoint_pending_writes) > 100  # å¾…å†™å…¥è¿‡å¤š
        )
```

---

## ğŸ”§ é«˜çº§ç‰¹æ€§ä¸æ‰©å±•æœºåˆ¶

### 4.1 ä¸­æ–­ä¸æ¢å¤æœºåˆ¶

**ç²¾ç»†åŒ–ä¸­æ–­æ§åˆ¶**ï¼š
```python
class InterruptManager:
    """ä¸­æ–­ç®¡ç†å™¨ï¼šæ”¯æŒç²¾ç»†åŒ–çš„æ‰§è¡Œæ§åˆ¶"""
    
    def __init__(self, 
                 interrupt_before: Sequence[str] = (),
                 interrupt_after: Sequence[str] = ()):
        self.interrupt_before = set(interrupt_before)
        self.interrupt_after = set(interrupt_after)
        self.interrupt_handlers: dict[str, Callable] = {}
    
    def should_interrupt_before(self, tasks: dict[str, PregelTask]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”åœ¨æ‰§è¡Œå‰ä¸­æ–­"""
        for task in tasks.values():
            if task.node_name in self.interrupt_before:
                return True
        return False
    
    def should_interrupt_after(self, completed_tasks: dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”åœ¨æ‰§è¡Œåä¸­æ–­"""
        for task_id, result in completed_tasks.items():
            if result.node_name in self.interrupt_after:
                return True
        return False
    
    def register_interrupt_handler(self, node_name: str, handler: Callable):
        """æ³¨å†Œä¸­æ–­å¤„ç†å™¨"""
        self.interrupt_handlers[node_name] = handler
    
    async def handle_interrupt(self, interrupt_type: str, context: dict) -> dict:
        """å¤„ç†ä¸­æ–­äº‹ä»¶"""
        node_name = context.get("node_name")
        if handler := self.interrupt_handlers.get(node_name):
            return await handler(interrupt_type, context)
        
        # é»˜è®¤å¤„ç†ï¼šä¿å­˜çŠ¶æ€å¹¶ç­‰å¾…ç”¨æˆ·å¹²é¢„
        return {
            "action": "pause",
            "checkpoint_id": context["checkpoint_id"],
            "message": f"Interrupted at {node_name}, awaiting user action"
        }
```

**å¯æ¢å¤çš„é•¿æ—¶é—´è¿è¡Œä»»åŠ¡**ï¼š
```python
class ResumableTask:
    """å¯æ¢å¤çš„é•¿æ—¶é—´è¿è¡Œä»»åŠ¡"""
    
    def __init__(self, task_id: str, checkpointer: BaseCheckpointSaver):
        self.task_id = task_id
        self.checkpointer = checkpointer
        self.progress_markers: list[str] = []
    
    async def execute_resumable(self, 
                               work_function: Callable,
                               inputs: dict,
                               resume_from: str | None = None) -> Any:
        """å¯æ¢å¤åœ°æ‰§è¡Œé•¿æ—¶é—´ä»»åŠ¡"""
        
        # 1. æ¢å¤ä¹‹å‰çš„è¿›åº¦
        if resume_from:
            checkpoint = await self.checkpointer.aget({"task_id": self.task_id})
            if checkpoint:
                self.progress_markers = checkpoint.get("progress_markers", [])
                inputs = checkpoint.get("current_inputs", inputs)
        
        try:
            # 2. åˆ†é˜¶æ®µæ‰§è¡Œ
            for phase in self._get_execution_phases():
                if resume_from and phase in self.progress_markers:
                    continue  # è·³è¿‡å·²å®Œæˆçš„é˜¶æ®µ
                
                # æ‰§è¡Œå½“å‰é˜¶æ®µ
                result = await self._execute_phase(phase, work_function, inputs)
                
                # è®°å½•è¿›åº¦
                self.progress_markers.append(phase)
                await self._save_progress(inputs, result)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
                if self._should_pause():
                    return {"status": "paused", "resume_from": phase}
            
            return result
            
        except Exception as e:
            # ä¿å­˜é”™è¯¯çŠ¶æ€ä»¥ä¾¿è°ƒè¯•
            await self._save_error_state(e, inputs)
            raise
    
    async def _save_progress(self, inputs: dict, partial_result: Any):
        """ä¿å­˜æ‰§è¡Œè¿›åº¦"""
        progress_checkpoint = {
            "task_id": self.task_id,
            "progress_markers": self.progress_markers,
            "current_inputs": inputs,
            "partial_result": partial_result,
            "timestamp": datetime.now().isoformat()
        }
        
        await self.checkpointer.aput(
            {"task_id": self.task_id},
            progress_checkpoint,
            {"phase": "progress_save"}
        )
```

### 4.2 è‡ªå®šä¹‰èŠ‚ç‚¹ä¸æ‰©å±•

**é«˜çº§èŠ‚ç‚¹ç±»å‹çš„å®ç°**ï¼š
```python
class ConditionalNode(PregelNode):
    """æ¡ä»¶èŠ‚ç‚¹ï¼šæ ¹æ®çŠ¶æ€åŠ¨æ€å†³å®šæ‰§è¡Œé€»è¾‘"""
    
    def __init__(self, 
                 condition_func: Callable[[Any], str],
                 execution_map: dict[str, Callable]):
        self.condition_func = condition_func
        self.execution_map = execution_map
    
    async def arun(self, input: Any, config: RunnableConfig) -> Any:
        """å¼‚æ­¥æ‰§è¡Œæ¡ä»¶é€»è¾‘"""
        
        # 1. è¯„ä¼°æ¡ä»¶
        condition_result = await self._safe_evaluate_condition(input)
        
        # 2. é€‰æ‹©æ‰§è¡Œå‡½æ•°
        if execution_func := self.execution_map.get(condition_result):
            return await self._safe_execute(execution_func, input, config)
        
        # 3. é»˜è®¤è¡Œä¸º
        return self._handle_unknown_condition(condition_result, input)
    
    async def _safe_evaluate_condition(self, input: Any) -> str:
        """å®‰å…¨åœ°è¯„ä¼°æ¡ä»¶"""
        try:
            if asyncio.iscoroutinefunction(self.condition_func):
                return await self.condition_func(input)
            else:
                return self.condition_func(input)
        except Exception as e:
            logger.error(f"Condition evaluation failed: {e}")
            return "error"

class ParallelNode(PregelNode):
    """å¹¶è¡ŒèŠ‚ç‚¹ï¼šåŒæ—¶æ‰§è¡Œå¤šä¸ªå­ä»»åŠ¡"""
    
    def __init__(self, 
                 subtasks: list[Callable],
                 aggregation_func: Callable[[list], Any],
                 max_concurrent: int = 5):
        self.subtasks = subtasks
        self.aggregation_func = aggregation_func
        self.max_concurrent = max_concurrent
    
    async def arun(self, input: Any, config: RunnableConfig) -> Any:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå­ä»»åŠ¡"""
        
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def execute_subtask(subtask: Callable) -> Any:
            async with semaphore:
                try:
                    return await self._safe_execute(subtask, input, config)
                except Exception as e:
                    return {"error": str(e), "subtask": subtask.__name__}
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å­ä»»åŠ¡
        results = await asyncio.gather(
            *[execute_subtask(subtask) for subtask in self.subtasks],
            return_exceptions=True
        )
        
        # èšåˆç»“æœ
        return self.aggregation_func(results)

class RetryNode(PregelNode):
    """é‡è¯•èŠ‚ç‚¹ï¼šå¸¦æœ‰è‡ªå®šä¹‰é‡è¯•é€»è¾‘çš„èŠ‚ç‚¹"""
    
    def __init__(self, 
                 base_func: Callable,
                 retry_policy: RetryPolicy,
                 fallback_func: Callable | None = None):
        self.base_func = base_func
        self.retry_policy = retry_policy
        self.fallback_func = fallback_func
    
    async def arun(self, input: Any, config: RunnableConfig) -> Any:
        """å¸¦é‡è¯•çš„æ‰§è¡Œ"""
        
        last_exception = None
        
        for attempt in range(self.retry_policy.max_attempts):
            try:
                return await self._safe_execute(self.base_func, input, config)
            
            except Exception as e:
                last_exception = e
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self.retry_policy.should_retry(e, attempt):
                    break
                
                # ç­‰å¾…é‡è¯•é—´éš”
                await asyncio.sleep(self.retry_policy.get_delay(attempt))
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œå°è¯•fallback
        if self.fallback_func:
            try:
                return await self._safe_execute(self.fallback_func, input, config)
            except Exception as fallback_error:
                logger.error(f"Fallback also failed: {fallback_error}")
        
        # æœ€ç»ˆå¤±è´¥
        raise last_exception
```

---

## ğŸ“Š æ€§èƒ½åˆ†æä¸è°ƒä¼˜å®æˆ˜

### 5.1 æ‰§è¡Œæ€§èƒ½åˆ†æ

**Pregelæ‰§è¡Œçš„æ€§èƒ½æŒ‡æ ‡**ï¼š
```python
class PregelProfiler:
    """Pregelæ‰§è¡Œæ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.step_times: list[float] = []
        self.task_times: dict[str, list[float]] = {}
        self.memory_usage: list[float] = []
        self.channel_sizes: dict[str, list[int]] = {}
        
    def profile_execution(self, pregel_loop: PregelLoop) -> dict:
        """å…¨é¢çš„æ‰§è¡Œæ€§èƒ½åˆ†æ"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œæ€§èƒ½ç›‘æ§
        with self._monitor_execution():
            while pregel_loop.status == "pending":
                step_start = time.time()
                
                # è®°å½•æ­¥éª¤å‰çŠ¶æ€
                self._record_pre_step_metrics(pregel_loop)
                
                # æ‰§è¡Œä¸€æ­¥
                pregel_loop = pregel_loop.tick()
                
                # è®°å½•æ­¥éª¤åçŠ¶æ€
                step_time = time.time() - step_start
                self.step_times.append(step_time)
                self._record_post_step_metrics(pregel_loop)
        
        total_time = time.time() - start_time
        peak_memory = max(self.memory_usage) if self.memory_usage else start_memory
        
        return {
            "total_time": total_time,
            "total_steps": len(self.step_times),
            "avg_step_time": sum(self.step_times) / len(self.step_times),
            "slowest_step": max(self.step_times),
            "peak_memory_mb": peak_memory,
            "memory_growth": peak_memory - start_memory,
            "task_performance": self._analyze_task_performance(),
            "bottlenecks": self._identify_bottlenecks(),
            "optimization_suggestions": self._generate_suggestions()
        }
    
    def _analyze_task_performance(self) -> dict:
        """åˆ†æä»»åŠ¡çº§åˆ«çš„æ€§èƒ½"""
        analysis = {}
        
        for task_name, times in self.task_times.items():
            if times:
                analysis[task_name] = {
                    "total_executions": len(times),
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times),
                    "std_dev": statistics.stdev(times) if len(times) > 1 else 0
                }
        
        return analysis
    
    def _identify_bottlenecks(self) -> list[dict]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # 1. æ…¢æ­¥éª¤è¯†åˆ«
        if self.step_times:
            avg_step_time = sum(self.step_times) / len(self.step_times)
            slow_steps = [
                {"step": i, "time": time_val, "slowdown_factor": time_val / avg_step_time}
                for i, time_val in enumerate(self.step_times)
                if time_val > avg_step_time * 2  # è¶…è¿‡å¹³å‡å€¼2å€çš„æ­¥éª¤
            ]
            if slow_steps:
                bottlenecks.extend(slow_steps)
        
        # 2. å†…å­˜å¢é•¿è¯†åˆ«
        if len(self.memory_usage) > 1:
            memory_growth_rate = (self.memory_usage[-1] - self.memory_usage[0]) / len(self.memory_usage)
            if memory_growth_rate > 10:  # æ¯æ­¥è¶…è¿‡10MBå¢é•¿
                bottlenecks.append({
                    "type": "memory_leak",
                    "growth_rate_mb_per_step": memory_growth_rate
                })
        
        # 3. ä»»åŠ¡æ€§èƒ½å¼‚å¸¸
        for task_name, analysis in self._analyze_task_performance().items():
            if analysis["std_dev"] > analysis["avg_time"] * 0.5:  # æ ‡å‡†å·®è¿‡å¤§
                bottlenecks.append({
                    "type": "task_variance",
                    "task": task_name,
                    "variance_ratio": analysis["std_dev"] / analysis["avg_time"]
                })
        
        return bottlenecks
```

### 5.2 æ™ºèƒ½ä¼˜åŒ–å»ºè®®

**è‡ªåŠ¨ä¼˜åŒ–å»ºè®®ç”Ÿæˆ**ï¼š
```python
class PregelOptimizer:
    """Pregelæ‰§è¡Œçš„æ™ºèƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, profiler: PregelProfiler):
        self.profiler = profiler
        self.optimization_rules = self._load_optimization_rules()
    
    def generate_optimization_plan(self, profile_data: dict) -> dict:
        """ç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        
        plan = {
            "immediate_actions": [],    # ç«‹å³å¯æ‰§è¡Œçš„ä¼˜åŒ–
            "structural_changes": [],   # éœ€è¦ä»£ç ä¿®æ”¹çš„ä¼˜åŒ–
            "infrastructure_upgrades": [], # éœ€è¦åŸºç¡€è®¾æ–½å‡çº§çš„ä¼˜åŒ–
            "monitoring_recommendations": [] # ç›‘æ§å»ºè®®
        }
        
        # 1. åŸºäºæ€§èƒ½æ•°æ®çš„ä¼˜åŒ–å»ºè®®
        self._analyze_performance_patterns(profile_data, plan)
        
        # 2. åŸºäºèµ„æºä½¿ç”¨çš„ä¼˜åŒ–å»ºè®®
        self._analyze_resource_usage(profile_data, plan)
        
        # 3. åŸºäºä»»åŠ¡åˆ†å¸ƒçš„ä¼˜åŒ–å»ºè®®
        self._analyze_task_distribution(profile_data, plan)
        
        return plan
    
    def _analyze_performance_patterns(self, profile_data: dict, plan: dict):
        """åˆ†ææ€§èƒ½æ¨¡å¼å¹¶æä¾›å»ºè®®"""
        
        avg_step_time = profile_data.get("avg_step_time", 0)
        
        # æ…¢æ­¥éª¤ä¼˜åŒ–
        if avg_step_time > 5.0:  # æ­¥éª¤å¹³å‡è¶…è¿‡5ç§’
            plan["immediate_actions"].append({
                "type": "enable_caching",
                "description": "å¯ç”¨ä»»åŠ¡ç»“æœç¼“å­˜ä»¥å‡å°‘é‡å¤è®¡ç®—",
                "expected_improvement": "30-50%æ€§èƒ½æå‡",
                "implementation": """
                # æ·»åŠ ç¼“å­˜é…ç½®
                from langgraph.cache import MemoryCache
                
                cache = MemoryCache(ttl=3600)  # 1å°æ—¶ç¼“å­˜
                app = graph.compile(checkpointer=checkpointer, cache=cache)
                """
            })
        
        # å¹¶å‘ä¼˜åŒ–
        if profile_data.get("total_steps", 0) > 20:
            plan["structural_changes"].append({
                "type": "increase_parallelism",
                "description": "å¢åŠ å¹¶è¡ŒèŠ‚ç‚¹æ‰§è¡Œä»¥æé«˜ååé‡",
                "expected_improvement": "2-3å€æ€§èƒ½æå‡",
                "implementation": """
                # è¯†åˆ«å¯å¹¶è¡Œçš„èŠ‚ç‚¹
                # ä½¿ç”¨trigger_to_nodesä¼˜åŒ–èŠ‚ç‚¹é€‰æ‹©
                trigger_mapping = {
                    "input_channel": ["node1", "node2"],  # å¹¶è¡Œæ‰§è¡Œ
                    "analysis_complete": ["report_node", "notification_node"]
                }
                """
            })
        
        # å†…å­˜ä¼˜åŒ–
        memory_growth = profile_data.get("memory_growth", 0)
        if memory_growth > 100:  # å†…å­˜å¢é•¿è¶…è¿‡100MB
            plan["immediate_actions"].append({
                "type": "memory_optimization",
                "description": "å¯ç”¨æµå¼å¤„ç†å‡å°‘å†…å­˜å ç”¨",
                "expected_improvement": "60-80%å†…å­˜å‡å°‘",
                "implementation": """
                # ä½¿ç”¨æµå¼æ¨¡å¼
                for chunk in app.stream(input_data, config):
                    process_chunk(chunk)  # ç«‹å³å¤„ç†ï¼Œä¸ç´¯ç§¯
                """
            })
    
    def apply_optimization(self, optimization: dict, pregel: Pregel) -> Pregel:
        """åº”ç”¨ä¼˜åŒ–å»ºè®®"""
        
        optimization_type = optimization["type"]
        
        if optimization_type == "enable_caching":
            return self._enable_caching(pregel, optimization)
        elif optimization_type == "increase_parallelism":
            return self._increase_parallelism(pregel, optimization)
        elif optimization_type == "memory_optimization":
            return self._optimize_memory(pregel, optimization)
        
        return pregel
    
    def _enable_caching(self, pregel: Pregel, config: dict) -> Pregel:
        """å¯ç”¨æ™ºèƒ½ç¼“å­˜"""
        from langgraph.cache import MemoryCache
        
        # åˆ›å»ºç¼“å­˜å®ä¾‹
        cache = MemoryCache(
            ttl=config.get("ttl", 3600),
            max_size=config.get("max_size", 1000)
        )
        
        # è¿”å›å¸¦ç¼“å­˜çš„æ–°å®ä¾‹
        return pregel.copy(cache=cache)
```

---

## ğŸ¯ å®æˆ˜æ¡ˆä¾‹ï¼šæ„å»ºé«˜æ€§èƒ½AIå·¥ä½œæµ

### 6.1 å¤æ‚å¤šä»£ç†ç³»ç»Ÿçš„Pregelå®ç°

```python
class AdvancedAIWorkflow:
    """é«˜æ€§èƒ½å¤šä»£ç†AIå·¥ä½œæµ"""
    
    def __init__(self):
        self.checkpointer = PostgresSaver.from_conn_string(
            "postgresql://user:pass@localhost/ai_workflow"
        )
        self.cache = RedisCache("redis://localhost:6379")
        
    def build_research_pipeline(self) -> Pregel:
        """æ„å»ºç ”ç©¶ç®¡é“"""
        
        class ResearchState(TypedDict):
            query: str
            research_tasks: list[dict]
            collected_data: list[dict]
            analysis_results: dict
            synthesis: str
            quality_score: float
        
        def task_distributor(state: ResearchState) -> ResearchState:
            """ä»»åŠ¡åˆ†å‘å™¨ï¼šå°†å¤§æŸ¥è¯¢åˆ†è§£ä¸ºå­ä»»åŠ¡"""
            query = state["query"]
            
            # ä½¿ç”¨LLMåˆ†è§£æŸ¥è¯¢
            tasks = llm.invoke(f"åˆ†è§£æŸ¥è¯¢ä¸ºå…·ä½“ç ”ç©¶ä»»åŠ¡: {query}")
            
            return {
                **state,
                "research_tasks": tasks,
            }
        
        def parallel_researcher(state: ResearchState) -> ResearchState:
            """å¹¶è¡Œç ”ç©¶å™¨ï¼šåŒæ—¶æ‰§è¡Œå¤šä¸ªç ”ç©¶ä»»åŠ¡"""
            tasks = state["research_tasks"]
            
            # å¹¶è¡Œæ‰§è¡Œç ”ç©¶ä»»åŠ¡
            async def research_single_task(task):
                # æ¨¡æ‹Ÿç ”ç©¶è¿‡ç¨‹
                result = await web_search.ainvoke(task["query"])
                return {"task": task, "result": result}
            
            # ä½¿ç”¨Pregelçš„å¹¶è¡Œèƒ½åŠ›
            results = asyncio.gather(*[
                research_single_task(task) for task in tasks
            ])
            
            return {
                **state,
                "collected_data": results,
            }
        
        def data_analyzer(state: ResearchState) -> ResearchState:
            """æ•°æ®åˆ†æå™¨ï¼šåˆ†ææ”¶é›†çš„æ•°æ®"""
            data = state["collected_data"]
            
            # åˆ†ææ•°æ®æ¨¡å¼å’Œå…³è”
            analysis = llm.invoke(f"åˆ†æç ”ç©¶æ•°æ®: {data}")
            
            return {
                **state,
                "analysis_results": analysis,
            }
        
        def synthesizer(state: ResearchState) -> ResearchState:
            """åˆæˆå™¨ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
            analysis = state["analysis_results"]
            
            synthesis = llm.invoke(f"åˆæˆç ”ç©¶æŠ¥å‘Š: {analysis}")
            
            return {
                **state,
                "synthesis": synthesis,
            }
        
        def quality_assessor(state: ResearchState) -> ResearchState:
            """è´¨é‡è¯„ä¼°å™¨ï¼šè¯„ä¼°ç»“æœè´¨é‡"""
            synthesis = state["synthesis"]
            
            quality_score = quality_model.invoke(synthesis)
            
            return {
                **state,
                "quality_score": quality_score,
            }
        
        # æ„å»ºå›¾
        workflow = StateGraph(ResearchState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("distribute", task_distributor)
        workflow.add_node("research", parallel_researcher)
        workflow.add_node("analyze", data_analyzer)
        workflow.add_node("synthesize", synthesizer)
        workflow.add_node("assess", quality_assessor)
        
        # æ·»åŠ è¾¹
        workflow.add_edge(START, "distribute")
        workflow.add_edge("distribute", "research")
        workflow.add_edge("research", "analyze")
        workflow.add_edge("analyze", "synthesize")
        workflow.add_edge("synthesize", "assess")
        
        # æ¡ä»¶è¾¹ï¼šè´¨é‡ä¸å¤Ÿæ—¶é‡æ–°åˆæˆ
        def should_improve(state: ResearchState) -> str:
            return "synthesize" if state["quality_score"] < 0.8 else END
        
        workflow.add_conditional_edges("assess", should_improve)
        
        # ç¼–è¯‘ä¸ºPregelæ‰§è¡Œå™¨
        return workflow.compile(
            checkpointer=self.checkpointer,
            cache=self.cache,
            interrupt_after_nodes=["assess"],  # å…è®¸äººå·¥å¹²é¢„
        )
    
    async def run_research(self, query: str) -> dict:
        """è¿è¡Œç ”ç©¶æµç¨‹"""
        
        app = self.build_research_pipeline()
        
        # é…ç½®æ‰§è¡Œå‚æ•°
        config = {
            "configurable": {
                "thread_id": f"research-{uuid.uuid4()}",
                "max_steps": 50,
                "timeout": 3600,  # 1å°æ—¶è¶…æ—¶
            }
        }
        
        # åˆå§‹çŠ¶æ€
        initial_state = {
            "query": query,
            "research_tasks": [],
            "collected_data": [],
            "analysis_results": {},
            "synthesis": "",
            "quality_score": 0.0,
        }
        
        # æµå¼æ‰§è¡Œ
        final_result = None
        async for chunk in app.astream(initial_state, config):
            print(f"æ­¥éª¤å®Œæˆ: {chunk}")
            final_result = chunk
        
        return final_result
```

### 6.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
class PregelBenchmark:
    """Pregelæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.test_configs = [
            {"nodes": 5, "steps": 10, "concurrency": 1},
            {"nodes": 10, "steps": 20, "concurrency": 2}, 
            {"nodes": 20, "steps": 50, "concurrency": 4},
            {"nodes": 50, "steps": 100, "concurrency": 8},
        ]
    
    async def run_benchmark_suite(self) -> dict:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        
        results = {}
        
        for config in self.test_configs:
            print(f"è¿è¡ŒåŸºå‡†æµ‹è¯•: {config}")
            
            # åˆ›å»ºæµ‹è¯•å›¾
            test_graph = self._create_test_graph(config)
            
            # è¿è¡Œæµ‹è¯•
            start_time = time.time()
            await self._run_test_graph(test_graph, config)
            execution_time = time.time() - start_time
            
            # æ”¶é›†æŒ‡æ ‡
            results[f"config_{config['nodes']}_{config['steps']}"] = {
                "execution_time": execution_time,
                "throughput": config["steps"] / execution_time,
                "memory_peak": self._measure_memory_peak(),
                "cpu_usage": self._measure_cpu_usage(),
            }
        
        return results
    
    def _create_test_graph(self, config: dict) -> Pregel:
        """åˆ›å»ºæµ‹è¯•å›¾"""
        
        class TestState(TypedDict):
            counter: int
            data: list[int]
        
        def compute_node(state: TestState) -> TestState:
            # æ¨¡æ‹Ÿè®¡ç®—å¯†é›†å‹ä»»åŠ¡
            import math
            result = sum(math.sqrt(i) for i in range(1000))
            
            return {
                "counter": state["counter"] + 1,
                "data": state["data"] + [result]
            }
        
        # åˆ›å»ºå›¾
        workflow = StateGraph(TestState)
        
        # æ·»åŠ è®¡ç®—èŠ‚ç‚¹
        for i in range(config["nodes"]):
            workflow.add_node(f"compute_{i}", compute_node)
        
        # è¿æ¥èŠ‚ç‚¹ï¼ˆçº¿æ€§é“¾ï¼‰
        workflow.add_edge(START, "compute_0")
        for i in range(config["nodes"] - 1):
            workflow.add_edge(f"compute_{i}", f"compute_{i+1}")
        workflow.add_edge(f"compute_{config['nodes']-1}", END)
        
        return workflow.compile()
    
    def analyze_benchmark_results(self, results: dict) -> dict:
        """åˆ†æåŸºå‡†æµ‹è¯•ç»“æœ"""
        
        analysis = {
            "scalability": {},
            "efficiency": {},
            "recommendations": []
        }
        
        # å¯æ‰©å±•æ€§åˆ†æ
        node_counts = []
        throughputs = []
        
        for config_name, metrics in results.items():
            parts = config_name.split("_")
            node_count = int(parts[1])
            throughput = metrics["throughput"]
            
            node_counts.append(node_count)
            throughputs.append(throughput)
        
        # è®¡ç®—å¯æ‰©å±•æ€§ç³»æ•°
        if len(throughputs) > 1:
            scalability_factor = throughputs[-1] / throughputs[0]
            node_factor = node_counts[-1] / node_counts[0]
            
            analysis["scalability"]["factor"] = scalability_factor / node_factor
            
            if scalability_factor / node_factor > 0.8:
                analysis["recommendations"].append("æ‰©å±•æ€§è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­å¢åŠ èŠ‚ç‚¹")
            else:
                analysis["recommendations"].append("å­˜åœ¨æ‰©å±•ç“¶é¢ˆï¼Œå»ºè®®ä¼˜åŒ–èŠ‚ç‚¹é€šä¿¡")
        
        return analysis
```

---

## ğŸ“ ç« èŠ‚æ€»ç»“ä¸æºç å­¦ä¹ ä»·å€¼

### æ ¸å¿ƒçŸ¥è¯†å›é¡¾

é€šè¿‡æ·±å…¥åˆ†æLangGraphçš„Pregelæ‰§è¡Œå¼•æ“ï¼Œä½ è·å¾—äº†ï¼š

**ğŸ§  ç®—æ³•æ€æƒ³ç†è§£**ï¼š
- âœ… **åˆ†å¸ƒå¼å›¾è®¡ç®—**ï¼šä»Google Pregelåˆ°AIå·¥ä½œæµçš„æ€æƒ³ä¼ æ‰¿
- âœ… **é¡¶ç‚¹ä¸­å¿ƒæ€ç»´**ï¼šæ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹å†³ç­–çš„è®¾è®¡å“²å­¦
- âœ… **æ¶ˆæ¯ä¼ é€’æ¨¡å‹**ï¼šçŠ¶æ€å’Œæ¶ˆæ¯çš„ä¼ é€’æœºåˆ¶
- âœ… **è‡ªé€‚åº”ç»ˆæ­¢**ï¼šåŸºäºå…¨å±€çŠ¶æ€çš„æ™ºèƒ½ç»ˆæ­¢æ¡ä»¶

**âš™ï¸ å·¥ç¨‹å®ç°æŠ€å·§**ï¼š
- âœ… **ä»»åŠ¡è°ƒåº¦ç®—æ³•**ï¼š`prepare_next_tasks`çš„æ™ºèƒ½èŠ‚ç‚¹é€‰æ‹©
- âœ… **å¹¶å‘æ§åˆ¶æœºåˆ¶**ï¼šåŒæ­¥å’Œå¼‚æ­¥çš„æ‰§è¡Œå¾ªç¯è®¾è®¡
- âœ… **çŠ¶æ€ç®¡ç†ä¼˜åŒ–**ï¼šå†™æ—¶å¤åˆ¶å’Œå¢é‡æ›´æ–°ç­–ç•¥
- âœ… **ä¸­æ–­æ¢å¤ç³»ç»Ÿ**ï¼šç²¾ç»†åŒ–çš„æ‰§è¡Œæ§åˆ¶å’Œæ¢å¤æœºåˆ¶

### æºç å­¦ä¹ çš„æ·±å±‚ä»·å€¼

**ğŸ—ï¸ ç³»ç»Ÿæ¶æ„èƒ½åŠ›**ï¼š
- ç†è§£å¦‚ä½•å°†å­¦æœ¯ç®—æ³•è½¬åŒ–ä¸ºå·¥ç¨‹å®ç°
- æŒæ¡å¤§å‹ç³»ç»Ÿçš„æ¨¡å—åŒ–è®¾è®¡åŸåˆ™
- å­¦ä¼šè®¾è®¡å¯æ‰©å±•çš„æ‰§è¡Œå¼•æ“æ¶æ„

**ğŸ’¡ ç®—æ³•å·¥ç¨‹åŒ–æŠ€èƒ½**ï¼š
- å­¦ä¼šä¼˜åŒ–ç®—æ³•çš„å®é™…å·¥ç¨‹è€ƒé‡
- ç†è§£æ€§èƒ½ç“¶é¢ˆçš„è¯†åˆ«å’Œè§£å†³æ–¹æ³•
- æŒæ¡å¹¶å‘æ§åˆ¶å’Œèµ„æºç®¡ç†çš„å®è·µæŠ€å·§

**ğŸ¯ å¯è¿ç§»çš„è®¾è®¡æ¨¡å¼**ï¼š
- **æ‰§è¡Œå¼•æ“æ¨¡å¼**ï¼šå¯åº”ç”¨äºä»»ä½•éœ€è¦å¤æ‚ä»»åŠ¡è°ƒåº¦çš„ç³»ç»Ÿ
- **æ¶ˆæ¯ä¼ é€’æ¶æ„**ï¼šé€‚ç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿå’Œå¾®æœåŠ¡æ¶æ„
- **æ£€æŸ¥ç‚¹ç³»ç»Ÿé›†æˆ**ï¼šçŠ¶æ€ç®¡ç†å’ŒæŒä¹…åŒ–çš„é€šç”¨è§£å†³æ–¹æ¡ˆ

### ä¸å‰åºçŸ¥è¯†çš„è¿æ¥

Pregelæ‰§è¡Œå¼•æ“ä¸ä¹‹å‰å­¦ä¹ çš„æ£€æŸ¥ç‚¹ç³»ç»Ÿï¼ˆL4ï¼‰æ·±åº¦é›†æˆï¼š

```python
# L4: æ£€æŸ¥ç‚¹ç³»ç»Ÿæä¾›çŠ¶æ€æŒä¹…åŒ–èƒ½åŠ›
checkpointer = PostgresSaver(...)

# L5: Pregelå¼•æ“åˆ©ç”¨æ£€æŸ¥ç‚¹å®ç°å¯æ¢å¤æ‰§è¡Œ
pregel = Pregel(
    nodes=nodes,
    channels=channels,
    checkpointer=checkpointer  # é›†æˆæ£€æŸ¥ç‚¹ç³»ç»Ÿ
)

# æ‰§è¡Œè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¿å­˜å’Œæ¢å¤çŠ¶æ€
for chunk in pregel.stream(input_data, config):
    # æ¯ä¸ªæ­¥éª¤éƒ½ä¼šè‡ªåŠ¨åˆ›å»ºæ£€æŸ¥ç‚¹
    # æ”¯æŒä»»æ„æ—¶ç‚¹çš„ä¸­æ–­å’Œæ¢å¤
    process(chunk)
```

### ä¸ºåç»­å­¦ä¹ å¥ å®šåŸºç¡€

ç†è§£Pregelæ‰§è¡Œå¼•æ“ä¸ºåç»­ç« èŠ‚å¥ å®šäº†é‡è¦åŸºç¡€ï¼š

- **L6: æ¶ˆæ¯ç³»ç»Ÿä¸Channelæ¶æ„** - ç†è§£Pregelå¦‚ä½•é€šè¿‡Channelä¼ é€’çŠ¶æ€
- **L7: é«˜çº§ç‰¹æ€§ä¸æ€§èƒ½ä¼˜åŒ–** - åŸºäºPregelçš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- **L8: ä¼ä¸šçº§éƒ¨ç½²ä¸è¿ç»´** - Pregelåœ¨ç”Ÿäº§ç¯å¢ƒçš„ç›‘æ§å’Œè°ƒä¼˜

---

**ğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº†LangGraphçš„æ ¸å¿ƒæ‰§è¡Œæœºåˆ¶ã€‚**

Pregelæ‰§è¡Œå¼•æ“æ˜¯LangGraphçš„æŠ€æœ¯ç²¾é«“ï¼Œç†è§£äº†å®ƒï¼Œä½ å°±ç†è§£äº†ç°ä»£AIå·¥ä½œæµç³»ç»Ÿçš„æœ¬è´¨ã€‚è¿™ä¸ä»…æ˜¯å¯¹Googleåˆ†å¸ƒå¼è®¡ç®—æ€æƒ³çš„ä¼ æ‰¿ï¼Œæ›´æ˜¯AIå·¥ç¨‹åŒ–çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚

ä¸‹ä¸€ç« æˆ‘ä»¬å°†æ·±å…¥[07-æ¶ˆæ¯ç³»ç»Ÿä¸Channelæ¶æ„](./07-æ¶ˆæ¯ç³»ç»Ÿä¸Channelæ¶æ„.md)ï¼Œæ¢ç´¢çŠ¶æ€ä¼ é€’çš„åº•å±‚æœºåˆ¶ï¼