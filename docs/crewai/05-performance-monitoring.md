# CrewAI æ€§èƒ½ä¸ç›‘æ§æ·±åº¦åˆ†æ

## æ¦‚è¿°

æœ¬æ–‡æ¡£æ·±å…¥åˆ†æ CrewAI æ¡†æ¶çš„æ€§èƒ½ç‰¹å¾å’Œç›‘æ§ä½“ç³»ï¼ŒåŸºäºå¯¹æ¡†æ¶æ ¸å¿ƒå®ç°çš„æ·±åº¦ç†è§£ï¼Œæä¾›ä¼ä¸šçº§æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œå…¨é¢çš„å¯è§‚æµ‹æ€§è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„æ€§èƒ½åˆ†æå’Œç›‘æ§å®è·µï¼Œå¸®åŠ©å¼€å‘è€…æ„å»ºé«˜æ€§èƒ½ã€å¯ç›‘æ§çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿã€‚

## 1. æ€§èƒ½åˆ†ææ¶æ„

### 1.1 æ€§èƒ½åˆ†æå±‚æ¬¡æ¨¡å‹

#### 1.1.1 å››å±‚æ€§èƒ½åˆ†ææ¡†æ¶

```python
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import psutil
import threading
import time
import asyncio
from collections import defaultdict, deque
import statistics

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    
    # åº”ç”¨å±‚æŒ‡æ ‡
    task_execution_time: float
    agent_response_time: float
    crew_throughput: int
    success_rate: float
    
    # æ¡†æ¶å±‚æŒ‡æ ‡
    context_window_usage: float
    cache_hit_rate: float
    event_processing_latency: float
    memory_pool_efficiency: float
    
    # ç³»ç»Ÿå±‚æŒ‡æ ‡
    cpu_usage: float
    memory_usage: float
    network_io: int
    disk_io: int
    
    # å¤–éƒ¨æœåŠ¡å±‚æŒ‡æ ‡
    llm_api_latency: float
    llm_api_error_rate: float
    tool_execution_time: float
    database_query_time: float
    
    # å…ƒæ•°æ®
    timestamp: datetime
    measurement_duration: float

class PerformanceAnalyzer:
    """å¤šå±‚æ¬¡æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, measurement_window: int = 60):
        self.measurement_window = measurement_window
        self.metrics_history: deque = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000ä¸ªæµ‹é‡ç‚¹
        self.real_time_metrics: Dict[str, Any] = {}
        self.performance_baselines: Dict[str, float] = {}
        self.alert_thresholds: Dict[str, Dict] = {}
        
        # æ€§èƒ½åˆ†æç»„ä»¶
        self.application_profiler = ApplicationLayerProfiler()
        self.framework_profiler = FrameworkLayerProfiler()
        self.system_profiler = SystemLayerProfiler()
        self.service_profiler = ExternalServiceProfiler()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitoring_thread = threading.Thread(
            target=self._continuous_monitoring, 
            daemon=True
        )
        self.monitoring_active = True
        self.monitoring_thread.start()
    
    def _continuous_monitoring(self):
        """æŒç»­æ€§èƒ½ç›‘æ§å¾ªç¯"""
        
        while self.monitoring_active:
            try:
                start_time = time.time()
                
                # æ”¶é›†å„å±‚æ€§èƒ½æ•°æ®
                app_metrics = self.application_profiler.collect_metrics()
                framework_metrics = self.framework_profiler.collect_metrics()
                system_metrics = self.system_profiler.collect_metrics()
                service_metrics = self.service_profiler.collect_metrics()
                
                # ç»„åˆæ€§èƒ½æŒ‡æ ‡
                combined_metrics = PerformanceMetrics(
                    # åº”ç”¨å±‚
                    task_execution_time=app_metrics.get('avg_task_time', 0.0),
                    agent_response_time=app_metrics.get('avg_agent_time', 0.0),
                    crew_throughput=app_metrics.get('tasks_per_minute', 0),
                    success_rate=app_metrics.get('success_rate', 1.0),
                    
                    # æ¡†æ¶å±‚
                    context_window_usage=framework_metrics.get('context_usage', 0.0),
                    cache_hit_rate=framework_metrics.get('cache_hit_rate', 0.0),
                    event_processing_latency=framework_metrics.get('event_latency', 0.0),
                    memory_pool_efficiency=framework_metrics.get('memory_efficiency', 1.0),
                    
                    # ç³»ç»Ÿå±‚
                    cpu_usage=system_metrics.get('cpu_percent', 0.0),
                    memory_usage=system_metrics.get('memory_percent', 0.0),
                    network_io=system_metrics.get('network_bytes', 0),
                    disk_io=system_metrics.get('disk_bytes', 0),
                    
                    # å¤–éƒ¨æœåŠ¡å±‚
                    llm_api_latency=service_metrics.get('llm_latency', 0.0),
                    llm_api_error_rate=service_metrics.get('llm_error_rate', 0.0),
                    tool_execution_time=service_metrics.get('tool_time', 0.0),
                    database_query_time=service_metrics.get('db_time', 0.0),
                    
                    # å…ƒæ•°æ®
                    timestamp=datetime.now(),
                    measurement_duration=time.time() - start_time
                )
                
                # å­˜å‚¨æŒ‡æ ‡å†å²
                self.metrics_history.append(combined_metrics)
                
                # æ›´æ–°å®æ—¶æŒ‡æ ‡
                self._update_real_time_metrics(combined_metrics)
                
                # æ£€æŸ¥å¼‚å¸¸å’Œå‘Šè­¦
                self._check_performance_anomalies(combined_metrics)
                
                # æ§åˆ¶ç›‘æ§é¢‘ç‡
                time.sleep(max(0, 1.0 - (time.time() - start_time)))
                
            except Exception as e:
                print(f"Performance monitoring error: {e}")
                time.sleep(1.0)
    
    def _update_real_time_metrics(self, metrics: PerformanceMetrics):
        """æ›´æ–°å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
        
        self.real_time_metrics.update({
            'current_cpu': metrics.cpu_usage,
            'current_memory': metrics.memory_usage,
            'current_throughput': metrics.crew_throughput,
            'current_success_rate': metrics.success_rate,
            'current_llm_latency': metrics.llm_api_latency,
            'last_update': metrics.timestamp
        })
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        if len(self.metrics_history) >= 10:
            recent_metrics = list(self.metrics_history)[-10:]
            
            self.real_time_metrics.update({
                'cpu_trend': self._calculate_trend([m.cpu_usage for m in recent_metrics]),
                'memory_trend': self._calculate_trend([m.memory_usage for m in recent_metrics]),
                'latency_trend': self._calculate_trend([m.llm_api_latency for m in recent_metrics]),
                'throughput_trend': self._calculate_trend([m.crew_throughput for m in recent_metrics])
            })
    
    def get_performance_report(self, time_range: Optional[timedelta] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        if time_range is None:
            time_range = timedelta(minutes=30)
        
        cutoff_time = datetime.now() - time_range
        relevant_metrics = [
            m for m in self.metrics_history 
            if m.timestamp >= cutoff_time
        ]
        
        if not relevant_metrics:
            return {"error": "No metrics available for the specified time range"}
        
        report = {
            "analysis_period": {
                "start": relevant_metrics[0].timestamp,
                "end": relevant_metrics[-1].timestamp,
                "duration_minutes": time_range.total_seconds() / 60,
                "sample_count": len(relevant_metrics)
            },
            
            "application_performance": self._analyze_application_performance(relevant_metrics),
            "framework_performance": self._analyze_framework_performance(relevant_metrics),
            "system_performance": self._analyze_system_performance(relevant_metrics),
            "service_performance": self._analyze_service_performance(relevant_metrics),
            
            "performance_insights": self._generate_performance_insights(relevant_metrics),
            "optimization_recommendations": self._generate_optimization_recommendations(relevant_metrics)
        }
        
        return report
```

#### 1.1.2 åº”ç”¨å±‚æ€§èƒ½åˆ†æå™¨

```python
class ApplicationLayerProfiler:
    """åº”ç”¨å±‚æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.task_metrics: Dict[str, List[float]] = defaultdict(list)
        self.agent_metrics: Dict[str, List[float]] = defaultdict(list)
        self.crew_metrics: Dict[str, List[float]] = defaultdict(list)
        self.execution_history: deque = deque(maxlen=1000)
    
    def track_task_execution(self, task_id: str, execution_time: float, 
                           success: bool, complexity_score: float = 1.0):
        """è·Ÿè¸ªä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡"""
        
        self.task_metrics[task_id].append(execution_time)
        
        # è®°å½•æ‰§è¡Œå†å²
        self.execution_history.append({
            'timestamp': datetime.now(),
            'type': 'task',
            'id': task_id,
            'execution_time': execution_time,
            'success': success,
            'complexity_score': complexity_score
        })
        
        # ç»´æŠ¤å†å²æ•°æ®å¤§å°
        if len(self.task_metrics[task_id]) > 100:
            self.task_metrics[task_id] = self.task_metrics[task_id][-100:]
    
    def track_agent_interaction(self, agent_id: str, response_time: float, 
                              token_count: int, tool_calls: int = 0):
        """è·Ÿè¸ªæ™ºèƒ½ä½“äº¤äº’æŒ‡æ ‡"""
        
        self.agent_metrics[agent_id].append(response_time)
        
        self.execution_history.append({
            'timestamp': datetime.now(),
            'type': 'agent',
            'id': agent_id,
            'response_time': response_time,
            'token_count': token_count,
            'tool_calls': tool_calls
        })
    
    def track_crew_execution(self, crew_id: str, total_time: float, 
                           task_count: int, success_rate: float):
        """è·Ÿè¸ªå›¢é˜Ÿæ‰§è¡ŒæŒ‡æ ‡"""
        
        self.crew_metrics[crew_id].append(total_time)
        
        self.execution_history.append({
            'timestamp': datetime.now(),
            'type': 'crew',
            'id': crew_id,
            'total_time': total_time,
            'task_count': task_count,
            'success_rate': success_rate
        })
    
    def collect_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†åº”ç”¨å±‚æŒ‡æ ‡"""
        
        recent_executions = [
            ex for ex in self.execution_history 
            if ex['timestamp'] > datetime.now() - timedelta(minutes=1)
        ]
        
        task_executions = [ex for ex in recent_executions if ex['type'] == 'task']
        agent_interactions = [ex for ex in recent_executions if ex['type'] == 'agent']
        crew_executions = [ex for ex in recent_executions if ex['type'] == 'crew']
        
        metrics = {
            'tasks_per_minute': len(task_executions),
            'agents_per_minute': len(agent_interactions),
            'crews_per_minute': len(crew_executions),
            
            'avg_task_time': statistics.mean([ex['execution_time'] for ex in task_executions]) if task_executions else 0.0,
            'avg_agent_time': statistics.mean([ex['response_time'] for ex in agent_interactions]) if agent_interactions else 0.0,
            'avg_crew_time': statistics.mean([ex['total_time'] for ex in crew_executions]) if crew_executions else 0.0,
            
            'success_rate': statistics.mean([ex.get('success', True) for ex in recent_executions]) if recent_executions else 1.0,
            
            'complexity_distribution': self._analyze_complexity_distribution(task_executions),
            'performance_trends': self._calculate_performance_trends()
        }
        
        return metrics
    
    def _analyze_complexity_distribution(self, executions: List[Dict]) -> Dict[str, int]:
        """åˆ†æä»»åŠ¡å¤æ‚åº¦åˆ†å¸ƒ"""
        
        distribution = {'low': 0, 'medium': 0, 'high': 0}
        
        for execution in executions:
            complexity = execution.get('complexity_score', 1.0)
            if complexity < 0.5:
                distribution['low'] += 1
            elif complexity < 1.5:
                distribution['medium'] += 1
            else:
                distribution['high'] += 1
        
        return distribution
```

### 1.2 æ¡†æ¶å±‚æ€§èƒ½åˆ†æ

#### 1.2.1 CrewAI æ ¸å¿ƒç»„ä»¶æ€§èƒ½ç›‘æ§

```python
class FrameworkLayerProfiler:
    """æ¡†æ¶å±‚æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.context_usage_history: deque = deque(maxlen=200)
        self.cache_statistics: Dict[str, int] = defaultdict(int)
        self.event_processing_times: deque = deque(maxlen=500)
        self.memory_pool_stats: Dict[str, Any] = {}
        
        # æŒ‚é’©åˆ°CrewAIäº‹ä»¶ç³»ç»Ÿ
        self._setup_framework_hooks()
    
    def _setup_framework_hooks(self):
        """è®¾ç½®æ¡†æ¶æ€§èƒ½ç›‘æ§æŒ‚é’©"""
        
        # è¿™é‡Œéœ€è¦ä¸CrewAIçš„äº‹ä»¶ç³»ç»Ÿé›†æˆ
        # ç›‘æ§å…³é”®æ¡†æ¶äº‹ä»¶çš„æ€§èƒ½
        pass
    
    def track_context_usage(self, agent_id: str, context_size: int, 
                          max_context: int, compression_applied: bool = False):
        """è·Ÿè¸ªä¸Šä¸‹æ–‡çª—å£ä½¿ç”¨æƒ…å†µ"""
        
        usage_ratio = context_size / max_context if max_context > 0 else 0
        
        self.context_usage_history.append({
            'timestamp': datetime.now(),
            'agent_id': agent_id,
            'context_size': context_size,
            'max_context': max_context,
            'usage_ratio': usage_ratio,
            'compression_applied': compression_applied
        })
    
    def track_cache_operation(self, operation: str, hit: bool, 
                            cache_type: str = 'default'):
        """è·Ÿè¸ªç¼“å­˜æ“ä½œ"""
        
        key = f"{cache_type}_{operation}_{'hit' if hit else 'miss'}"
        self.cache_statistics[key] += 1
        
        # è®°å½•ç¼“å­˜æ“ä½œæ—¶é—´æˆ³ç”¨äºè®¡ç®—å‘½ä¸­ç‡
        self.cache_statistics[f"{cache_type}_total"] += 1
        if hit:
            self.cache_statistics[f"{cache_type}_hits"] += 1
    
    def track_event_processing(self, event_type: str, processing_time: float, 
                             handler_count: int, success: bool):
        """è·Ÿè¸ªäº‹ä»¶å¤„ç†æ€§èƒ½"""
        
        self.event_processing_times.append({
            'timestamp': datetime.now(),
            'event_type': event_type,
            'processing_time': processing_time,
            'handler_count': handler_count,
            'success': success
        })
    
    def track_memory_pool_usage(self, pool_type: str, allocated: int, 
                               total: int, fragmentation: float):
        """è·Ÿè¸ªå†…å­˜æ± ä½¿ç”¨æƒ…å†µ"""
        
        self.memory_pool_stats[pool_type] = {
            'timestamp': datetime.now(),
            'allocated': allocated,
            'total': total,
            'utilization': allocated / total if total > 0 else 0,
            'fragmentation': fragmentation
        }
    
    def collect_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ¡†æ¶å±‚æŒ‡æ ‡"""
        
        # è®¡ç®—ä¸Šä¸‹æ–‡ä½¿ç”¨ç‡
        recent_context = [
            ctx for ctx in self.context_usage_history 
            if ctx['timestamp'] > datetime.now() - timedelta(minutes=5)
        ]
        
        avg_context_usage = statistics.mean([
            ctx['usage_ratio'] for ctx in recent_context
        ]) if recent_context else 0.0
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rates = {}
        for cache_type in set(key.split('_')[0] for key in self.cache_statistics.keys()):
            total = self.cache_statistics.get(f"{cache_type}_total", 0)
            hits = self.cache_statistics.get(f"{cache_type}_hits", 0)
            cache_hit_rates[cache_type] = hits / total if total > 0 else 0.0
        
        overall_cache_hit_rate = statistics.mean(cache_hit_rates.values()) if cache_hit_rates else 0.0
        
        # è®¡ç®—äº‹ä»¶å¤„ç†å»¶è¿Ÿ
        recent_events = [
            evt for evt in self.event_processing_times 
            if evt['timestamp'] > datetime.now() - timedelta(minutes=5)
        ]
        
        avg_event_latency = statistics.mean([
            evt['processing_time'] for evt in recent_events
        ]) if recent_events else 0.0
        
        # è®¡ç®—å†…å­˜æ± æ•ˆç‡
        total_utilization = 0.0
        active_pools = 0
        
        for pool_type, stats in self.memory_pool_stats.items():
            if stats['timestamp'] > datetime.now() - timedelta(minutes=1):
                total_utilization += stats['utilization']
                active_pools += 1
        
        avg_memory_efficiency = total_utilization / active_pools if active_pools > 0 else 1.0
        
        return {
            'context_usage': avg_context_usage,
            'cache_hit_rate': overall_cache_hit_rate,
            'cache_hit_rates_by_type': cache_hit_rates,
            'event_latency': avg_event_latency,
            'memory_efficiency': avg_memory_efficiency,
            
            'context_compression_rate': len([ctx for ctx in recent_context if ctx['compression_applied']]) / len(recent_context) if recent_context else 0.0,
            'event_success_rate': len([evt for evt in recent_events if evt['success']]) / len(recent_events) if recent_events else 1.0,
            'memory_fragmentation': statistics.mean([stats['fragmentation'] for stats in self.memory_pool_stats.values()]) if self.memory_pool_stats else 0.0
        }
```

## 2. å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI) ä½“ç³»

### 2.1 å¤šç»´åº¦ KPI æ¡†æ¶

#### 2.1.1 ä¸šåŠ¡å…³é”®æŒ‡æ ‡

```python
class BusinessKPITracker:
    """ä¸šåŠ¡å…³é”®ç»©æ•ˆæŒ‡æ ‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.kpi_definitions = {
            # æ•ˆç‡æŒ‡æ ‡
            "task_completion_rate": {
                "description": "ä»»åŠ¡å®Œæˆç‡",
                "unit": "percentage",
                "target": 0.95,
                "critical_threshold": 0.85,
                "calculation": "completed_tasks / total_tasks"
            },
            
            "average_task_duration": {
                "description": "å¹³å‡ä»»åŠ¡æ‰§è¡Œæ—¶é—´",
                "unit": "seconds", 
                "target": 120.0,
                "critical_threshold": 300.0,
                "calculation": "sum(task_durations) / count(tasks)"
            },
            
            "crew_throughput": {
                "description": "å›¢é˜Ÿååé‡",
                "unit": "tasks_per_hour",
                "target": 50.0,
                "critical_threshold": 20.0,
                "calculation": "completed_tasks_per_hour"
            },
            
            # è´¨é‡æŒ‡æ ‡
            "output_quality_score": {
                "description": "è¾“å‡ºè´¨é‡è¯„åˆ†",
                "unit": "score_0_to_10",
                "target": 8.0,
                "critical_threshold": 6.0,
                "calculation": "average_quality_ratings"
            },
            
            "error_rate": {
                "description": "é”™è¯¯ç‡",
                "unit": "percentage",
                "target": 0.02,
                "critical_threshold": 0.10,
                "calculation": "failed_executions / total_executions"
            },
            
            "retry_rate": {
                "description": "é‡è¯•ç‡",
                "unit": "percentage", 
                "target": 0.05,
                "critical_threshold": 0.20,
                "calculation": "retry_attempts / total_attempts"
            },
            
            # æˆæœ¬æŒ‡æ ‡
            "cost_per_task": {
                "description": "å•ä»»åŠ¡æˆæœ¬",
                "unit": "currency",
                "target": 0.50,
                "critical_threshold": 2.00,
                "calculation": "total_costs / completed_tasks"
            },
            
            "token_efficiency": {
                "description": "Tokenä½¿ç”¨æ•ˆç‡", 
                "unit": "tokens_per_task",
                "target": 5000.0,
                "critical_threshold": 15000.0,
                "calculation": "total_tokens / completed_tasks"
            },
            
            # ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
            "response_time_p95": {
                "description": "95åˆ†ä½å“åº”æ—¶é—´",
                "unit": "seconds",
                "target": 30.0,
                "critical_threshold": 120.0,
                "calculation": "95th_percentile(response_times)"
            },
            
            "user_satisfaction": {
                "description": "ç”¨æˆ·æ»¡æ„åº¦",
                "unit": "score_1_to_5",
                "target": 4.2,
                "critical_threshold": 3.0,
                "calculation": "average_user_ratings"
            }
        }
        
        self.current_values: Dict[str, float] = {}
        self.historical_data: Dict[str, deque] = {
            kpi: deque(maxlen=1440)  # 24å°æ—¶æ•°æ®ï¼ŒæŒ‰åˆ†é’Ÿ
            for kpi in self.kpi_definitions
        }
        self.alerts: List[Dict] = []
    
    def update_kpi(self, kpi_name: str, value: float, timestamp: Optional[datetime] = None):
        """æ›´æ–°KPIå€¼"""
        
        if kpi_name not in self.kpi_definitions:
            raise ValueError(f"Unknown KPI: {kpi_name}")
        
        if timestamp is None:
            timestamp = datetime.now()
        
        # æ›´æ–°å½“å‰å€¼
        self.current_values[kpi_name] = value
        
        # æ·»åŠ åˆ°å†å²æ•°æ®
        self.historical_data[kpi_name].append({
            'timestamp': timestamp,
            'value': value
        })
        
        # æ£€æŸ¥å‘Šè­¦æ¡ä»¶
        self._check_kpi_thresholds(kpi_name, value)
    
    def _check_kpi_thresholds(self, kpi_name: str, value: float):
        """æ£€æŸ¥KPIé˜ˆå€¼å‘Šè­¦"""
        
        definition = self.kpi_definitions[kpi_name]
        target = definition['target']
        critical_threshold = definition['critical_threshold']
        
        # ç¡®å®šæ˜¯å¦è¶…å‡ºä¸´ç•Œé˜ˆå€¼
        is_critical = False
        
        # å¯¹äº"è¶Šå°è¶Šå¥½"çš„æŒ‡æ ‡ï¼ˆå¦‚é”™è¯¯ç‡ã€å“åº”æ—¶é—´ï¼‰
        if kpi_name in ['error_rate', 'retry_rate', 'cost_per_task', 'token_efficiency', 'average_task_duration', 'response_time_p95']:
            if value > critical_threshold:
                is_critical = True
                severity = "critical"
            elif value > target:
                severity = "warning"
            else:
                return  # æ­£å¸¸èŒƒå›´å†…
        
        # å¯¹äº"è¶Šå¤§è¶Šå¥½"çš„æŒ‡æ ‡ï¼ˆå¦‚å®Œæˆç‡ã€è´¨é‡è¯„åˆ†ï¼‰
        else:
            if value < critical_threshold:
                is_critical = True
                severity = "critical"
            elif value < target:
                severity = "warning"
            else:
                return  # æ­£å¸¸èŒƒå›´å†…
        
        # åˆ›å»ºå‘Šè­¦
        alert = {
            'timestamp': datetime.now(),
            'kpi_name': kpi_name,
            'current_value': value,
            'target_value': target,
            'critical_threshold': critical_threshold,
            'severity': severity,
            'message': f"{definition['description']} ({value:.2f}) {'ä½äº' if not is_critical else 'è¶…å‡º'}{'ç›®æ ‡' if severity == 'warning' else 'ä¸´ç•Œ'}é˜ˆå€¼"
        }
        
        self.alerts.append(alert)
        
        # ä¿æŒå‘Šè­¦å†å²é•¿åº¦
        if len(self.alerts) > 1000:
            self.alerts = self.alerts[-1000:]
    
    def get_kpi_dashboard(self) -> Dict[str, Any]:
        """è·å–KPIä»ªè¡¨æ¿æ•°æ®"""
        
        dashboard = {
            'overview': {
                'total_kpis': len(self.kpi_definitions),
                'healthy_kpis': 0,
                'warning_kpis': 0,
                'critical_kpis': 0,
                'last_updated': max([
                    data[-1]['timestamp'] for data in self.historical_data.values() 
                    if data
                ]) if any(self.historical_data.values()) else None
            },
            'current_status': {},
            'trends': {},
            'recent_alerts': self.alerts[-10:] if self.alerts else []
        }
        
        # åˆ†ææ¯ä¸ªKPIçš„å½“å‰çŠ¶æ€
        for kpi_name, definition in self.kpi_definitions.items():
            current_value = self.current_values.get(kpi_name, 0.0)
            target = definition['target']
            critical_threshold = definition['critical_threshold']
            
            # è®¡ç®—çŠ¶æ€
            if kpi_name in ['error_rate', 'retry_rate', 'cost_per_task', 'token_efficiency', 'average_task_duration', 'response_time_p95']:
                if current_value > critical_threshold:
                    status = "critical"
                    dashboard['overview']['critical_kpis'] += 1
                elif current_value > target:
                    status = "warning"
                    dashboard['overview']['warning_kpis'] += 1
                else:
                    status = "healthy"
                    dashboard['overview']['healthy_kpis'] += 1
            else:
                if current_value < critical_threshold:
                    status = "critical"
                    dashboard['overview']['critical_kpis'] += 1
                elif current_value < target:
                    status = "warning"
                    dashboard['overview']['warning_kpis'] += 1
                else:
                    status = "healthy"
                    dashboard['overview']['healthy_kpis'] += 1
            
            dashboard['current_status'][kpi_name] = {
                'value': current_value,
                'status': status,
                'target': target,
                'description': definition['description'],
                'unit': definition['unit']
            }
            
            # è®¡ç®—è¶‹åŠ¿
            historical_values = [
                point['value'] for point in self.historical_data[kpi_name][-60:]  # æœ€è¿‘60åˆ†é’Ÿ
            ]
            
            if len(historical_values) >= 2:
                trend = self._calculate_trend(historical_values)
                dashboard['trends'][kpi_name] = trend
        
        return dashboard
```

### 2.2 å®æ—¶ç›‘æ§ä½“ç³»

#### 2.2.1 å®æ—¶æŒ‡æ ‡æ”¶é›†å™¨

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from prometheus_client import Counter, Histogram, Gauge, start_http_server

class RealTimeMetricsCollector:
    """å®æ—¶æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, port: int = 8000):
        self.port = port
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # PrometheusæŒ‡æ ‡å®šä¹‰
        self.task_counter = Counter('crewai_tasks_total', 'Total number of tasks', ['status', 'agent_type'])
        self.task_duration = Histogram('crewai_task_duration_seconds', 'Task execution duration')
        self.agent_response_time = Histogram('crewai_agent_response_seconds', 'Agent response time')
        self.memory_usage = Gauge('crewai_memory_usage_bytes', 'Memory usage in bytes')
        self.active_tasks = Gauge('crewai_active_tasks', 'Number of active tasks')
        self.llm_api_calls = Counter('crewai_llm_api_calls_total', 'Total LLM API calls', ['model', 'status'])
        self.llm_latency = Histogram('crewai_llm_latency_seconds', 'LLM API latency')
        self.cache_operations = Counter('crewai_cache_operations_total', 'Cache operations', ['type', 'result'])
        
        # è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†
        self.custom_metrics: Dict[str, Any] = {}
        self.metric_callbacks: List[callable] = []
        
        # å¯åŠ¨PrometheusæœåŠ¡å™¨
        start_http_server(self.port)
        
        # å¯åŠ¨å®æ—¶æ”¶é›†å¾ªç¯
        self.collection_active = True
        asyncio.create_task(self._real_time_collection_loop())
    
    async def _real_time_collection_loop(self):
        """å®æ—¶æŒ‡æ ‡æ”¶é›†å¾ªç¯"""
        
        while self.collection_active:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                await self._collect_system_metrics()
                
                # æ‰§è¡Œè‡ªå®šä¹‰å›è°ƒ
                for callback in self.metric_callbacks:
                    try:
                        await callback()
                    except Exception as e:
                        print(f"Metric callback error: {e}")
                
                # æ¯ç§’æ”¶é›†ä¸€æ¬¡
                await asyncio.sleep(1.0)
                
            except Exception as e:
                print(f"Real-time collection error: {e}")
                await asyncio.sleep(5.0)
    
    async def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»Ÿçº§æŒ‡æ ‡"""
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_info = psutil.virtual_memory()
        self.memory_usage.set(memory_info.used)
        
        # è¿›ç¨‹ä¿¡æ¯
        process = psutil.Process()
        process_memory = process.memory_info().rss
        
        # æ›´æ–°è‡ªå®šä¹‰æŒ‡æ ‡
        self.custom_metrics.update({
            'system_memory_percent': memory_info.percent,
            'process_memory_mb': process_memory / 1024 / 1024,
            'cpu_count': psutil.cpu_count(),
            'load_average': psutil.getloadavg()[0] if hasattr(psutil, 'getloadavg') else 0.0
        })
    
    def record_task_execution(self, duration: float, success: bool, agent_type: str = "default"):
        """è®°å½•ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡"""
        
        status = "success" if success else "failure"
        self.task_counter.labels(status=status, agent_type=agent_type).inc()
        self.task_duration.observe(duration)
    
    def record_agent_response(self, response_time: float):
        """è®°å½•æ™ºèƒ½ä½“å“åº”æ—¶é—´"""
        
        self.agent_response_time.observe(response_time)
    
    def record_llm_api_call(self, model: str, latency: float, success: bool):
        """è®°å½•LLM APIè°ƒç”¨"""
        
        status = "success" if success else "failure"
        self.llm_api_calls.labels(model=model, status=status).inc()
        self.llm_latency.observe(latency)
    
    def record_cache_operation(self, operation_type: str, hit: bool):
        """è®°å½•ç¼“å­˜æ“ä½œ"""
        
        result = "hit" if hit else "miss"
        self.cache_operations.labels(type=operation_type, result=result).inc()
    
    def set_active_tasks(self, count: int):
        """è®¾ç½®å½“å‰æ´»è·ƒä»»åŠ¡æ•°"""
        
        self.active_tasks.set(count)
    
    def add_custom_metric_callback(self, callback: callable):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡å›è°ƒ"""
        
        self.metric_callbacks.append(callback)
    
    def get_current_metrics_summary(self) -> Dict[str, Any]:
        """è·å–å½“å‰æŒ‡æ ‡æ‘˜è¦"""
        
        return {
            'timestamp': datetime.now(),
            'system_metrics': self.custom_metrics,
            'active_tasks': self.active_tasks._value._value,
            'prometheus_metrics_url': f'http://localhost:{self.port}/metrics'
        }
```

## 3. æ€§èƒ½ç“¶é¢ˆè¯†åˆ«ä¸ä¼˜åŒ–

### 3.1 æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹

#### 3.1.1 å¤šå±‚æ¬¡ç“¶é¢ˆåˆ†æå™¨

```python
class PerformanceBottleneckDetector:
    """æ€§èƒ½ç“¶é¢ˆæ£€æµ‹å™¨"""
    
    def __init__(self, analyzer: PerformanceAnalyzer):
        self.analyzer = analyzer
        self.bottleneck_patterns = self._initialize_bottleneck_patterns()
        self.optimization_strategies = self._initialize_optimization_strategies()
    
    def _initialize_bottleneck_patterns(self) -> Dict[str, Dict]:
        """åˆå§‹åŒ–ç“¶é¢ˆè¯†åˆ«æ¨¡å¼"""
        
        return {
            "high_cpu_usage": {
                "condition": lambda metrics: metrics.cpu_usage > 80,
                "severity": "high",
                "category": "system",
                "description": "CPUä½¿ç”¨ç‡è¿‡é«˜",
                "potential_causes": [
                    "è®¡ç®—å¯†é›†å‹ä»»åŠ¡è¿‡å¤š",
                    "å¹¶å‘æ‰§è¡Œè¿‡åº¦",
                    "ç®—æ³•æ•ˆç‡é—®é¢˜",
                    "æ— é™å¾ªç¯æˆ–æ­»å¾ªç¯"
                ]
            },
            
            "memory_exhaustion": {
                "condition": lambda metrics: metrics.memory_usage > 85,
                "severity": "critical",
                "category": "system", 
                "description": "å†…å­˜ä½¿ç”¨ç‡æ¥è¿‘æé™",
                "potential_causes": [
                    "å†…å­˜æ³„æ¼",
                    "å¤§å¯¹è±¡æœªé‡Šæ”¾",
                    "ç¼“å­˜ç­–ç•¥ä¸å½“",
                    "æ‰¹å¤„ç†æ•°æ®è¿‡å¤§"
                ]
            },
            
            "llm_api_latency": {
                "condition": lambda metrics: metrics.llm_api_latency > 10.0,
                "severity": "medium",
                "category": "external_service",
                "description": "LLM APIå“åº”å»¶è¿Ÿè¿‡é«˜", 
                "potential_causes": [
                    "ç½‘ç»œè¿æ¥é—®é¢˜",
                    "APIæœåŠ¡å™¨è´Ÿè½½è¿‡é«˜",
                    "è¯·æ±‚å‚æ•°è®¾ç½®ä¸å½“",
                    "æ¨¡å‹é€‰æ‹©ä¸åˆé€‚"
                ]
            },
            
            "low_cache_hit_rate": {
                "condition": lambda metrics: metrics.cache_hit_rate < 0.5,
                "severity": "medium",
                "category": "framework",
                "description": "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½",
                "potential_causes": [
                    "ç¼“å­˜ç­–ç•¥è®¾ç½®é”™è¯¯",
                    "ç¼“å­˜é”®è®¾è®¡é—®é¢˜",
                    "ç¼“å­˜è¿‡æœŸæ—¶é—´è¿‡çŸ­",
                    "æ•°æ®å˜åŒ–é¢‘ç‡è¿‡é«˜"
                ]
            },
            
            "high_context_usage": {
                "condition": lambda metrics: metrics.context_window_usage > 0.9,
                "severity": "high",
                "category": "framework",
                "description": "ä¸Šä¸‹æ–‡çª—å£ä½¿ç”¨ç‡è¿‡é«˜",
                "potential_causes": [
                    "å¯¹è¯å†å²è¿‡é•¿",
                    "ä»»åŠ¡æè¿°è¿‡äºè¯¦ç»†",
                    "ä¸Šä¸‹æ–‡å‹ç¼©ä¸è¶³",
                    "æ¨¡å‹é€‰æ‹©ä¸å½“"
                ]
            },
            
            "low_success_rate": {
                "condition": lambda metrics: metrics.success_rate < 0.8,
                "severity": "critical",
                "category": "application",
                "description": "ä»»åŠ¡æˆåŠŸç‡è¿‡ä½",
                "potential_causes": [
                    "ä»»åŠ¡è®¾è®¡é—®é¢˜",
                    "æ™ºèƒ½ä½“èƒ½åŠ›ä¸è¶³",
                    "å·¥å…·é…ç½®é”™è¯¯",
                    "å¤–éƒ¨ä¾èµ–ä¸ç¨³å®š"
                ]
            },
            
            "poor_throughput": {
                "condition": lambda metrics: metrics.crew_throughput < 5,
                "severity": "high",
                "category": "application", 
                "description": "ç³»ç»Ÿååé‡è¿‡ä½",
                "potential_causes": [
                    "ä»»åŠ¡æ‰§è¡Œæ—¶é—´è¿‡é•¿",
                    "å¹¶å‘åº¦è®¾ç½®è¿‡ä½",
                    "èµ„æºäº‰ç”¨é—®é¢˜",
                    "ä¾èµ–æœåŠ¡å“åº”æ…¢"
                ]
            }
        }
    
    def _initialize_optimization_strategies(self) -> Dict[str, Dict]:
        """åˆå§‹åŒ–ä¼˜åŒ–ç­–ç•¥"""
        
        return {
            "high_cpu_usage": {
                "immediate_actions": [
                    "é™ä½å¹¶å‘ä»»åŠ¡æ•°é‡",
                    "æš‚åœéå…³é”®ä»»åŠ¡",
                    "å¯ç”¨ä»»åŠ¡é˜Ÿåˆ—ç¼“å†²"
                ],
                "short_term_solutions": [
                    "ä¼˜åŒ–ç®—æ³•å®ç°",
                    "æ·»åŠ ä»»åŠ¡ä¼˜å…ˆçº§ç®¡ç†",
                    "å®æ–½è´Ÿè½½å‡è¡¡"
                ],
                "long_term_solutions": [
                    "å‡çº§ç¡¬ä»¶é…ç½®",
                    "é‡æ„è®¡ç®—å¯†é›†å‹æ¨¡å—",
                    "å®æ–½åˆ†å¸ƒå¼å¤„ç†"
                ]
            },
            
            "memory_exhaustion": {
                "immediate_actions": [
                    "å¼ºåˆ¶åƒåœ¾å›æ”¶",
                    "æ¸…ç†ç¼“å­˜æ•°æ®",
                    "ç»ˆæ­¢å†…å­˜å ç”¨å¤§çš„ä»»åŠ¡"
                ],
                "short_term_solutions": [
                    "ä¼˜åŒ–æ•°æ®ç»“æ„ä½¿ç”¨",
                    "å®æ–½å†…å­˜æ± ç®¡ç†",
                    "æ·»åŠ å†…å­˜ä½¿ç”¨ç›‘æ§"
                ],
                "long_term_solutions": [
                    "é‡æ„å†…å­˜ä½¿ç”¨æ¨¡å¼",
                    "å®æ–½æ•°æ®æµå¼å¤„ç†",
                    "å‡çº§æœåŠ¡å™¨å†…å­˜"
                ]
            },
            
            "llm_api_latency": {
                "immediate_actions": [
                    "å¯ç”¨è¯·æ±‚ç¼“å­˜",
                    "é™ä½å¹¶å‘è¯·æ±‚æ•°",
                    "åˆ‡æ¢åˆ°æ›´å¿«çš„æ¨¡å‹"
                ],
                "short_term_solutions": [
                    "ä¼˜åŒ–è¯·æ±‚å‚æ•°",
                    "å®æ–½è¯·æ±‚æ‰¹å¤„ç†",
                    "æ·»åŠ è¿æ¥æ± ç®¡ç†"
                ],
                "long_term_solutions": [
                    "éƒ¨ç½²æœ¬åœ°LLMæœåŠ¡",
                    "å®æ–½å¤šLLMè´Ÿè½½å‡è¡¡",
                    "ä¼˜åŒ–ç½‘ç»œæ¶æ„"
                ]
            },
            
            "low_cache_hit_rate": {
                "immediate_actions": [
                    "è°ƒæ•´ç¼“å­˜è¿‡æœŸæ—¶é—´",
                    "ä¼˜åŒ–ç¼“å­˜é”®ç­–ç•¥",
                    "å¢åŠ ç¼“å­˜å®¹é‡"
                ],
                "short_term_solutions": [
                    "å®æ–½å¤šçº§ç¼“å­˜",
                    "ä¼˜åŒ–ç¼“å­˜æ•°æ®ç»“æ„",
                    "æ·»åŠ ç¼“å­˜é¢„çƒ­æœºåˆ¶"
                ],
                "long_term_solutions": [
                    "é‡è®¾è®¡ç¼“å­˜æ¶æ„",
                    "å®æ–½åˆ†å¸ƒå¼ç¼“å­˜",
                    "ä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼"
                ]
            }
        }
    
    def detect_bottlenecks(self, time_window: timedelta = timedelta(minutes=5)) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ€§èƒ½ç“¶é¢ˆ"""
        
        # è·å–æ€§èƒ½æŠ¥å‘Š
        report = self.analyzer.get_performance_report(time_window)
        
        if "error" in report:
            return []
        
        # æå–å…³é”®æŒ‡æ ‡
        recent_metrics = self.analyzer.metrics_history[-1] if self.analyzer.metrics_history else None
        
        if not recent_metrics:
            return []
        
        detected_bottlenecks = []
        
        # æ£€æŸ¥æ¯ä¸ªç“¶é¢ˆæ¨¡å¼
        for pattern_name, pattern_config in self.bottleneck_patterns.items():
            try:
                if pattern_config["condition"](recent_metrics):
                    bottleneck = {
                        "name": pattern_name,
                        "severity": pattern_config["severity"],
                        "category": pattern_config["category"],
                        "description": pattern_config["description"],
                        "potential_causes": pattern_config["potential_causes"],
                        "detected_at": datetime.now(),
                        "current_metrics": {
                            "cpu_usage": recent_metrics.cpu_usage,
                            "memory_usage": recent_metrics.memory_usage,
                            "llm_api_latency": recent_metrics.llm_api_latency,
                            "cache_hit_rate": recent_metrics.cache_hit_rate,
                            "context_window_usage": recent_metrics.context_window_usage,
                            "success_rate": recent_metrics.success_rate,
                            "crew_throughput": recent_metrics.crew_throughput
                        }
                    }
                    
                    # æ·»åŠ ä¼˜åŒ–å»ºè®®
                    if pattern_name in self.optimization_strategies:
                        bottleneck["optimization_suggestions"] = self.optimization_strategies[pattern_name]
                    
                    detected_bottlenecks.append(bottleneck)
                    
            except Exception as e:
                print(f"Error checking bottleneck pattern {pattern_name}: {e}")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        detected_bottlenecks.sort(key=lambda x: severity_order.get(x["severity"], 3))
        
        return detected_bottlenecks
    
    def generate_optimization_plan(self, bottlenecks: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        
        if not bottlenecks:
            return {"status": "healthy", "message": "æœªæ£€æµ‹åˆ°æ€§èƒ½ç“¶é¢ˆ"}
        
        plan = {
            "analysis_timestamp": datetime.now(),
            "bottlenecks_detected": len(bottlenecks),
            "severity_distribution": {},
            "immediate_actions": [],
            "short_term_plan": [],
            "long_term_strategy": [],
            "estimated_impact": {}
        }
        
        # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        for bottleneck in bottlenecks:
            severity = bottleneck["severity"]
            plan["severity_distribution"][severity] = plan["severity_distribution"].get(severity, 0) + 1
        
        # æ”¶é›†ä¼˜åŒ–å»ºè®®
        immediate_actions = set()
        short_term_solutions = set()
        long_term_solutions = set()
        
        for bottleneck in bottlenecks:
            suggestions = bottleneck.get("optimization_suggestions", {})
            
            immediate_actions.update(suggestions.get("immediate_actions", []))
            short_term_solutions.update(suggestions.get("short_term_solutions", []))
            long_term_solutions.update(suggestions.get("long_term_solutions", []))
        
        plan["immediate_actions"] = list(immediate_actions)
        plan["short_term_plan"] = list(short_term_solutions)
        plan["long_term_strategy"] = list(long_term_solutions)
        
        # è¯„ä¼°ä¼˜åŒ–å½±å“
        plan["estimated_impact"] = self._estimate_optimization_impact(bottlenecks)
        
        return plan
    
    def _estimate_optimization_impact(self, bottlenecks: List[Dict]) -> Dict[str, Any]:
        """ä¼°ç®—ä¼˜åŒ–å½±å“"""
        
        impact_scores = {
            "performance_improvement": 0,
            "cost_reduction": 0,
            "reliability_improvement": 0,
            "user_experience_improvement": 0
        }
        
        impact_weights = {
            "critical": {"performance": 0.8, "cost": 0.6, "reliability": 0.9, "ux": 0.7},
            "high": {"performance": 0.6, "cost": 0.4, "reliability": 0.7, "ux": 0.5},
            "medium": {"performance": 0.4, "cost": 0.3, "reliability": 0.5, "ux": 0.3},
            "low": {"performance": 0.2, "cost": 0.1, "reliability": 0.3, "ux": 0.2}
        }
        
        for bottleneck in bottlenecks:
            severity = bottleneck["severity"]
            weights = impact_weights.get(severity, impact_weights["low"])
            
            impact_scores["performance_improvement"] += weights["performance"]
            impact_scores["cost_reduction"] += weights["cost"] 
            impact_scores["reliability_improvement"] += weights["reliability"]
            impact_scores["user_experience_improvement"] += weights["ux"]
        
        # å½’ä¸€åŒ–åˆ†æ•° (0-100)
        max_possible_score = len(bottlenecks) * 0.9
        for key in impact_scores:
            impact_scores[key] = min(100, (impact_scores[key] / max_possible_score) * 100) if max_possible_score > 0 else 0
        
        return impact_scores
```

### 3.2 è‡ªåŠ¨ä¼˜åŒ–å¼•æ“

#### 3.2.1 æ™ºèƒ½å‚æ•°è°ƒä¼˜

```python
class AutoOptimizationEngine:
    """è‡ªåŠ¨ä¼˜åŒ–å¼•æ“"""
    
    def __init__(self, performance_analyzer: PerformanceAnalyzer, 
                 bottleneck_detector: PerformanceBottleneckDetector):
        self.analyzer = performance_analyzer
        self.detector = bottleneck_detector
        
        # ä¼˜åŒ–å‚æ•°èŒƒå›´
        self.parameter_ranges = {
            "max_iterations": (5, 50),
            "temperature": (0.1, 1.0), 
            "cache_ttl": (60, 3600),
            "batch_size": (1, 20),
            "concurrent_tasks": (1, 10),
            "context_window_threshold": (0.5, 0.9)
        }
        
        # ä¼˜åŒ–å†å²
        self.optimization_history: List[Dict] = []
        self.current_config: Dict[str, Any] = {}
        self.baseline_metrics: Optional[Dict] = None
        
        # å¯åŠ¨è‡ªåŠ¨ä¼˜åŒ–
        self.auto_optimization_enabled = False
        asyncio.create_task(self._auto_optimization_loop())
    
    async def _auto_optimization_loop(self):
        """è‡ªåŠ¨ä¼˜åŒ–å¾ªç¯"""
        
        while True:
            try:
                if self.auto_optimization_enabled:
                    # æ£€æµ‹ç“¶é¢ˆ
                    bottlenecks = self.detector.detect_bottlenecks()
                    
                    if bottlenecks:
                        # æ‰§è¡Œè‡ªåŠ¨ä¼˜åŒ–
                        await self._execute_auto_optimization(bottlenecks)
                
                # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                await asyncio.sleep(300)
                
            except Exception as e:
                print(f"Auto-optimization error: {e}")
                await asyncio.sleep(60)
    
    async def _execute_auto_optimization(self, bottlenecks: List[Dict]):
        """æ‰§è¡Œè‡ªåŠ¨ä¼˜åŒ–"""
        
        print(f"ğŸ”§ æ£€æµ‹åˆ° {len(bottlenecks)} ä¸ªæ€§èƒ½ç“¶é¢ˆï¼Œå¼€å§‹è‡ªåŠ¨ä¼˜åŒ–...")
        
        # å¤‡ä»½å½“å‰é…ç½®
        previous_config = self.current_config.copy()
        
        # æ ¹æ®ç“¶é¢ˆç±»å‹é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        optimization_applied = False
        
        for bottleneck in bottlenecks:
            if bottleneck["name"] == "high_cpu_usage":
                # é™ä½å¹¶å‘åº¦å’Œè¿­ä»£æ¬¡æ•°
                if "concurrent_tasks" in self.current_config:
                    new_value = max(1, int(self.current_config["concurrent_tasks"] * 0.7))
                    self.current_config["concurrent_tasks"] = new_value
                    optimization_applied = True
                
                if "max_iterations" in self.current_config:
                    new_value = max(5, int(self.current_config["max_iterations"] * 0.8))
                    self.current_config["max_iterations"] = new_value
                    optimization_applied = True
            
            elif bottleneck["name"] == "memory_exhaustion":
                # å‡å°‘æ‰¹å¤„ç†å¤§å°å’Œç¼“å­˜æ—¶é—´
                if "batch_size" in self.current_config:
                    new_value = max(1, int(self.current_config["batch_size"] * 0.5))
                    self.current_config["batch_size"] = new_value
                    optimization_applied = True
                
                if "cache_ttl" in self.current_config:
                    new_value = max(60, int(self.current_config["cache_ttl"] * 0.6))
                    self.current_config["cache_ttl"] = new_value
                    optimization_applied = True
            
            elif bottleneck["name"] == "llm_api_latency":
                # è°ƒæ•´æ¸©åº¦å‚æ•°å’Œä¸Šä¸‹æ–‡é˜ˆå€¼
                if "temperature" in self.current_config:
                    new_value = min(1.0, self.current_config["temperature"] * 0.8)
                    self.current_config["temperature"] = new_value
                    optimization_applied = True
                
                if "context_window_threshold" in self.current_config:
                    new_value = max(0.5, self.current_config["context_window_threshold"] * 0.9)
                    self.current_config["context_window_threshold"] = new_value
                    optimization_applied = True
        
        if optimization_applied:
            # åº”ç”¨æ–°é…ç½®
            await self._apply_configuration(self.current_config)
            
            # ç­‰å¾…é…ç½®ç”Ÿæ•ˆ
            await asyncio.sleep(60)
            
            # è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
            improvement = await self._evaluate_optimization_impact(previous_config)
            
            # è®°å½•ä¼˜åŒ–å†å²
            self.optimization_history.append({
                "timestamp": datetime.now(),
                "bottlenecks": bottlenecks,
                "previous_config": previous_config,
                "new_config": self.current_config.copy(),
                "improvement": improvement,
                "success": improvement.get("overall_improvement", 0) > 0.05
            })
            
            print(f"âœ… è‡ªåŠ¨ä¼˜åŒ–å®Œæˆï¼Œæ€»ä½“æ”¹è¿›: {improvement.get('overall_improvement', 0):.2%}")
        
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é€‚ç”¨çš„è‡ªåŠ¨ä¼˜åŒ–ç­–ç•¥")
    
    async def _apply_configuration(self, config: Dict[str, Any]):
        """åº”ç”¨é…ç½®æ›´æ”¹"""
        
        # è¿™é‡Œéœ€è¦ä¸CrewAIçš„é…ç½®ç³»ç»Ÿé›†æˆ
        # åº”ç”¨æ–°çš„å‚æ•°é…ç½®
        print(f"åº”ç”¨æ–°é…ç½®: {config}")
    
    async def _evaluate_optimization_impact(self, previous_config: Dict) -> Dict[str, float]:
        """è¯„ä¼°ä¼˜åŒ–å½±å“"""
        
        # æ”¶é›†ä¼˜åŒ–å‰åçš„æ€§èƒ½æ•°æ®
        current_report = self.analyzer.get_performance_report(timedelta(minutes=5))
        
        if "error" in current_report:
            return {"error": "æ— æ³•è·å–æ€§èƒ½æ•°æ®"}
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡çš„æ”¹è¿›æƒ…å†µ
        improvements = {}
        
        # è·å–å½“å‰æŒ‡æ ‡
        current_metrics = self.analyzer.real_time_metrics
        
        if self.baseline_metrics:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„æ”¹è¿›
            for metric_name in ["cpu_usage", "memory_usage", "llm_api_latency", "success_rate", "crew_throughput"]:
                if metric_name in current_metrics and metric_name in self.baseline_metrics:
                    current_value = current_metrics[metric_name]
                    baseline_value = self.baseline_metrics[metric_name]
                    
                    if baseline_value != 0:
                        # å¯¹äº"è¶Šå°è¶Šå¥½"çš„æŒ‡æ ‡ï¼ˆCPUã€å†…å­˜ã€å»¶è¿Ÿï¼‰
                        if metric_name in ["cpu_usage", "memory_usage", "llm_api_latency"]:
                            improvement = (baseline_value - current_value) / baseline_value
                        else:  # å¯¹äº"è¶Šå¤§è¶Šå¥½"çš„æŒ‡æ ‡ï¼ˆæˆåŠŸç‡ã€ååé‡ï¼‰
                            improvement = (current_value - baseline_value) / baseline_value
                        
                        improvements[f"{metric_name}_improvement"] = improvement
        
        # è®¡ç®—æ€»ä½“æ”¹è¿›åˆ†æ•°
        if improvements:
            overall_improvement = statistics.mean(improvements.values())
            improvements["overall_improvement"] = overall_improvement
        
        return improvements
    
    def enable_auto_optimization(self, baseline_collection_time: int = 300):
        """å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–"""
        
        print(f"ğŸš€ å¯ç”¨è‡ªåŠ¨ä¼˜åŒ–ï¼Œæ”¶é›†åŸºçº¿æ•°æ® {baseline_collection_time} ç§’...")
        
        # æ”¶é›†åŸºçº¿æŒ‡æ ‡
        asyncio.create_task(self._collect_baseline_metrics(baseline_collection_time))
        
        self.auto_optimization_enabled = True
    
    async def _collect_baseline_metrics(self, collection_time: int):
        """æ”¶é›†åŸºçº¿æŒ‡æ ‡"""
        
        await asyncio.sleep(collection_time)
        
        # è®¾ç½®åŸºçº¿æŒ‡æ ‡
        self.baseline_metrics = self.analyzer.real_time_metrics.copy()
        
        print("âœ… åŸºçº¿æŒ‡æ ‡æ”¶é›†å®Œæˆï¼Œè‡ªåŠ¨ä¼˜åŒ–å·²æ¿€æ´»")
    
    def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        
        if not self.optimization_history:
            return {"message": "æš‚æ— ä¼˜åŒ–å†å²"}
        
        successful_optimizations = [opt for opt in self.optimization_history if opt["success"]]
        
        report = {
            "total_optimizations": len(self.optimization_history),
            "successful_optimizations": len(successful_optimizations),
            "success_rate": len(successful_optimizations) / len(self.optimization_history),
            "average_improvement": statistics.mean([
                opt["improvement"].get("overall_improvement", 0)
                for opt in successful_optimizations
            ]) if successful_optimizations else 0,
            "recent_optimizations": self.optimization_history[-5:],
            "current_config": self.current_config,
            "auto_optimization_enabled": self.auto_optimization_enabled
        }
        
        return report
```

## 4. å¯è§‚æµ‹æ€§æœ€ä½³å®è·µ

### 4.1 åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ª

#### 4.1.1 CrewAI ä»»åŠ¡é“¾è·¯è¿½è¸ª

```python
import uuid
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

class CrewAITracer:
    """CrewAIä¸“ç”¨é“¾è·¯è¿½è¸ªå™¨"""
    
    def __init__(self, service_name: str = "crewai-service", jaeger_endpoint: str = "http://localhost:14268/api/traces"):
        self.service_name = service_name
        
        # è®¾ç½®è¿½è¸ªæä¾›è€…
        trace.set_tracer_provider(TracerProvider())
        
        # é…ç½®Jaegerå¯¼å‡ºå™¨
        jaeger_exporter = JaegerExporter(
            agent_host_name="localhost",
            agent_port=6831,
            collector_endpoint=jaeger_endpoint,
        )
        
        # æ·»åŠ spanå¤„ç†å™¨
        span_processor = BatchSpanProcessor(jaeger_exporter)
        trace.get_tracer_provider().add_span_processor(span_processor)
        
        # è·å–è¿½è¸ªå™¨
        self.tracer = trace.get_tracer(service_name)
        
        # è‡ªåŠ¨åŒ–HTTPè¯·æ±‚è¿½è¸ª
        RequestsInstrumentor().instrument()
        
        # å­˜å‚¨æ´»è·ƒspan
        self.active_spans: Dict[str, Any] = {}
    
    def start_crew_execution(self, crew_id: str, inputs: Dict[str, Any]) -> str:
        """å¼€å§‹Crewæ‰§è¡Œè¿½è¸ª"""
        
        span = self.tracer.start_span(
            f"crew_execution",
            attributes={
                "crew.id": crew_id,
                "crew.input_size": len(str(inputs)),
                "crew.agents_count": inputs.get("agents_count", 0),
                "crew.tasks_count": inputs.get("tasks_count", 0)
            }
        )
        
        trace_id = str(uuid.uuid4())
        self.active_spans[trace_id] = span
        
        return trace_id
    
    def start_agent_execution(self, trace_id: str, agent_id: str, task_description: str) -> str:
        """å¼€å§‹Agentæ‰§è¡Œè¿½è¸ª"""
        
        parent_span = self.active_spans.get(trace_id)
        
        with self.tracer.start_as_current_span(
            f"agent_execution",
            context=trace.set_span_in_context(parent_span) if parent_span else None,
            attributes={
                "agent.id": agent_id,
                "agent.task_length": len(task_description),
                "agent.task_hash": hash(task_description)
            }
        ) as span:
            
            agent_trace_id = f"{trace_id}_agent_{agent_id}"
            self.active_spans[agent_trace_id] = span
            
            return agent_trace_id
    
    def track_llm_call(self, trace_id: str, model: str, prompt_tokens: int, 
                      completion_tokens: int, latency: float):
        """è¿½è¸ªLLMè°ƒç”¨"""
        
        parent_span = self.active_spans.get(trace_id)
        
        with self.tracer.start_as_current_span(
            f"llm_api_call",
            context=trace.set_span_in_context(parent_span) if parent_span else None,
            attributes={
                "llm.model": model,
                "llm.prompt_tokens": prompt_tokens,
                "llm.completion_tokens": completion_tokens,
                "llm.total_tokens": prompt_tokens + completion_tokens,
                "llm.latency_ms": latency * 1000
            }
        ):
            pass  # Spanä¼šè‡ªåŠ¨å®Œæˆ
    
    def track_tool_execution(self, trace_id: str, tool_name: str, 
                           execution_time: float, success: bool, error_message: str = None):
        """è¿½è¸ªå·¥å…·æ‰§è¡Œ"""
        
        parent_span = self.active_spans.get(trace_id)
        
        with self.tracer.start_as_current_span(
            f"tool_execution",
            context=trace.set_span_in_context(parent_span) if parent_span else None,
            attributes={
                "tool.name": tool_name,
                "tool.execution_time_ms": execution_time * 1000,
                "tool.success": success,
                "tool.error_message": error_message or ""
            }
        ):
            pass
    
    def end_execution_trace(self, trace_id: str, success: bool = True, 
                          result_summary: str = "", error_message: str = ""):
        """ç»“æŸæ‰§è¡Œè¿½è¸ª"""
        
        if trace_id in self.active_spans:
            span = self.active_spans[trace_id]
            
            # æ·»åŠ ç»“æœä¿¡æ¯
            span.set_attributes({
                "execution.success": success,
                "execution.result_length": len(result_summary),
                "execution.error_message": error_message
            })
            
            # è®¾ç½®çŠ¶æ€
            if not success:
                span.set_status(trace.Status(trace.StatusCode.ERROR, error_message))
            else:
                span.set_status(trace.Status(trace.StatusCode.OK))
            
            # ç»“æŸspan
            span.end()
            
            # æ¸…ç†
            del self.active_spans[trace_id]
    
    def get_trace_context(self, trace_id: str) -> Dict[str, str]:
        """è·å–è¿½è¸ªä¸Šä¸‹æ–‡ç”¨äºä¼ æ’­"""
        
        if trace_id in self.active_spans:
            span = self.active_spans[trace_id]
            span_context = span.get_span_context()
            
            return {
                "trace_id": format(span_context.trace_id, '032x'),
                "span_id": format(span_context.span_id, '016x'),
                "trace_flags": f"{span_context.trace_flags:02x}"
            }
        
        return {}

# ä½¿ç”¨ç¤ºä¾‹
tracer = CrewAITracer()

def instrumented_crew_execution(crew, inputs):
    """å¸¦è¿½è¸ªçš„Crewæ‰§è¡Œ"""
    
    # å¼€å§‹è¿½è¸ª
    trace_id = tracer.start_crew_execution(
        crew_id=getattr(crew, 'id', 'unknown'),
        inputs=inputs
    )
    
    try:
        # æ‰§è¡ŒCrew
        result = crew.kickoff(inputs)
        
        # æˆåŠŸç»“æŸè¿½è¸ª
        tracer.end_execution_trace(
            trace_id=trace_id,
            success=True,
            result_summary=str(result)[:500]  # æˆªå–å‰500å­—ç¬¦
        )
        
        return result
        
    except Exception as e:
        # é”™è¯¯ç»“æŸè¿½è¸ª
        tracer.end_execution_trace(
            trace_id=trace_id,
            success=False,
            error_message=str(e)
        )
        raise
```

### 4.2 ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ

#### 4.2.1 ä¼ä¸šçº§æ—¥å¿—æ¶æ„

```python
import logging
import json
from datetime import datetime
from typing import Dict, Any, Optional
import threading
from contextlib import contextmanager

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, service_name: str, log_level: str = "INFO"):
        self.service_name = service_name
        self.session_context: Dict[str, Any] = {}
        self.context_lock = threading.RLock()
        
        # é…ç½®ç»“æ„åŒ–æ—¥å¿—æ ¼å¼
        self.logger = logging.getLogger(service_name)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # åˆ›å»ºç»“æ„åŒ–å¤„ç†å™¨
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = StructuredFormatter()
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    @contextmanager
    def context(self, **context_data):
        """æ—¥å¿—ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        
        with self.context_lock:
            # ä¿å­˜å½“å‰ä¸Šä¸‹æ–‡
            previous_context = self.session_context.copy()
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self.session_context.update(context_data)
            
            try:
                yield
            finally:
                # æ¢å¤ä¸Šä¸‹æ–‡
                self.session_context = previous_context
    
    def _create_log_record(self, level: str, message: str, **extra_data) -> Dict[str, Any]:
        """åˆ›å»ºç»“æ„åŒ–æ—¥å¿—è®°å½•"""
        
        with self.context_lock:
            record = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "service": self.service_name,
                "level": level,
                "message": message,
                "thread": threading.current_thread().name,
                "context": self.session_context.copy()
            }
            
            # æ·»åŠ é¢å¤–æ•°æ®
            if extra_data:
                record["data"] = extra_data
            
            return record
    
    def info(self, message: str, **extra_data):
        """è®°å½•ä¿¡æ¯æ—¥å¿—"""
        record = self._create_log_record("INFO", message, **extra_data)
        self.logger.info(json.dumps(record, ensure_ascii=False))
    
    def warning(self, message: str, **extra_data):
        """è®°å½•è­¦å‘Šæ—¥å¿—"""
        record = self._create_log_record("WARNING", message, **extra_data)
        self.logger.warning(json.dumps(record, ensure_ascii=False))
    
    def error(self, message: str, **extra_data):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        record = self._create_log_record("ERROR", message, **extra_data)
        self.logger.error(json.dumps(record, ensure_ascii=False))
    
    def debug(self, message: str, **extra_data):
        """è®°å½•è°ƒè¯•æ—¥å¿—"""
        record = self._create_log_record("DEBUG", message, **extra_data)
        self.logger.debug(json.dumps(record, ensure_ascii=False))

class StructuredFormatter(logging.Formatter):
    """ç»“æ„åŒ–æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    def format(self, record):
        # ç›´æ¥è¿”å›æ¶ˆæ¯ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨æ¶ˆæ¯ä¸­åŒ…å«äº†JSON
        return record.getMessage()

class CrewAILogger:
    """CrewAIä¸“ç”¨æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self):
        self.structured_logger = StructuredLogger("crewai")
        self.performance_logger = StructuredLogger("crewai-performance")
        self.audit_logger = StructuredLogger("crewai-audit")
        self.security_logger = StructuredLogger("crewai-security")
    
    def log_crew_start(self, crew_id: str, agents: List[str], tasks: List[str], inputs: Dict[str, Any]):
        """è®°å½•Crewå¼€å§‹æ‰§è¡Œ"""
        
        with self.structured_logger.context(crew_id=crew_id):
            self.structured_logger.info(
                "Crew execution started",
                agents=agents,
                tasks=tasks,
                input_keys=list(inputs.keys()),
                agent_count=len(agents),
                task_count=len(tasks)
            )
    
    def log_agent_action(self, agent_id: str, action_type: str, details: Dict[str, Any]):
        """è®°å½•AgentåŠ¨ä½œ"""
        
        with self.structured_logger.context(agent_id=agent_id):
            self.structured_logger.info(
                f"Agent {action_type}",
                action_type=action_type,
                **details
            )
    
    def log_performance_metric(self, metric_name: str, value: float, 
                             category: str = "general", **context):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        
        self.performance_logger.info(
            f"Performance metric: {metric_name}",
            metric_name=metric_name,
            value=value,
            category=category,
            **context
        )
    
    def log_security_event(self, event_type: str, severity: str, 
                          details: Dict[str, Any], user_id: str = None):
        """è®°å½•å®‰å…¨äº‹ä»¶"""
        
        with self.security_logger.context(user_id=user_id, event_type=event_type):
            self.security_logger.info(
                f"Security event: {event_type}",
                severity=severity,
                **details
            )
    
    def log_audit_trail(self, action: str, resource: str, user_id: str = None, 
                       success: bool = True, **metadata):
        """è®°å½•å®¡è®¡è½¨è¿¹"""
        
        with self.audit_logger.context(user_id=user_id):
            self.audit_logger.info(
                f"Audit: {action} on {resource}",
                action=action,
                resource=resource,
                success=success,
                **metadata
            )
    
    def log_error_with_context(self, error: Exception, context: Dict[str, Any]):
        """å¸¦ä¸Šä¸‹æ–‡è®°å½•é”™è¯¯"""
        
        self.structured_logger.error(
            f"Error occurred: {str(error)}",
            error_type=type(error).__name__,
            error_message=str(error),
            context=context,
            traceback=str(error.__traceback__) if error.__traceback__ else None
        )

# å…¨å±€æ—¥å¿—å®ä¾‹
crewai_logger = CrewAILogger()

# ä½¿ç”¨ç¤ºä¾‹è£…é¥°å™¨
def log_execution(log_category: str = "general"):
    """æ‰§è¡Œæ—¥å¿—è£…é¥°å™¨"""
    
    def decorator(func):
        def wrapper(*args, **kwargs):
            start_time = datetime.now()
            
            with crewai_logger.structured_logger.context(
                function=func.__name__,
                category=log_category
            ):
                crewai_logger.structured_logger.info(
                    f"Function execution started: {func.__name__}",
                    args_count=len(args),
                    kwargs_keys=list(kwargs.keys())
                )
                
                try:
                    result = func(*args, **kwargs)
                    execution_time = (datetime.now() - start_time).total_seconds()
                    
                    crewai_logger.structured_logger.info(
                        f"Function execution completed: {func.__name__}",
                        execution_time=execution_time,
                        success=True
                    )
                    
                    # è®°å½•æ€§èƒ½æŒ‡æ ‡
                    crewai_logger.log_performance_metric(
                        metric_name=f"{func.__name__}_execution_time",
                        value=execution_time,
                        category=log_category
                    )
                    
                    return result
                    
                except Exception as e:
                    execution_time = (datetime.now() - start_time).total_seconds()
                    
                    crewai_logger.log_error_with_context(
                        error=e,
                        context={
                            "function": func.__name__,
                            "execution_time": execution_time,
                            "category": log_category
                        }
                    )
                    raise
        
        return wrapper
    return decorator
```

## 4.3 åŸºäºCrewAI Taskå®ç°çš„æ·±åº¦ä»»åŠ¡ç›‘æ§

### 4.3.1 Taskç”Ÿå‘½å‘¨æœŸæ·±åº¦åˆ†æ

åŸºäºå¯¹CrewAI Taskæºç çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥æ„å»ºæ›´ç²¾ç¡®çš„ä»»åŠ¡æ€§èƒ½ç›‘æ§ç³»ç»Ÿï¼Œå……åˆ†åˆ©ç”¨Taskçš„å†…ç½®ç‰¹æ€§ï¼š

```python
from crewai import Task
from crewai.tasks.task_output import TaskOutput
from crewai.utilities.events import TaskStartedEvent, TaskCompletedEvent, TaskFailedEvent
from crewai.utilities.events.crewai_event_bus import crewai_event_bus
from datetime import datetime, timedelta
import threading
import psutil
import time
import hashlib
from typing import Dict, Any, Optional, List, Set
from concurrent.futures import Future

class TaskPerformanceMonitor:
    """åŸºäºCrewAI Taskå®ç°çš„é«˜ç²¾åº¦ä»»åŠ¡æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.task_metrics: Dict[str, Dict] = {}  # æŒ‰task.keyç´¢å¼•
        self.active_tasks: Dict[str, Dict] = {}  # æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡
        self.completed_tasks: List[Dict] = []    # å·²å®Œæˆä»»åŠ¡å†å²
        
        # å¢å¼ºçš„æ€§èƒ½é˜ˆå€¼é…ç½®
        self.performance_thresholds = {
            'execution_time': 30.0,        # ç§’
            'memory_usage': 512,           # MB
            'tool_errors': 3,              # æœ€å¤§å·¥å…·é”™è¯¯æ•°
            'retry_limit': 3,              # æœ€å¤§é‡è¯•æ¬¡æ•°
            'guardrail_failures': 2,       # æœ€å¤§å®ˆæŠ¤å¤±è´¥æ•°
            'delegation_limit': 5,         # æœ€å¤§å§”æ‰˜æ¬¡æ•°
            'context_overflow_threshold': 0.9,  # ä¸Šä¸‹æ–‡æº¢å‡ºé˜ˆå€¼
        }
        
        # æ³¨å†ŒCrewAIäº‹ä»¶ç›‘å¬å™¨
        self._register_event_listeners()
        
        # å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._background_monitoring,
            daemon=True
        )
        self.monitor_thread.start()
    
    def _register_event_listeners(self):
        """æ³¨å†ŒCrewAIäº‹ä»¶ç›‘å¬å™¨"""
        
        # ç›‘å¬ä»»åŠ¡å¼€å§‹äº‹ä»¶
        crewai_event_bus.subscribe(
            TaskStartedEvent,
            self._on_task_started
        )
        
        # ç›‘å¬ä»»åŠ¡å®Œæˆäº‹ä»¶
        crewai_event_bus.subscribe(
            TaskCompletedEvent,
            self._on_task_completed
        )
        
        # ç›‘å¬ä»»åŠ¡å¤±è´¥äº‹ä»¶
        crewai_event_bus.subscribe(
            TaskFailedEvent,
            self._on_task_failed
        )
    
    def _on_task_started(self, event: TaskStartedEvent):
        """å¤„ç†ä»»åŠ¡å¼€å§‹äº‹ä»¶"""
        
        task = event.task
        task_key = task.key  # ä½¿ç”¨MD5å“ˆå¸Œé”®ä½œä¸ºå”¯ä¸€æ ‡è¯†
        
        metrics = {
            'task_id': str(task.id),
            'task_key': task_key,
            'task_name': task.name or f"Task-{str(task.id)[:8]}",
            'agent_role': task.agent.role if task.agent else "unknown",
            'start_time': task.start_time or datetime.now(),
            'description_hash': hashlib.md5(task.description.encode()).hexdigest()[:8],
            
            # ä»»åŠ¡é…ç½®ä¿¡æ¯
            'tools_count': len(task.tools) if task.tools else 0,
            'async_execution': task.async_execution,
            'has_context': task.context is not None and task.context != [],
            'context_tasks_count': len(task.context) if isinstance(task.context, list) else 0,
            'has_guardrail': task._guardrail is not None,
            'max_retries': task.max_retries,
            'human_input_required': task.human_input,
            'markdown_output': task.markdown,
            'has_output_file': task.output_file is not None,
            
            # è¾“å‡ºæ ¼å¼é…ç½®
            'output_format': {
                'json': task.output_json is not None,
                'pydantic': task.output_pydantic is not None,
                'file': task.output_file is not None,
                'create_directory': task.create_directory
            },
            
            # å®æ—¶çŠ¶æ€è¿½è¸ª
            'current_retries': task.retry_count,
            'tools_used': task.used_tools,
            'tools_errors': task.tools_errors,
            'delegations': task.delegations,
            'processed_by_agents': list(task.processed_by_agents),
            
            # ç³»ç»ŸæŒ‡æ ‡
            'memory_before': self._get_memory_usage(),
            'cpu_before': psutil.cpu_percent(),
            'thread_id': threading.current_thread().ident,
            'process_id': psutil.Process().pid
        }
        
        # å­˜å‚¨åˆ°æ´»è·ƒä»»åŠ¡è¿½è¸ª
        self.active_tasks[task_key] = metrics
        
        print(f"ğŸ“Š Task started monitoring: {metrics['task_name']} [{task_key[:8]}]")
    
    def _on_task_completed(self, event: TaskCompletedEvent):
        """å¤„ç†ä»»åŠ¡å®Œæˆäº‹ä»¶"""
        
        task = event.task
        task_output = event.output
        task_key = task.key
        
        if task_key in self.active_tasks:
            metrics = self.active_tasks[task_key]
            
            # æ›´æ–°å®ŒæˆæŒ‡æ ‡
            end_time = task.end_time or datetime.now()
            execution_duration = task.execution_duration or (
                end_time - metrics['start_time']
            ).total_seconds()
            
            metrics.update({
                'end_time': end_time,
                'execution_duration': execution_duration,
                'success': True,
                'error_message': None,
                
                # ä»»åŠ¡è¾“å‡ºä¿¡æ¯
                'output_size': len(str(task_output.raw)) if task_output else 0,
                'output_format_actual': task_output.output_format.value if task_output else 'none',
                'has_pydantic_output': task_output.pydantic is not None if task_output else False,
                'has_json_output': task_output.json_dict is not None if task_output else False,
                
                # æœ€ç»ˆçŠ¶æ€ç»Ÿè®¡
                'final_retries': task.retry_count,
                'final_tools_used': task.used_tools,
                'final_tools_errors': task.tools_errors,
                'final_delegations': task.delegations,
                'final_processed_agents': list(task.processed_by_agents),
                
                # ç³»ç»ŸæŒ‡æ ‡å·®å¼‚
                'memory_after': self._get_memory_usage(),
                'cpu_after': psutil.cpu_percent(),
                'memory_delta': self._get_memory_usage() - metrics['memory_before'],
                'cpu_delta': psutil.cpu_percent() - metrics['cpu_before']
            })
            
            # æ€§èƒ½åˆ†æ
            metrics['performance_analysis'] = self._analyze_task_performance(metrics)
            
            # ç§»åŠ¨åˆ°å®Œæˆåˆ—è¡¨
            self.completed_tasks.append(metrics)
            del self.active_tasks[task_key]
            
            # ç»´æŠ¤å†å²è®°å½•å¤§å°
            if len(self.completed_tasks) > 10000:
                self.completed_tasks = self.completed_tasks[-10000:]
            
            print(f"âœ… Task completed: {metrics['task_name']} in {execution_duration:.2f}s")
    
    def _background_monitoring(self):
        """åå°ç›‘æ§å¾ªç¯"""
        
        while self.monitoring_active:
            try:
                # æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
                current_time = datetime.now()
                for task_key, metrics in list(self.active_tasks.items()):
                    elapsed = (current_time - metrics['start_time']).total_seconds()
                    
                    # é•¿æ—¶é—´è¿è¡Œè­¦å‘Š
                    if elapsed > self.performance_thresholds['execution_time'] * 2:
                        print(f"âš ï¸ é•¿æ—¶é—´è¿è¡Œä»»åŠ¡: {metrics['task_name']} ({elapsed:.1f}s)")
                    
                    # æ›´æ–°å®æ—¶æŒ‡æ ‡
                    metrics['elapsed_time'] = elapsed
                    metrics['current_memory'] = self._get_memory_usage()
                
                # æ£€æŸ¥ç³»ç»Ÿèµ„æº
                self._check_system_resources()
                
                time.sleep(10)  # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"Background monitoring error: {e}")
                time.sleep(30)
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except Exception:
            return 0.0
    
    def get_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent_tasks = [
            task for task in self.completed_tasks 
            if task['start_time'] >= cutoff_time
        ]
        
        if not recent_tasks:
            return {'message': f'è¿‡å»{hours}å°æ—¶æ— å®Œæˆä»»åŠ¡'}
        
        # åŸºç¡€ç»Ÿè®¡
        total_tasks = len(recent_tasks)
        successful_tasks = [t for t in recent_tasks if t['success']]
        failed_tasks = [t for t in recent_tasks if not t['success']]
        
        success_rate = len(successful_tasks) / total_tasks if total_tasks > 0 else 0
        
        # æ€§èƒ½ç»Ÿè®¡
        execution_times = [t['execution_duration'] for t in recent_tasks]
        avg_execution_time = sum(execution_times) / len(execution_times) if execution_times else 0
        
        # å·¥å…·å’Œé”™è¯¯ç»Ÿè®¡
        total_tool_errors = sum(t.get('final_tools_errors', 0) for t in recent_tasks)
        total_retries = sum(t.get('final_retries', 0) for t in recent_tasks)
        
        return {
            'period': f'{hours}å°æ—¶',
            'total_tasks': total_tasks,
            'success_rate': success_rate,
            'failed_tasks': len(failed_tasks),
            
            'performance_metrics': {
                'avg_execution_time': avg_execution_time,
                'min_execution_time': min(execution_times) if execution_times else 0,
                'max_execution_time': max(execution_times) if execution_times else 0,
                'total_tool_errors': total_tool_errors,
                'total_retries': total_retries,
            },
            
            'recent_failures': [
                {
                    'task_name': t['task_name'],
                    'error_message': t['error_message'],
                    'duration': t['execution_duration']
                }
                for t in failed_tasks[-5:]  # æœ€è¿‘5ä¸ªå¤±è´¥ä»»åŠ¡
            ]
        }

    def shutdown(self):
        """å…³é—­ç›‘æ§å™¨"""
        self.monitoring_active = False
        if self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
        print("ğŸ“Š Task performance monitor shut down")
```

### 4.3.2 Taskå®ˆæŠ¤è§„åˆ™æ€§èƒ½å½±å“åˆ†æ

åŸºäºTask.pyä¸­çš„å®ˆæŠ¤è§„åˆ™å®ç°ï¼Œç›‘æ§å®ˆæŠ¤è§„åˆ™å¯¹æ€§èƒ½çš„å½±å“ï¼š

```python
class GuardrailPerformanceAnalyzer:
    """å®ˆæŠ¤è§„åˆ™æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.guardrail_metrics = {}
        self.validation_history = []
    
    def track_guardrail_validation(self, task: Any, validation_time: float, 
                                  retry_count: int, success: bool):
        """è·Ÿè¸ªå®ˆæŠ¤è§„åˆ™éªŒè¯æ€§èƒ½"""
        
        guardrail_type = "callable" if callable(task.guardrail) else "llm_based"
        task_key = getattr(task, 'key', str(id(task)))
        
        metrics = {
            'timestamp': datetime.now(),
            'task_key': task_key,
            'guardrail_type': guardrail_type,
            'validation_time': validation_time,
            'retry_count': retry_count,
            'success': success,
            'max_retries': task.max_retries,
            'exhausted_retries': retry_count >= task.max_retries
        }
        
        self.validation_history.append(metrics)
        
        # ç»´æŠ¤å†å²å¤§å°
        if len(self.validation_history) > 1000:
            self.validation_history = self.validation_history[-1000:]
    
    def get_guardrail_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå®ˆæŠ¤è§„åˆ™æ€§èƒ½æŠ¥å‘Š"""
        
        if not self.validation_history:
            return {'message': 'æ— å®ˆæŠ¤è§„åˆ™éªŒè¯å†å²'}
        
        # æŒ‰ç±»å‹åˆ†ç»„ç»Ÿè®¡
        callable_validations = [v for v in self.validation_history if v['guardrail_type'] == 'callable']
        llm_validations = [v for v in self.validation_history if v['guardrail_type'] == 'llm_based']
        
        report = {
            'total_validations': len(self.validation_history),
            'callable_guardrails': {
                'count': len(callable_validations),
                'avg_time': sum(v['validation_time'] for v in callable_validations) / len(callable_validations) if callable_validations else 0,
                'success_rate': sum(v['success'] for v in callable_validations) / len(callable_validations) if callable_validations else 0
            },
            'llm_guardrails': {
                'count': len(llm_validations),
                'avg_time': sum(v['validation_time'] for v in llm_validations) / len(llm_validations) if llm_validations else 0,
                'success_rate': sum(v['success'] for v in llm_validations) / len(llm_validations) if llm_validations else 0
            },
            'retry_analysis': {
                'avg_retries': sum(v['retry_count'] for v in self.validation_history) / len(self.validation_history),
                'exhausted_retries_rate': sum(v['exhausted_retries'] for v in self.validation_history) / len(self.validation_history)
            }
        }
        
        return report
```

## 5. ç”Ÿäº§ç¯å¢ƒç›‘æ§ç³»ç»Ÿ

### 5.1 å‘Šè­¦å’Œé€šçŸ¥ç³»ç»Ÿ

#### 5.1.1 å¤šæ¸ é“å‘Šè­¦ç®¡ç†

```python
import smtplib
import requests
import asyncio
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from abc import ABC, abstractmethod
from typing import List, Dict, Any
from enum import Enum

class AlertSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class AlertChannel(ABC):
    """å‘Šè­¦é€šé“æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    async def send_alert(self, alert: Dict[str, Any]) -> bool:
        pass

class EmailAlertChannel(AlertChannel):
    """é‚®ä»¶å‘Šè­¦é€šé“"""
    
    def __init__(self, smtp_host: str, smtp_port: int, username: str, password: str):
        self.smtp_host = smtp_host
        self.smtp_port = smtp_port
        self.username = username
        self.password = password
    
    async def send_alert(self, alert: Dict[str, Any]) -> bool:
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        
        try:
            msg = MIMEMultipart()
            msg['From'] = self.username
            msg['To'] = ", ".join(alert['recipients'])
            msg['Subject'] = f"[{alert['severity'].upper()}] CrewAI Alert: {alert['title']}"
            
            # æ„å»ºé‚®ä»¶å†…å®¹
            body = self._build_email_body(alert)
            msg.attach(MIMEText(body, 'html'))
            
            # å‘é€é‚®ä»¶
            server = smtplib.SMTP(self.smtp_host, self.smtp_port)
            server.starttls()
            server.login(self.username, self.password)
            
            text = msg.as_string()
            server.sendmail(self.username, alert['recipients'], text)
            server.quit()
            
            return True
            
        except Exception as e:
            print(f"Failed to send email alert: {e}")
            return False
    
    def _build_email_body(self, alert: Dict[str, Any]) -> str:
        """æ„å»ºé‚®ä»¶å†…å®¹"""
        
        severity_colors = {
            "low": "#28a745",
            "medium": "#ffc107", 
            "high": "#fd7e14",
            "critical": "#dc3545"
        }
        
        color = severity_colors.get(alert['severity'], "#6c757d")
        
        body = f"""
        <html>
        <body>
            <div style="font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;">
                <div style="background-color: {color}; color: white; padding: 20px; border-radius: 5px 5px 0 0;">
                    <h2 style="margin: 0;">CrewAI ç³»ç»Ÿå‘Šè­¦</h2>
                    <p style="margin: 5px 0 0 0;">ä¸¥é‡ç¨‹åº¦: {alert['severity'].upper()}</p>
                </div>
                
                <div style="border: 1px solid #ddd; border-top: none; padding: 20px; border-radius: 0 0 5px 5px;">
                    <h3>{alert['title']}</h3>
                    <p><strong>æ—¶é—´:</strong> {alert['timestamp']}</p>
                    <p><strong>æè¿°:</strong> {alert['message']}</p>
                    
                    {self._format_metrics_table(alert.get('metrics', {}))}
                    
                    {self._format_recommendations(alert.get('recommendations', []))}
                </div>
            </div>
        </body>
        </html>
        """
        
        return body
    
    def _format_metrics_table(self, metrics: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡è¡¨æ ¼"""
        
        if not metrics:
            return ""
        
        rows = ""
        for key, value in metrics.items():
            rows += f"<tr><td>{key}</td><td>{value}</td></tr>"
        
        return f"""
        <h4>ç›¸å…³æŒ‡æ ‡:</h4>
        <table style="width: 100%; border-collapse: collapse; margin: 10px 0;">
            <thead>
                <tr style="background-color: #f8f9fa;">
                    <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">æŒ‡æ ‡</th>
                    <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">å€¼</th>
                </tr>
            </thead>
            <tbody>
                {rows}
            </tbody>
        </table>
        """
    
    def _format_recommendations(self, recommendations: List[str]) -> str:
        """æ ¼å¼åŒ–å»ºè®®åˆ—è¡¨"""
        
        if not recommendations:
            return ""
        
        items = "".join([f"<li>{rec}</li>" for rec in recommendations])
        
        return f"""
        <h4>å»ºè®®æ“ä½œ:</h4>
        <ul style="margin: 10px 0; padding-left: 20px;">
            {items}
        </ul>
        """

class SlackAlertChannel(AlertChannel):
    """Slackå‘Šè­¦é€šé“"""
    
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
    
    async def send_alert(self, alert: Dict[str, Any]) -> bool:
        """å‘é€Slackå‘Šè­¦"""
        
        try:
            # æ„å»ºSlackæ¶ˆæ¯
            payload = self._build_slack_payload(alert)
            
            # å‘é€åˆ°Slack
            response = requests.post(self.webhook_url, json=payload, timeout=10)
            
            return response.status_code == 200
            
        except Exception as e:
            print(f"Failed to send Slack alert: {e}")
            return False
    
    def _build_slack_payload(self, alert: Dict[str, Any]) -> Dict[str, Any]:
        """æ„å»ºSlackæ¶ˆæ¯è½½è·"""
        
        severity_colors = {
            "low": "good",
            "medium": "warning",
            "high": "warning", 
            "critical": "danger"
        }
        
        color = severity_colors.get(alert['severity'], "good")
        
        fields = []
        
        # æ·»åŠ æŒ‡æ ‡å­—æ®µ
        for key, value in alert.get('metrics', {}).items():
            fields.append({
                "title": key,
                "value": str(value),
                "short": True
            })
        
        payload = {
            "attachments": [{
                "color": color,
                "title": f"[{alert['severity'].upper()}] {alert['title']}",
                "text": alert['message'],
                "fields": fields,
                "footer": "CrewAI Monitoring",
                "ts": int(alert['timestamp'].timestamp()) if hasattr(alert['timestamp'], 'timestamp') else None
            }]
        }
        
        return payload

class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.channels: List[AlertChannel] = []
        self.alert_rules: Dict[str, Dict] = {}
        self.alert_history: List[Dict] = []
        self.suppression_rules: Dict[str, Dict] = {}
        
        # é»˜è®¤æ”¶ä»¶äººé…ç½®
        self.default_recipients = {
            "low": ["monitoring@company.com"],
            "medium": ["devops@company.com", "monitoring@company.com"],
            "high": ["devops@company.com", "management@company.com"],
            "critical": ["devops@company.com", "management@company.com", "cto@company.com"]
        }
    
    def add_channel(self, channel: AlertChannel):
        """æ·»åŠ å‘Šè­¦é€šé“"""
        self.channels.append(channel)
    
    def add_alert_rule(self, rule_name: str, condition: callable, 
                      severity: AlertSeverity, title: str, message: str,
                      recommendations: List[str] = None):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        
        self.alert_rules[rule_name] = {
            "condition": condition,
            "severity": severity,
            "title": title, 
            "message": message,
            "recommendations": recommendations or [],
            "last_triggered": None,
            "trigger_count": 0
        }
    
    async def evaluate_alerts(self, metrics: PerformanceMetrics):
        """è¯„ä¼°å‘Šè­¦æ¡ä»¶"""
        
        for rule_name, rule_config in self.alert_rules.items():
            try:
                if rule_config["condition"](metrics):
                    await self._trigger_alert(rule_name, rule_config, metrics)
                    
            except Exception as e:
                print(f"Error evaluating alert rule {rule_name}: {e}")
    
    async def _trigger_alert(self, rule_name: str, rule_config: Dict, metrics: PerformanceMetrics):
        """è§¦å‘å‘Šè­¦"""
        
        # æ£€æŸ¥æ˜¯å¦è¢«æŠ‘åˆ¶
        if self._is_suppressed(rule_name):
            return
        
        # æ›´æ–°è§„åˆ™ç»Ÿè®¡
        rule_config["last_triggered"] = datetime.now()
        rule_config["trigger_count"] += 1
        
        # æ„å»ºå‘Šè­¦æ¶ˆæ¯
        alert = {
            "id": str(uuid.uuid4()),
            "rule_name": rule_name,
            "severity": rule_config["severity"].value,
            "title": rule_config["title"],
            "message": rule_config["message"],
            "timestamp": datetime.now(),
            "metrics": {
                "cpu_usage": f"{metrics.cpu_usage:.1f}%",
                "memory_usage": f"{metrics.memory_usage:.1f}%", 
                "llm_api_latency": f"{metrics.llm_api_latency:.2f}s",
                "success_rate": f"{metrics.success_rate:.2%}",
                "crew_throughput": f"{metrics.crew_throughput} tasks/min"
            },
            "recommendations": rule_config["recommendations"],
            "recipients": self.default_recipients.get(rule_config["severity"].value, [])
        }
        
        # å‘é€åˆ°æ‰€æœ‰é€šé“
        success_count = 0
        for channel in self.channels:
            try:
                if await channel.send_alert(alert):
                    success_count += 1
            except Exception as e:
                print(f"Failed to send alert via channel: {e}")
        
        # è®°å½•å‘Šè­¦å†å²
        alert["channels_notified"] = success_count
        alert["total_channels"] = len(self.channels)
        self.alert_history.append(alert)
        
        # æ¸…ç†å†å²è®°å½•
        if len(self.alert_history) > 1000:
            self.alert_history = self.alert_history[-1000:]
        
        print(f"ğŸš¨ Alert triggered: {alert['title']} (sent to {success_count}/{len(self.channels)} channels)")
    
    def _is_suppressed(self, rule_name: str) -> bool:
        """æ£€æŸ¥å‘Šè­¦æ˜¯å¦è¢«æŠ‘åˆ¶"""
        
        if rule_name in self.suppression_rules:
            suppression = self.suppression_rules[rule_name]
            
            # æ£€æŸ¥æŠ‘åˆ¶æ—¶é—´çª—å£
            if "until" in suppression and datetime.now() < suppression["until"]:
                return True
            
            # æ£€æŸ¥æŠ‘åˆ¶æ¡ä»¶
            if "condition" in suppression and suppression["condition"]():
                return True
        
        return False
    
    def suppress_alert(self, rule_name: str, duration_minutes: int = 60, reason: str = ""):
        """ä¸´æ—¶æŠ‘åˆ¶å‘Šè­¦"""
        
        self.suppression_rules[rule_name] = {
            "until": datetime.now() + timedelta(minutes=duration_minutes),
            "reason": reason,
            "suppressed_at": datetime.now()
        }
        
        print(f"ğŸ”‡ Alert {rule_name} suppressed for {duration_minutes} minutes: {reason}")
    
    def get_alert_summary(self) -> Dict[str, Any]:
        """è·å–å‘Šè­¦æ‘˜è¦"""
        
        recent_alerts = [
            alert for alert in self.alert_history
            if alert['timestamp'] > datetime.now() - timedelta(hours=24)
        ]
        
        severity_counts = {"low": 0, "medium": 0, "high": 0, "critical": 0}
        for alert in recent_alerts:
            severity_counts[alert['severity']] += 1
        
        return {
            "total_rules": len(self.alert_rules),
            "active_suppressions": len(self.suppression_rules),
            "recent_alerts_24h": len(recent_alerts),
            "severity_breakdown": severity_counts,
            "most_triggered_rules": sorted(
                [(name, config["trigger_count"]) for name, config in self.alert_rules.items()],
                key=lambda x: x[1],
                reverse=True
            )[:5],
            "recent_alerts": recent_alerts[-10:]
        }

# ä½¿ç”¨ç¤ºä¾‹
def setup_production_monitoring():
    """è®¾ç½®ç”Ÿäº§ç¯å¢ƒç›‘æ§"""
    
    # åˆ›å»ºå‘Šè­¦ç®¡ç†å™¨
    alert_manager = AlertManager()
    
    # æ·»åŠ å‘Šè­¦é€šé“
    email_channel = EmailAlertChannel(
        smtp_host="smtp.company.com",
        smtp_port=587,
        username="monitoring@company.com",
        password="password"
    )
    
    slack_channel = SlackAlertChannel(
        webhook_url="https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
    )
    
    alert_manager.add_channel(email_channel)
    alert_manager.add_channel(slack_channel)
    
    # æ·»åŠ å‘Šè­¦è§„åˆ™
    alert_manager.add_alert_rule(
        "high_cpu_usage",
        condition=lambda m: m.cpu_usage > 80,
        severity=AlertSeverity.HIGH,
        title="CPUä½¿ç”¨ç‡è¿‡é«˜",
        message="ç³»ç»ŸCPUä½¿ç”¨ç‡è¶…è¿‡80%ï¼Œå¯èƒ½å½±å“æ€§èƒ½",
        recommendations=[
            "æ£€æŸ¥å½“å‰è¿è¡Œçš„ä»»åŠ¡æ•°é‡",
            "è€ƒè™‘é™ä½å¹¶å‘åº¦", 
            "ç›‘æ§è¿›ç¨‹èµ„æºä½¿ç”¨æƒ…å†µ"
        ]
    )
    
    alert_manager.add_alert_rule(
        "low_success_rate",
        condition=lambda m: m.success_rate < 0.9,
        severity=AlertSeverity.CRITICAL,
        title="ä»»åŠ¡æˆåŠŸç‡è¿‡ä½",
        message="ä»»åŠ¡æ‰§è¡ŒæˆåŠŸç‡ä½äº90%ï¼Œéœ€è¦ç«‹å³å…³æ³¨",
        recommendations=[
            "æ£€æŸ¥é”™è¯¯æ—¥å¿—",
            "éªŒè¯å¤–éƒ¨æœåŠ¡çŠ¶æ€",
            "æ£€æŸ¥ä»»åŠ¡é…ç½®æ˜¯å¦æ­£ç¡®"
        ]
    )
    
    return alert_manager
```

## ç»“è®º

CrewAI çš„æ€§èƒ½ä¸ç›‘æ§ä½“ç³»æ˜¯ç¡®ä¿ç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œçš„å…³é”®åŸºç¡€è®¾æ–½ã€‚é€šè¿‡æœ¬æ–‡æ¡£æä¾›çš„æ·±åº¦åˆ†æå’Œå®è·µæŒ‡å¯¼ï¼Œå¼€å‘è€…å¯ä»¥ï¼š

### æ ¸å¿ƒèƒ½åŠ›å»ºè®¾

1. **å…¨æ–¹ä½æ€§èƒ½ç›‘æ§**
   - å››å±‚æ€§èƒ½åˆ†ææ¶æ„ï¼ˆåº”ç”¨å±‚ã€æ¡†æ¶å±‚ã€ç³»ç»Ÿå±‚ã€å¤–éƒ¨æœåŠ¡å±‚ï¼‰
   - å®æ—¶æŒ‡æ ‡æ”¶é›†å’Œå†å²è¶‹åŠ¿åˆ†æ
   - æ™ºèƒ½ç“¶é¢ˆæ£€æµ‹å’Œè‡ªåŠ¨ä¼˜åŒ–

2. **ä¼ä¸šçº§å¯è§‚æµ‹æ€§**  
   - åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªç³»ç»Ÿ
   - ç»“æ„åŒ–æ—¥å¿—ç®¡ç†
   - å¤šç»´åº¦ KPI è·Ÿè¸ªä½“ç³»

3. **æ™ºèƒ½è¿ç»´èƒ½åŠ›**
   - è‡ªåŠ¨åŒ–æ€§èƒ½ä¼˜åŒ–å¼•æ“
   - å¤šæ¸ é“å‘Šè­¦é€šçŸ¥ç³»ç»Ÿ
   - é¢„æµ‹æ€§æ•…éšœé¢„é˜²

### ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

1. **ç›‘æ§ç­–ç•¥**: é‡‡ç”¨åˆ†å±‚ç›‘æ§æ¨¡å¼ï¼Œä»ä¸šåŠ¡æŒ‡æ ‡åˆ°ç³»ç»Ÿèµ„æºçš„å…¨æ ˆè¦†ç›–
2. **å‘Šè­¦è®¾è®¡**: åŸºäº SLI/SLO çš„æ™ºèƒ½å‘Šè­¦ï¼Œå‡å°‘è¯¯æŠ¥æé«˜å“åº”æ•ˆç‡
3. **æ€§èƒ½ä¼˜åŒ–**: æ•°æ®é©±åŠ¨çš„æŒç»­ä¼˜åŒ–ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿å’Œæ”¹è¿›é—­ç¯
4. **æ•…éšœå¤„ç†**: å®Œæ•´çš„äº‹ä»¶å“åº”æµç¨‹ï¼Œå¿«é€Ÿå®šä½å’Œæ¢å¤èƒ½åŠ›

### æŒç»­æ”¹è¿›å»ºè®®

1. **æŒ‡æ ‡æ¼”è¿›**: æ ¹æ®ä¸šåŠ¡å‘å±•æŒç»­å®Œå–„ç›‘æ§æŒ‡æ ‡ä½“ç³»
2. **è‡ªåŠ¨åŒ–æå‡**: é€æ­¥æé«˜ç›‘æ§ã€å‘Šè­¦ã€æ¢å¤çš„è‡ªåŠ¨åŒ–æ°´å¹³  
3. **æˆæœ¬ä¼˜åŒ–**: å»ºç«‹æ€§èƒ½ä¸æˆæœ¬çš„å¹³è¡¡æœºåˆ¶ï¼Œå®ç°æœ€ä½³æ€§ä»·æ¯”
4. **å›¢é˜Ÿèƒ½åŠ›**: åŸ¹å…»å›¢é˜Ÿçš„å¯è§‚æµ‹æ€§æ€ç»´å’Œæ•…éšœå¤„ç†èƒ½åŠ›

é€šè¿‡ç³»ç»Ÿæ€§åœ°å®æ–½æœ¬æ–‡æ¡£æä¾›çš„æ€§èƒ½ç›‘æ§æ–¹æ¡ˆï¼Œå¯ä»¥ä¸º CrewAI åº”ç”¨æ„å»ºèµ·ä¼ä¸šçº§çš„å¯è§‚æµ‹æ€§åŸºç¡€è®¾æ–½ï¼Œç¡®ä¿ç³»ç»Ÿåœ¨å¤æ‚ç”Ÿäº§ç¯å¢ƒä¸­çš„ç¨³å®šé«˜æ•ˆè¿è¡Œã€‚